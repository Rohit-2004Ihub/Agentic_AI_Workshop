[{"text": "Providedproperattributionisprovided,Googleherebygrantspermissionto\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\nscholarlyworks.\nAttention Is All You Need\nAshishVaswani\u2217 NoamShazeer\u2217 NikiParmar\u2217 JakobUszkoreit\u2217\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones\u2217 AidanN.Gomez\u2217 \u2020 \u0141ukaszKaiser\u2217\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n\u2217Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n\u2020WorkperformedwhileatGoogleBrain.\n\u2021WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n3202\nguA\n2\n]LC.sc[\n7v26730.6071:viXra", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 1}}, {"text": "1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t\u22121\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\n2", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 2}}, {"text": "Figure1: TheTransformer-modelarchitecture.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlayers,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\n3", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 3}}, {"text": "ScaledDot-ProductAttention Multi-HeadAttention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattentionlayersrunninginparallel.\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3.2.1 ScaledDot-ProductAttention\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\nqueriesandkeysofdimensiond\nk\n,a\u221andvaluesofdimensiond\nv\n. Wecomputethedotproductsofthe\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\nk\nvalues.\nInpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\nintoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\nthematrixofoutputsas:\nQKT\nAttention(Q,K,V)=softmax( \u221a )V (1)\nd\nk\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\nplicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\nof \u221a1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\ndk\nasinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\nmatrixmultiplicationcode.\nWhileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\nk\ndotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof\nk\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\nk\nextremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby \u221a1 .\ndk\n3.2.2 Multi-HeadAttention\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\nmodel\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\nk k v\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\nv\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\nvariableswithmean0andvariance1.Thentheirdotproduct,q\u00b7k=\n(cid:80)dk\nq k ,hasmean0andvarianced .\ni=1 i i k\n4", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 4}}, {"text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepictedinFigure2.\nMulti-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\nsubspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.\nMultiHead(Q,K,V)=Concat(head ,...,head )WO\n1 h\nwherehead =Attention(QWQ,KWK,VWV)\ni i i i\nWheretheprojectionsareparametermatricesWQ \u2208Rdmodel\u00d7dk,WK \u2208Rdmodel\u00d7dk,WV \u2208Rdmodel\u00d7dv\ni i i\nandWO \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\nk v model\nissimilartothatofsingle-headattentionwithfulldimensionality.\n3.2.3 ApplicationsofAttentioninourModel\nTheTransformerusesmulti-headattentioninthreedifferentways:\n\u2022 In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\nandthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\npositioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38,2,9].\n\u2022 Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\nencoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\nencoder.\n\u2022 Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\nallpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward\ninformationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\ninsideofscaleddot-productattentionbymaskingout(settingto\u2212\u221e)allvaluesintheinput\nofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.\n3.3 Position-wiseFeed-ForwardNetworks\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\nconsistsoftwolineartransformationswithaReLUactivationinbetween.\nFFN(x)=max(0,xW +b )W +b (2)\n1 1 2 2\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is d = 512, and the inner-layer has dimensionality\nmodel\nd =2048.\nff\n3.4 EmbeddingsandSoftmax\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\ntokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-\nmodel\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-\u221asoftmax\nlineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d .\nmodel\n5", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 5}}, {"text": "Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\nfordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\nLayerType ComplexityperLayer Sequential MaximumPathLength\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(log (n))\nk\nSelf-Attention(restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 PositionalEncoding\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\norderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\ntokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\nbottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\nmodel\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\nlearnedandfixed[9].\nInthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\nPE =sin(pos/100002i/dmodel)\n(pos,2i)\nPE =cos(pos/100002i/dmodel)\n(pos,2i+1)\nwhereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\ncorrespondstoasinusoid. Thewavelengthsformageometricprogressionfrom2\u03c0to10000\u00b72\u03c0. We\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\nrelativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\npos+k\nPE .\npos\nWealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo\nversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\nduringtraining.\n4 WhySelf-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\n(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z \u2208 Rd, such as a hidden\n1 n 1 n i i\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\nconsiderthreedesiderata.\nOneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\ndependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\nandoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\ndifferentlayertypes.\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\n6", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 6}}, {"text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\n[38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\ntheinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\nAsingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\norO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths\nk\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, toO(k\u00b7n\u00b7d+n\u00b7d2). Evenwithk = n, however, thecomplexityofaseparable\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\ntheapproachwetakeinourmodel.\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\nandsemanticstructureofthesentences.\n5 Training\nThissectiondescribesthetrainingregimeforourmodels.\n5.1 TrainingDataandBatching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\nvocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\ntargettokens.\n5.2 HardwareandSchedule\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\n(3.5days).\n5.3 Optimizer\nWeusedtheAdamoptimizer[20]with\u03b2 =0.9,\u03b2 =0.98and\u03f5=10\u22129. Wevariedthelearning\n1 2\nrateoverthecourseoftraining,accordingtotheformula:\nlrate=d\u22120.5 \u00b7min(step_num\u22120.5,step_num\u00b7warmup_steps\u22121.5) (3)\nmodel\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\nwarmup_steps=4000.\n5.4 Regularization\nWeemploythreetypesofregularizationduringtraining:\n7", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 7}}, {"text": "Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\nBLEU TrainingCost(FLOPs)\nModel\nEN-DE EN-FR EN-DE EN-FR\nByteNet[18] 23.75\nDeep-Att+PosUnk[39] 39.2 1.0\u00b71020\nGNMT+RL[38] 24.6 39.92 2.3\u00b71019 1.4\u00b71020\nConvS2S[9] 25.16 40.46 9.6\u00b71018 1.5\u00b71020\nMoE[32] 26.03 40.56 2.0\u00b71019 1.2\u00b71020\nDeep-Att+PosUnkEnsemble[39] 40.4 8.0\u00b71020\nGNMT+RLEnsemble[38] 26.30 41.16 1.8\u00b71020 1.1\u00b71021\nConvS2SEnsemble[9] 26.36 41.29 7.7\u00b71019 1.2\u00b71021\nTransformer(basemodel) 27.3 38.1 3.3\u00b71018\nTransformer(big) 28.4 41.8 2.3\u00b71019\nResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\nP =0.1.\ndrop\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue\u03f5 = 0.1[36]. This\nls\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\n6 Results\n6.1 MachineTranslation\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\nlistedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\nthecompetitivemodels.\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\ndropoutrateP =0.1,insteadof0.3.\ndrop\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\nusedbeamsearchwithabeamsizeof4andlengthpenalty\u03b1 = 0.6[38]. Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5.\n6.2 ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n8", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 8}}, {"text": "Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities.\ntrain PPL BLEU params\nN d d h d d P \u03f5\nmodel ff k v drop ls steps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n(A)\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n16 5.16 25.1 58\n(B)\n32 5.01 25.4 60\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n(C) 256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging. WepresenttheseresultsinTable3.\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\nk\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\nresultstothebasemodel.\n6.3 EnglishConstituencyParsing\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\nWetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\nmodel\nPennTreebank[25],about40Ktrainingsentences. Wealsotraineditinasemi-supervisedsetting,\nusingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences\n[37]. Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\nforthesemi-supervisedsetting.\nWeperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual\n(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 9}}, {"text": "Table4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\nofWSJ)\nParser Training WSJ23F1\nVinyals&Kaiserelal. (2014)[37] WSJonly,discriminative 88.3\nPetrovetal. (2006)[29] WSJonly,discriminative 90.4\nZhuetal. (2013)[40] WSJonly,discriminative 90.4\nDyeretal. (2016)[8] WSJonly,discriminative 91.7\nTransformer(4layers) WSJonly,discriminative 91.3\nZhuetal. (2013)[40] semi-supervised 91.3\nHuang&Harper(2009)[14] semi-supervised 91.3\nMcCloskyetal. (2006)[26] semi-supervised 92.1\nVinyals&Kaiserelal. (2014)[37] semi-supervised 92.1\nTransformer(4layers) semi-supervised 92.7\nLuongetal. (2015)[23] multi-task 93.0\nDyeretal. (2016)[8] generative 93.3\nincreasedthemaximumoutputlengthtoinputlength+300. Weusedabeamsizeof21and\u03b1=0.3\nforbothWSJonlyandthesemi-supervisedsetting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\nRecurrentNeuralNetworkGrammar[8].\nIncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-\nParser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.\n7 Conclusion\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\nattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\nmulti-headedself-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\nmodeloutperformsevenallpreviouslyreportedensembles.\nWeareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\nsuchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\ncomments,correctionsandinspiration.\nReferences\n[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\narXiv:1607.06450,2016.\n[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014.\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\nmachinetranslationarchitectures. CoRR,abs/1703.03906,2017.\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\nreading. arXivpreprintarXiv:1601.06733,2016.\n10", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 10}}, {"text": "[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\nmachinetranslation. CoRR,abs/1406.1078,2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprintarXiv:1610.02357,2016.\n[7] JunyoungChung,\u00c7aglarG\u00fcl\u00e7ehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetworkgrammars. InProc.ofNAACL,2016.\n[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850,2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition,pages770\u2013778,2016.\n[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJ\u00fcrgenSchmidhuber. Gradientflowin\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780,1997.\n[14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations\nacrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages832\u2013841.ACL,August2009.\n[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\n[16] \u0141ukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural\nInformationProcessingSystems,(NIPS),2016.\n[17] \u0141ukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\nonLearningRepresentations(ICLR),2016.\n[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\n2017.\n[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\nInInternationalConferenceonLearningRepresentations,2017.\n[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\n[21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\narXiv:1703.10722,2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130,2017.\n[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\nsequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\n[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\n11", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 11}}, {"text": "[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\ncorpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313\u2013330,1993.\n[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\npages152\u2013159.ACL,June2006.\n[27] AnkurParikh,OscarT\u00e4ckstr\u00f6m,DipanjanDas,andJakobUszkoreit. Adecomposableattention\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\n[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\nsummarization. arXivpreprintarXiv:1705.04304,2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433\u2013440.ACL,July\n2006.\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\npreprintarXiv:1608.05859,2016.\n[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.\n[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\nlayer. arXivpreprintarXiv:1701.06538,2017.\n[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\nLearningResearch,15(1):1929\u20131958,2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440\u20132448.CurranAssociates,\nInc.,2015.\n[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104\u20133112,2014.\n[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\n[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\nAdvancesinNeuralInformationProcessingSystems,2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google\u2019sneuralmachine\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\narXiv:1609.08144,2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n1: LongPapers),pages434\u2013443.ACL,August2013.\n12", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 12}}, {"text": "Input-Input Layer5\nAttentionVisualizations\ntI\ntI\nsi\nsi\nni\nni\nsiht\nsiht\ntirips\ntirips\ntaht\ntaht\na\na\nytirojam\nytirojam\nfo\nfo\nnaciremA\nnaciremA\nstnemnrevog\nstnemnrevog\nevah\nevah\ndessap\ndessap\nwen\nwen\nswal\nswal\necnis\necnis\n9002\n9002\ngnikam\ngnikam\neht\neht\nnoitartsiger\nnoitartsiger\nro\nro\ngnitov\ngnitov\nssecorp\nssecorp\nerom\nerom\ntluciffid\ntluciffid\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\ntheverb\u2018making\u2019,completingthephrase\u2018making...moredifficult\u2019. Attentionshereshownonlyfor\ntheword\u2018making\u2019. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\n13", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 13}}, {"text": "Input-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\nFullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword\u2018its\u2019forattentionheads5\nand6. Notethattheattentionsareverysharpforthisword.\n14", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 14}}, {"text": "Input-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\nsentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\natlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\n15", "metadata": {"paper_title": "1706.03762v7.pdf", "page_number": 15}}, {"text": "Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nPatrickLewis\u2020\u2021,EthanPerez(cid:63),\nAleksandraPiktus\u2020,FabioPetroni\u2020,VladimirKarpukhin\u2020,NamanGoyal\u2020,HeinrichK\u00fcttler\u2020,\nMikeLewis\u2020,Wen-tauYih\u2020,TimRockt\u00e4schel\u2020\u2021,SebastianRiedel\u2020\u2021,DouweKiela\u2020\n\u2020FacebookAIResearch;\u2021UniversityCollegeLondon;(cid:63)NewYorkUniversity;\nplewis@fb.com\nAbstract\nLargepre-trainedlanguagemodelshavebeenshowntostorefactualknowledge\nintheirparameters,andachievestate-of-the-artresultswhenfine-tunedondown-\nstreamNLPtasks. However,theirabilitytoaccessandpreciselymanipulateknowl-\nedgeisstilllimited,andhenceonknowledge-intensivetasks,theirperformance\nlagsbehindtask-specificarchitectures.Additionally,providingprovenancefortheir\ndecisionsandupdatingtheirworldknowledgeremainopenresearchproblems. Pre-\ntrainedmodelswithadifferentiableaccessmechanismtoexplicitnon-parametric\nmemoryhavesofarbeenonlyinvestigatedforextractivedownstreamtasks. We\nexploreageneral-purposefine-tuningrecipeforretrieval-augmentedgeneration\n(RAG)\u2014modelswhichcombinepre-trainedparametricandnon-parametricmem-\nory for language generation. We introduce RAG models where the parametric\nmemoryisapre-trainedseq2seqmodelandthenon-parametricmemoryisadense\nvectorindexofWikipedia,accessedwithapre-trainedneuralretriever. Wecom-\nparetwoRAGformulations,onewhichconditionsonthesameretrievedpassages\nacrossthewholegeneratedsequence,andanotherwhichcanusedifferentpassages\npertoken. Wefine-tuneandevaluateourmodelsonawiderangeofknowledge-\nintensiveNLPtasksandsetthestateoftheartonthreeopendomainQAtasks,\noutperformingparametricseq2seqmodelsandtask-specificretrieve-and-extract\narchitectures. Forlanguagegenerationtasks,wefindthatRAGmodelsgenerate\nmorespecific,diverseandfactuallanguagethanastate-of-the-artparametric-only\nseq2seqbaseline.\n1 Introduction\nPre-trainedneurallanguagemodelshavebeenshowntolearnasubstantialamountofin-depthknowl-\nedgefromdata[47]. Theycandosowithoutanyaccesstoanexternalmemory,asaparameterized\nimplicitknowledgebase[51,52]. Whilethisdevelopmentisexciting,suchmodelsdohavedown-\nsides: Theycannoteasilyexpandorrevisetheirmemory,can\u2019tstraightforwardlyprovideinsightinto\ntheirpredictions,andmayproduce\u201challucinations\u201d[38]. Hybridmodelsthatcombineparametric\nmemorywithnon-parametric(i.e.,retrieval-based)memories[20,26,48]canaddresssomeofthese\nissuesbecauseknowledgecanbedirectlyrevisedandexpanded,andaccessedknowledgecanbe\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\ncombinemaskedlanguagemodels[8]withadifferentiableretriever,haveshownpromisingresults,\n1202\nrpA\n21\n]LC.sc[\n4v10411.5002:viXra", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 1}}, {"text": "Define \"middle ear\"(x) The middle ear includes\nEnd-to-End Backprop through q and p \u03b8 the tympanic cavity and\nQuestion Answering: the three ossicles. (y)\nQuestion Query\nE\nQ\nnc\nu\no\ne\nd\nry\ner\nRetriever p\n\u03b7\nDo\nI\nc\nn\nu\nd\nm\nex\nent Generator p\u03b8 Q\nA\nu\nn\ne\ns\ns\nw\nti\ne\no\nr\nn\nG\nA\ne\nn\nn\ns\ne\nw\nra\ne\nt\nr\ni\ni\no\nn\nn\ng:\n(Non-Parametric) (Parametric)\nB b a o r r a n c k i n O b H a a m w a a i w i a .( s x) q(x) d(z) z4 supports (y)\nFact Verification: Fact Query z3 Margin- Fact Verification:\nz2 alize Label Generation\nT C h o e m e D d i y v ( i x n ) e q MIPS z1 p \u03b8 T i h s i s d i 1 v 4 i t d h e d c e i n n t t u o r y 3 work\nJeopardy Question sections: \"Inferno\",\nGeneration: \"Purgatorio\" &\nAnswer Query \"Paradiso\" (y)\nQuestion Generation\nFigure1:Overviewofourapproach.Wecombineapre-trainedretriever(QueryEncoder+Document\nIndex)withapre-trainedseq2seqmodel(Generator)andfine-tuneend-to-end. Forqueryx,weuse\nMaximumInnerProductSearch(MIPS)tofindthetop-Kdocumentsz . Forfinalpredictiony,we\ni\ntreatzasalatentvariableandmarginalizeoverseq2seqpredictionsgivendifferentdocuments.\nbuthaveonlyexploredopen-domainextractivequestionanswering. Here,webringhybridparametric\nandnon-parametricmemorytothe\u201cworkhorseofNLP,\u201di.e. sequence-to-sequence(seq2seq)models.\nWeendowpre-trained,parametric-memorygenerationmodelswithanon-parametricmemorythrough\nageneral-purposefine-tuningapproachwhichwerefertoasretrieval-augmentedgeneration(RAG).\nWebuildRAGmodelswheretheparametricmemoryisapre-trainedseq2seqtransformer,andthe\nnon-parametricmemoryisadensevectorindexofWikipedia,accessedwithapre-trainedneural\nretriever. Wecombinethesecomponentsinaprobabilisticmodeltrainedend-to-end(Fig. 1). The\nretriever(DensePassageRetriever[26],henceforthDPR)provideslatentdocumentsconditionedon\ntheinput,andtheseq2seqmodel(BART[32])thenconditionsontheselatentdocumentstogetherwith\ntheinputtogeneratetheoutput. Wemarginalizethelatentdocumentswithatop-Kapproximation,\neitheronaper-outputbasis(assumingthesamedocumentisresponsibleforalltokens)oraper-token\nbasis(wheredifferentdocumentsareresponsiblefordifferenttokens). LikeT5[51]orBART,RAG\ncanbefine-tunedonanyseq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned.\nTherehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric\nmemorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack-\naugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\nparametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive\nknowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgeis\npresentwithoutadditionaltraining.\nOurresultshighlightthebenefitsofcombiningparametricandnon-parametricmemorywithgenera-\ntionforknowledge-intensivetasks\u2014tasksthathumanscouldnotreasonablybeexpectedtoperform\nwithoutaccesstoanexternalknowledgesource. OurRAGmodelsachievestate-of-the-artresults\nonopenNaturalQuestions[29],WebQuestions[3]andCuratedTrec[2]andstronglyoutperform\nrecentapproachesthatusespecialisedpre-trainingobjectivesonTriviaQA[24]. Despitethesebeing\nextractivetasks,wefindthatunconstrainedgenerationoutperformspreviousextractiveapproaches.\nForknowledge-intensivegeneration,weexperimentwithMS-MARCO[1]andJeopardyquestion\ngeneration, and we find that our models generate responses that are more factual, specific, and\ndiversethanaBARTbaseline. ForFEVER[56]factverification,weachieveresultswithin4.3%of\nstate-of-the-artpipelinemodelswhichusestrongretrievalsupervision. Finally,wedemonstratethat\nthenon-parametricmemorycanbereplacedtoupdatethemodels\u2019knowledgeastheworldchanges.1\n2 Methods\nWeexploreRAGmodels,whichusetheinputsequencextoretrievetextdocumentszandusethem\nas additional context when generating the target sequence y. As shown in Figure 1, our models\nleveragetwocomponents: (i)aretrieverp (z|x)withparameters\u03b7 thatreturns(top-Ktruncated)\n\u03b7\ndistributionsovertextpassagesgivenaqueryxand(ii)ageneratorp (y |x,z,y )parametrized\n\u03b8 i 1:i\u22121\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\nersLibrary[66]andcanbefoundathttps://github.com/huggingface/transformers/blob/master/\nexamples/rag/.AninteractivedemoofRAGmodelscanbefoundathttps://huggingface.co/rag/\n2", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 2}}, {"text": "by\u03b8thatgeneratesacurrenttokenbasedonacontextofthepreviousi\u22121tokensy ,theoriginal\n1:i\u22121\ninputxandaretrievedpassagez.\nTotraintheretrieverandgeneratorend-to-end,wetreattheretrieveddocumentasalatentvariable.\nWeproposetwomodelsthatmarginalizeoverthelatentdocumentsindifferentwaystoproducea\ndistributionovergeneratedtext. Inoneapproach,RAG-Sequence,themodelusesthesamedocument\ntopredicteachtargettoken. Thesecondapproach,RAG-Token,canpredicteachtargettokenbased\nonadifferentdocument. Inthefollowing,weformallyintroducebothmodelsandthendescribethe\np andp components,aswellasthetraininganddecodingprocedure.\n\u03b7 \u03b8\n2.1 Models\nRAG-SequenceModel TheRAG-Sequencemodelusesthesameretrieveddocumenttogenerate\nthecompletesequence. Technically,ittreatstheretrieveddocumentasasinglelatentvariablethat\nismarginalizedtogettheseq2seqprobabilityp(y|x)viaatop-Kapproximation. Concretely,the\ntopKdocumentsareretrievedusingtheretriever,andthegeneratorproducestheoutputsequence\nprobabilityforeachdocument,whicharethenmarginalized,\nN\n(cid:88) (cid:88) (cid:89)\np (y|x) \u2248 p (z|x)p (y|x,z) = p (z|x) p (y |x,z,y )\nRAG-Sequence \u03b7 \u03b8 \u03b7 \u03b8 i 1:i\u22121\nz\u2208top-k(p(\u00b7|x)) z\u2208top-k(p(\u00b7|x)) i\nRAG-TokenModel IntheRAG-Tokenmodelwecandrawadifferentlatentdocumentforeach\ntargettokenandmarginalizeaccordingly. Thisallowsthegeneratortochoosecontentfromseveral\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\nretriever,andthenthegeneratorproducesadistributionforthenextoutputtokenforeachdocument,\nbeforemarginalizing,andrepeatingtheprocesswiththefollowingoutputtoken,Formally,wedefine:\nN\n(cid:89) (cid:88)\np (y|x) \u2248 p (z|x)p (y |x,z,y )\nRAG-Token \u03b7 \u03b8 i 1:i\u22121\ni z\u2208top-k(p(\u00b7|x))\nFinally,wenotethatRAGcanbeusedforsequenceclassificationtasksbyconsideringthetargetclass\nasatargetsequenceoflengthone,inwhichcaseRAG-SequenceandRAG-Tokenareequivalent.\n2.2 Retriever: DPR\nTheretrievalcomponentp (z|x)isbasedonDPR[26]. DPRfollowsabi-encoderarchitecture:\n\u03b7\np (z|x)\u221dexp (cid:0) d(z)(cid:62)q(x) (cid:1) d(z)=BERT (z), q(x)=BERT (x)\n\u03b7 d q\nwhered(z)isadenserepresentationofadocumentproducedbyaBERT documentencoder[8],\nBASE\nandq(x)aqueryrepresentationproducedbyaqueryencoder,alsobasedonBERT . Calculating\nBASE\ntop-k(p (\u00b7|x)),thelistofkdocumentszwithhighestpriorprobabilityp (z|x),isaMaximumInner\n\u03b7 \u03b7\nProductSearch(MIPS)problem,whichcanbeapproximatelysolvedinsub-lineartime[23]. Weuse\napre-trainedbi-encoderfromDPRtoinitializeourretrieverandtobuildthedocumentindex. This\nretrieverwastrainedtoretrievedocumentswhichcontainanswerstoTriviaQA[24]questionsand\nNaturalQuestions[29]. Werefertothedocumentindexasthenon-parametricmemory.\n2.3 Generator: BART\nThegeneratorcomponentp (y |x,z,y )couldbemodelledusinganyencoder-decoder. Weuse\n\u03b8 i 1:i\u22121\nBART-large[32],apre-trainedseq2seqtransformer[58]with400Mparameters.Tocombinetheinput\nxwiththeretrievedcontentzwhengeneratingfromBART,wesimplyconcatenatethem. BARTwas\npre-trainedusingadenoisingobjectiveandavarietyofdifferentnoisingfunctions. Ithasobtained\nstate-of-the-artresultsonadiversesetofgenerationtasksandoutperformscomparably-sizedT5\nmodels[32]. WerefertotheBARTgeneratorparameters\u03b8astheparametricmemoryhenceforth.\n2.4 Training\nWe jointly train the retriever and generator components without any direct supervision on what\ndocumentshouldberetrieved. Givenafine-tuningtrainingcorpusofinput/outputpairs(x ,y ),we\nj j\n3", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 3}}, {"text": "(cid:80)\nminimizethenegativemarginallog-likelihoodofeachtarget, \u2212logp(y |x )usingstochastic\nj j j\ngradientdescentwithAdam[28]. UpdatingthedocumentencoderBERT duringtrainingiscostlyas\nd\nitrequiresthedocumentindextobeperiodicallyupdatedasREALMdoesduringpre-training[20].\nWe do not find this step necessary for strong performance, and keep the document encoder (and\nindex)fixed,onlyfine-tuningthequeryencoderBERT andtheBARTgenerator.\nq\n2.5 Decoding\nAttesttime,RAG-SequenceandRAG-Tokenrequiredifferentwaystoapproximateargmax p(y|x).\ny\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\ntor with transition probability: p(cid:48)(y |x,y ) = (cid:80) p (z |x)p (y |x,z ,y ) To\n\u03b8 i 1:i\u22121 z\u2208top-k(p(\u00b7|x)) \u03b7 i \u03b8 i i 1:i\u22121\ndecode,wecanplugp(cid:48)(y |x,y )intoastandardbeamdecoder.\n\u03b8 i 1:i\u22121\nRAG-Sequence ForRAG-Sequence,thelikelihoodp(y|x)doesnotbreakintoaconventionalper-\ntokenlikelihood,hencewecannotsolveitwithasinglebeamsearch. Instead,werunbeamsearchfor\neachdocumentz,scoringeachhypothesisusingp (y |x,z,y ). Thisyieldsasetofhypotheses\n\u03b8 i 1:i\u22121\nY,someofwhichmaynothaveappearedinthebeamsofalldocuments. Toestimatetheprobability\nof an hypothesis y we run an additional forward pass for each document z for which y does not\nappearinthebeam,multiplygeneratorprobabilitywithp (z|x)andthensumtheprobabilitiesacross\n\u03b7\nbeamsforthemarginals. Werefertothisdecodingprocedureas\u201cThoroughDecoding.\u201d Forlonger\noutputsequences,|Y|canbecomelarge,requiringmanyforwardpasses. Formoreefficientdecoding,\nwecanmakeafurtherapproximationthatp (y|x,z )\u22480whereywasnotgeneratedduringbeam\n\u03b8 i\nsearchfromx,z . ThisavoidstheneedtorunadditionalforwardpassesoncethecandidatesetY has\ni\nbeengenerated. Werefertothisdecodingprocedureas\u201cFastDecoding.\u201d\n3 Experiments\nWeexperimentwithRAGinawiderangeofknowledge-intensivetasks. Forallexperiments,weuse\nasingleWikipediadumpforournon-parametricknowledgesource. FollowingLeeetal.[31]and\nKarpukhinetal.[26],weusetheDecember2018dump. EachWikipediaarticleissplitintodisjoint\n100-wordchunks,tomakeatotalof21Mdocuments. Weusethedocumentencodertocomputean\nembeddingforeachdocument,andbuildasingleMIPSindexusingFAISS[23]withaHierarchical\nNavigableSmallWorldapproximationforfastretrieval[37]. Duringtraining,weretrievethetop\nkdocumentsforeachquery. Weconsiderk \u2208{5,10}fortrainingandsetkfortesttimeusingdev\ndata. Wenowdiscussexperimentaldetailsforeachtask.\n3.1 Open-domainQuestionAnswering\nOpen-domainquestionanswering(QA)isanimportantreal-worldapplicationandcommontestbed\nforknowledge-intensivetasks[20]. Wetreatquestionsandanswersasinput-outputtextpairs(x,y)\nandtrainRAGbydirectlyminimizingthenegativelog-likelihoodofanswers. WecompareRAGto\nthepopularextractiveQAparadigm[5,7,31,26],whereanswersareextractedspansfromretrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book\nQA\u201dapproaches[52],which,likeRAG,generateanswers,butwhichdonotexploitretrieval,instead\nrelyingpurelyonparametricknowledge.Weconsiderfourpopularopen-domainQAdatasets:Natural\nQuestions(NQ)[29],TriviaQA(TQA)[24]. WebQuestions(WQ)[3]andCuratedTrec(CT)[2]. As\nCTandWQaresmall,wefollowDPR[26]byinitializingCTandWQmodelswithourNQRAG\nmodel. Weusethesametrain/dev/testsplitsaspriorwork[31,26]andreportExactMatch(EM)\nscores. ForTQA,tocomparewithT5[52],wealsoevaluateontheTQAWikitestset.\n3.2 AbstractiveQuestionAnswering\nRAGmodelscangobeyondsimpleextractiveQAandanswerquestionswithfree-form,abstractive\ntextgeneration. TotestRAG\u2019snaturallanguagegeneration(NLG)inaknowledge-intensivesetting,\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\nretrieved from a search engine for each question, and a full sentence answer annotated from the\nretrievedpassages. Wedonotusethesuppliedpassages,onlythequestionsandanswers,totreat\n4", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 4}}, {"text": "MSMARCOasanopen-domainabstractiveQAtask. MSMARCOhassomequestionsthatcannotbe\nansweredinawaythatmatchesthereferenceanswerwithoutaccesstothegoldpassages,suchas\n\u201cWhatistheweatherinVolcano,CA?\u201dsoperformancewillbelowerwithoutusinggoldpassages.\nWealsonotethatsomeMSMARCOquestionscannotbeansweredusingWikipediaalone. Here,\nRAGcanrelyonparametricknowledgetogeneratereasonableresponses.\n3.3 JeopardyQuestionGeneration\nToevaluateRAG\u2019sgenerationabilitiesinanon-QAsetting,westudyopen-domainquestiongen-\neration. Ratherthanusequestionsfromstandardopen-domainQAtasks,whichtypicallyconsist\nofshort,simplequestions,weproposethemoredemandingtaskofgeneratingJeopardyquestions.\nJeopardyisanunusualformatthatconsistsoftryingtoguessanentityfromafactaboutthatentity.\nForexample,\u201cTheWorldCup\u201distheanswertothequestion\u201cIn1986Mexicoscoredasthefirst\ncountry to host this international sports competition twice.\u201d As Jeopardy questions are precise,\nfactualstatements,generatingJeopardyquestionsconditionedontheiranswerentitiesconstitutesa\nchallengingknowledge-intensivegenerationtask.\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\nthisisanewtask,wetrainaBARTmodelforcomparison. Following[67],weevaluateusingthe\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\nmatching entities and has higher correlation with human judgment for question generation than\nstandardmetrics. Wealsoperformtwohumanevaluations,onetoassessgenerationfactuality,and\noneforspecificity.Wedefinefactualityaswhetherastatementcanbecorroboratedbytrustedexternal\nsources,andspecificityashighmutualdependencebetweentheinputandoutput[33]. Wefollow\nbestpracticeandusepairwisecomparativeevaluation[34]. Evaluatorsareshownananswerandtwo\ngeneratedquestions,onefromBARTandonefromRAG.Theyarethenaskedtopickoneoffour\noptions\u2014quuestionAisbetter,questionBisbetter,botharegood,orneitherisgood.\n3.4 FactVerification\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\nwhethertheclaimistrue,false,orunverifiablefromWikipediaalone. FEVERisaretrievalproblem\ncoupledwithanchallengingentailmentreasoningtask. Italsoprovidesanappropriatetestbedfor\nexploringtheRAGmodels\u2019abilitytohandleclassificationratherthangeneration. WemapFEVER\nclasslabels(supports, refutes, ornotenoughinfo)tosingleoutputtokensanddirectlytrainwith\nclaim-classpairs. Crucially,unlikemostotherapproachestoFEVER,wedonotusesupervisionon\nretrievedevidence. Inmanyreal-worldapplications,retrievalsupervisionsignalsaren\u2019tavailable,and\nmodelsthatdonotrequiresuchsupervisionwillbeapplicabletoawiderrangeoftasks. Weexplore\ntwovariants: thestandard3-wayclassificationtask(supports/refutes/notenoughinfo)andthe2-way\n(supports/refutes)taskstudiedinThorneandVlachos[57]. Inbothcaseswereportlabelaccuracy.\n4 Results\n4.1 Open-domainQuestionAnswering\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks,RAGsetsanewstateoftheart(onlyontheT5-comparablesplitforTQA).RAGcombines\nthegenerationflexibilityofthe\u201cclosed-book\u201d(parametriconly)approachesandtheperformanceof\n\"open-book\"retrieval-basedapproaches. UnlikeREALMandT5+SSM,RAGenjoysstrongresults\nwithoutexpensive,specialized\u201csalientspanmasking\u201dpre-training[20]. ItisworthnotingthatRAG\u2019s\nretrieverisinitializedusingDPR\u2019sretriever,whichusesretrievalsupervisiononNaturalQuestions\nandTriviaQA.RAGcomparesfavourablytotheDPRQAsystem,whichusesaBERT-based\u201ccross-\nencoder\u201dtore-rankdocuments,alongwithanextractivereader. RAGdemonstratesthatneithera\nre-rankernorextractivereaderisnecessaryforstate-of-the-artperformance.\nThereareseveraladvantagestogeneratinganswersevenwhenitispossibletoextractthem. Docu-\nmentswithcluesabouttheanswerbutdonotcontaintheanswerverbatimcanstillcontributetowards\nacorrectanswerbeinggenerated,whichisnotpossiblewithstandardextractiveapproaches,leading\n5", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 5}}, {"text": "Table1:Open-DomainQATestScores.ForTQA, Table2:GenerationandclassificationTestScores.\nleft column uses the standard test set for Open- MS-MARCOSotAis[4],FEVER-3is[68]and\nDomain QA, right column uses the TQA-Wiki FEVER-2 is [57] *Uses gold context/evidence.\ntestset. SeeAppendixDforfurtherdetails. Bestmodelwithoutgoldaccessunderlined.\nModel NQ TQA WQ CT\nModel Jeopardy MSMARCO FVR3 FVR2\nClosed T5-11B[52] 34.5 - /50.1 37.4 -\nB-1 QB-1 R-L B-1 LabelAcc.\nBook T5-11B+SSM[52] 36.6 - /60.5 44.7 -\nSotA - - 49.8* 49.9* 76.8 92.2*\nOpen REALM[20] 40.4 - / - 40.7 46.8\nBook DPR[26] 41.5 57.9/ - 41.1 50.6 BART 15.1 19.7 38.2 41.6 64.0 81.1\nRAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Tok. 17.3 22.2 40.1 41.5\n72.5 89.5\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2 RAG-Seq. 14.7 21.4 40.8 44.2\ntomoreeffectivemarginalizationoverdocuments. Furthermore,RAGcangeneratecorrectanswers\nevenwhenthecorrectanswerisnotinanyretrieveddocument,achieving11.8%accuracyinsuch\ncasesforNQ,whereanextractivemodelwouldscore0%.\n4.2 AbstractiveQuestionAnswering\nAsshowninTable2,RAG-SequenceoutperformsBARTonOpenMS-MARCONLGby2.6Bleu\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\nimpressivegiventhat(i)thosemodelsaccessgoldpassageswithspecificinformationrequiredto\ngeneratethereferenceanswer,(ii)manyquestionsareunanswerablewithoutthegoldpassages,and\n(iii)notallquestionsareanswerablefromWikipediaalone. Table3showssomegeneratedanswers\nfromourmodels. Qualitatively,wefindthatRAGmodelshallucinatelessandgeneratefactually\ncorrecttextmoreoftenthanBART.Later,wealsoshowthatRAGgenerationsaremorediversethan\nBARTgenerations(see\u00a74.5).\n4.3 JeopardyQuestionGeneration\nTable2showsthatRAG-TokenperformsbetterthanRAG-SequenceonJeopardyquestiongeneration,\nwithbothmodelsoutperformingBARTonQ-BLEU-1. 4showshumanevaluationresults,over452\npairsofgenerationsfromBARTandRAG-Token. EvaluatorsindicatedthatBARTwasmorefactual\nthanRAGinonly7.1%ofcases,whileRAGwasmorefactualin42.7%ofcases,andbothRAGand\nBARTwerefactualinafurther17%ofcases,clearlydemonstratingtheeffectivenessofRAGon\nthetaskoverastate-of-the-artgenerationmodel. EvaluatorsalsofindRAGgenerationstobemore\nspecificbyalargemargin. Table3showstypicalgenerationsfromeachmodel.\nJeopardyquestionsoftencontaintwoseparatepiecesofinformation,andRAG-Tokenmayperform\nbestbecauseitcangenerateresponsesthatcombinecontentfromseveraldocuments. Figure2shows\nanexample. Whengenerating\u201cSun\u201d, theposteriorishighfordocument2whichmentions\u201cThe\nSun Also Rises\u201d. Similarly, document 1 dominates the posterior when \u201cA Farewell to Arms\u201d is\ngenerated. Intriguingly,afterthefirsttokenofeachbookisgenerated,thedocumentposteriorflattens.\nThisobservationsuggeststhatthegeneratorcancompletethetitleswithoutdependingonspecific\ndocuments. Inotherwords,themodel\u2019sparametricknowledgeissufficienttocompletethetitles. We\nfindevidenceforthishypothesisbyfeedingtheBART-onlybaselinewiththepartialdecoding\"The\nSun. BARTcompletesthegeneration\"TheSunAlsoRises\"isanovelbythisauthorof\"TheSun\nAlsoRises\"indicatingthetitle\"TheSunAlsoRises\"isstoredinBART\u2019sparameters. Similarly,\nBART will complete the partial decoding \"TheSunAlsoRises\"isanovelbythisauthorof\"A\nwith\"TheSunAlsoRises\"isanovelbythisauthorof\"AFarewelltoArms\". Thisexampleshows\nhowparametricandnon-parametricmemoriesworktogether\u2014thenon-parametriccomponenthelps\ntoguidethegeneration,drawingoutspecificknowledgestoredintheparametricmemory.\n4.4 FactVerification\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\nstate-of-the-artmodels,whicharecomplexpipelinesystemswithdomain-specificarchitecturesand\nsubstantialengineering,trainedusingintermediateretrievalsupervision,whichRAGdoesnotrequire.\n6", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 6}}, {"text": "Document1:hisworksareconsideredclassicsofAmerican Doc1\nliterature...Hiswartimeexperiencesformedthebasisforhisnovel Doc2\n\u201dAFarewelltoArms\u201d(1929)...\nDoc3\nDocument2:...artistsofthe1920s\u201dLostGeneration\u201dexpatriate\nDoc4\ncommunity.Hisdebutnovel,\u201dTheSunAlsoRises\u201d,waspublished\nin1926. Doc5\nB\nOS \u201d The Sun Also R ises \u201d is a novelby this authorof \u201d A Fare wellto\nAr\nms\u201d\nFigure2: RAG-Tokendocumentposteriorp(z |x,y ,y )foreachgeneratedtokenforinput\u201cHem-\ni i \u2212i\ningway\"forJeopardygenerationwith5retrieveddocuments. Theposteriorfordocument1ishigh\nwhengenerating\u201cAFarewelltoArms\"andfordocument2whengenerating\u201cTheSunAlsoRises\".\nTable3: Examplesfromgenerationtasks. RAGmodelsgeneratemorespecificandfactuallyaccurate\nresponses. \u2018?\u2019 indicatesfactuallyincorrectresponses,*indicatespartiallycorrectresponses.\nTask Input Model Generation\nBART ?Themiddleearisthepartoftheearbetweenthemiddleearandthenose.\ndefinemiddle\nRAG-T Themiddleearistheportionoftheearinternaltotheeardrum.\near\nMS- RAG-S Themiddleearincludesthetympaniccavityandthethreeossicles.\nMARCO\nwhatcurrency BART ThecurrencyneededinScotlandisPoundsterling.\nneededin RAG-T PoundisthecurrencyneededinScotland.\nscotland RAG-S ThecurrencyneededinScotlandisthepoundsterling.\nBART ?ThisstatehasthelargestnumberofcountiesintheU.S.\nJeopardy Washington RAG-T It\u2019stheonlyU.S.statenamedforaU.S.president\nQuestion RAG-S It\u2019sthestatewhereyou\u2019llfindMountRainierNationalPark\nGener\nBART *ThisepicpoembyDanteisdividedinto3parts:theInferno,thePurgatorio&thePurgatorio\n-ation TheDivine\nRAG-T Dante\u2019s\"Inferno\"isthefirstpartofthisepicpoem\nComedy\nRAG-S This14thcenturyworkisdividedinto3sections:\"Inferno\",\"Purgatorio\"&\"Paradiso\"\nFor2-wayclassification,wecompareagainstThorneandVlachos[57],whotrainRoBERTa[35]\ntoclassifytheclaimastrueorfalsegiventhegoldevidencesentence. RAGachievesanaccuracy\nwithin2.7%ofthismodel,despitebeingsuppliedwithonlytheclaimandretrievingitsownevidence.\nWealsoanalyzewhetherdocumentsretrievedbyRAGcorrespondtodocumentsannotatedasgold\nevidenceinFEVER.Wecalculatetheoverlapinarticletitlesbetweenthetopkdocumentsretrieved\nbyRAGandgoldevidenceannotations. Wefindthatthetopretrieveddocumentisfromagoldarticle\nin71%ofcases,andagoldarticleispresentinthetop10retrievedarticlesin90%ofcases.\n4.5 AdditionalResults\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specific than\nBARTforJeopardyquestiongeneration. Followingrecentworkondiversity-promotingdecoding\n[33,59,39],wealsoinvestigategenerationdiversitybycalculatingtheratioofdistinctngramsto\ntotalngramsgeneratedbydifferentmodels. Table5showsthatRAG-Sequence\u2019sgenerationsare\nmorediversethanRAG-Token\u2019s,andbotharesignificantlymorediversethanBARTwithoutneeding\nanydiversity-promotingdecoding.\nRetrievalAblations AkeyfeatureofRAGislearningtoretrieverelevantinformationforthetask.\nToassesstheeffectivenessoftheretrievalmechanism,werunablationswherewefreezetheretriever\nduringtraining. AsshowninTable6,learnedretrievalimprovesresultsforalltasks.\nWecompareRAG\u2019sdenseretrievertoawordoverlap-basedBM25retriever[53]. Here,wereplace\nRAG\u2019sretrieverwithafixedBM25system,anduseBM25retrievalscoresaslogitswhencalculating\np(z|x).Table6showstheresults.ForFEVER,BM25performsbest,perhapssinceFEVERclaimsare\nheavilyentity-centricandthuswell-suitedforwordoverlap-basedretrieval. Differentiableretrieval\nimprovesresultsonallothertasks,especiallyforOpen-DomainQA,whereitiscrucial.\nIndexhot-swapping Anadvantageofnon-parametricmemorymodelslikeRAGisthatknowledge\ncanbeeasilyupdatedattesttime. Parametric-onlymodelslikeT5orBARTneedfurthertrainingto\nupdatetheirbehaviorastheworldchanges. Todemonstrate,webuildanindexusingtheDrQA[5]\nWikipediadumpfromDecember2016andcompareoutputsfromRAGusingthisindextothenewer\nindexfromourmainresults(December2018). Wepreparealistof82worldleaderswhohadchanged\n7", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 7}}, {"text": "Table 4: Human assessments for the Jeopardy Table 5: Ratio of distinct to total tri-grams for\nQuestionGenerationTask. generationtasks.\nFactuality Specificity\nMSMARCO JeopardyQGen\nBARTbetter 7.1% 16.8%\nGold 89.6% 90.0%\nRAGbetter 42.7% 37.4%\nBART 70.7% 32.4%\nBothgood 11.7% 11.8%\nRAG-Token 77.8% 46.8%\nBothpoor 17.7% 6.9%\nRAG-Seq. 83.5% 53.8%\nNomajority 20.8% 20.1%\nTable6:Ablationsonthedevset.AsFEVERisaclassificationtask,bothRAGmodelsareequivalent.\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\nExactMatch B-1 QB-1 R-L B-1 LabelAccuracy\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4\n75.1 91.6\nRAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4\n72.9 89.4\nRAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4\n74.5 90.6\nRAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\nbetweenthesedatesanduseatemplate\u201cWhois{position}?\u201d (e.g. \u201cWhoisthePresidentofPeru?\u201d)\ntoqueryourNQRAGmodelwitheachindex. RAGanswers70%correctlyusingthe2016indexfor\n2016worldleadersand68%usingthe2018indexfor2018worldleaders. Accuracywithmismatched\nindicesislow(12%withthe2018indexand2016leaders,4%withthe2016indexand2018leaders).\nThisshowswecanupdateRAG\u2019sworldknowledgebysimplyreplacingitsnon-parametricmemory.\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\ndocuments,andwedonotobservesignificantdifferencesinperformancebetweenthem. Wehavethe\nflexibilitytoadjustthenumberofretrieveddocumentsattesttime,whichcanaffectperformanceand\nruntime. Figure3(left)showsthatretrievingmoredocumentsattesttimemonotonicallyimproves\nOpen-domainQAresultsforRAG-Sequence,butperformancepeaksforRAG-Tokenat10retrieved\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\nRAG-TokenattheexpenseofBleu-1,buttheeffectislesspronouncedforRAG-Sequence.\n44\n43\n42\n41\n40\n39\n10 20 30 40 50\nKRetrievedDocs\nhctaMtcaxEQN\n80\n70\n60\n50 RAG-Tok\nRAG-Seq 40\n10 20 30 40 50\nKRetrievedDocs\nK@llaceRrewsnAQN\n56\n54\n52 RAG-Tok\nRAG-Seq 50 FixedDPR\nBM25 48\n10 20 30 40 50\nKRetrievedDocs\nerocsL-eguoR/1-uelB\nRAG-TokR-L\nRAG-TokB-1\nRAG-SeqR-L\nRAG-SeqB-1\nFigure3: Left: NQperformanceasmoredocumentsareretrieved. Center: Retrievalrecallperfor-\nmanceinNQ.Right: MS-MARCOBleu-1andRouge-Lasmoredocumentsareretrieved.\n5 RelatedWork\nSingle-TaskRetrieval Priorworkhasshownthatretrievalimprovesperformanceacrossavarietyof\nNLPtaskswhenconsideredinisolation. Suchtasksincludeopen-domainquestionanswering[5,29],\nfact checking [56], fact completion [48], long-form question answering [12], Wikipedia article\ngeneration [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\nworkunifiesprevioussuccessesinincorporatingretrievalintoindividualtasks,showingthatasingle\nretrieval-basedarchitectureiscapableofachievingstrongperformanceacrossseveraltasks.\n8", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 8}}, {"text": "General-PurposeArchitecturesforNLP Priorworkongeneral-purposearchitecturesforNLP\ntaskshasshowngreatsuccesswithouttheuseofretrieval. Asingle, pre-trainedlanguagemodel\nhasbeenshowntoachievestrongperformanceonvariousclassificationtasksintheGLUEbench-\nmarks[60,61]afterfine-tuning[49,8].GPT-2[50]latershowedthatasingle,left-to-right,pre-trained\nlanguagemodelcouldachievestrongperformanceacrossbothdiscriminativeandgenerativetasks.\nForfurtherimprovement,BART[32]andT5[51,52]proposeasingle,pre-trainedencoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nandgenerativetasks. Ourworkaimstoexpandthespaceofpossibletaskswithasingle, unified\narchitecture,bylearningaretrievalmoduletoaugmentpre-trained,generativelanguagemodels.\nLearned Retrieval There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nworkoptimizestheretrievalmoduletoaidinaspecific,downstreamtasksuchasquestionanswering,\nusingsearch[46],reinforcementlearning[6,63,62],oralatentvariableapproach[31,20]asinour\nwork. Thesesuccessesleveragedifferentretrieval-basedarchitecturesandoptimizationtechniquesto\nachievestrongperformanceonasingletask,whileweshowthatasingleretrieval-basedarchitecture\ncanbefine-tunedforstrongperformanceonavarietyoftasks.\nMemory-basedArchitectures Ourdocumentindexcanbeseenasalargeexternalmemoryfor\nneuralnetworkstoattendto,analogoustomemorynetworks[64,55]. Concurrentwork[14]learns\ntoretrieveatrainedembeddingforeachentityintheinput,ratherthantoretrieverawtextasinour\nwork. Otherworkimprovestheabilityofdialogmodelstogeneratefactualtextbyattendingover\nfactembeddings[15,13]. Akeyfeatureofourmemoryisthatitiscomprisedofrawtextrather\ndistributedrepresentations,whichmakesthememoryboth(i)human-readable,lendingaformof\ninterpretabilitytoourmodel,and(ii)human-writable,enablingustodynamicallyupdatethemodel\u2019s\nmemorybyeditingthedocumentindex. Thisapproachhasalsobeenusedinknowledge-intensive\ndialog,wheregeneratorshavebeenconditionedonretrievedtextdirectly,albeitobtainedviaTF-IDF\nratherthanend-to-endlearntretrieval[9].\nRetrieve-and-Editapproaches Ourmethodsharessomesimilaritieswithretrieve-and-editstyle\napproaches,whereasimilartraininginput-outputpairisretrievedforagiveninput,andthenedited\ntoprovideafinaloutput. Theseapproacheshaveprovedsuccessfulinanumberofdomainsincluding\nMachineTranslation [18,22]andSemanticParsing[21].Ourapproachdoeshaveseveraldifferences,\nincludinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral\npiecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments\nratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and\ncouldrepresentpromisingfuturework.\n6 Discussion\nInthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric\nmemory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We\nfoundthatpeoplepreferRAG\u2019sgenerationoverpurelyparametricBART,findingRAGmorefactual\nandspecific. Weconductedanthoroughinvestigationofthelearnedretrievalcomponent,validating\nitseffectiveness,andweillustratedhowtheretrievalindexcanbehot-swappedtoupdatethemodel\nwithoutrequiringanyretraining.Infuturework,itmaybefruitfultoinvestigateifthetwocomponents\ncanbejointlypre-trainedfromscratch,eitherwithadenoisingobjectivesimilartoBARTorsome\nanotherobjective. Ourworkopensupnewresearchdirectionsonhowparametricandnon-parametric\nmemoriesinteractandhowtomosteffectivelycombinethem,showingpromiseinbeingappliedtoa\nwidevarietyofNLPtasks.\n9", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 9}}, {"text": "BroaderImpact\nThis work offers several positive societal benefits over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less\nwithgenerationsthataremorefactual,andoffersmorecontrolandinterpretability. RAGcouldbe\nemployedinawidevarietyofscenarioswithdirectbenefittosociety,forexamplebyendowingit\nwithamedicalindexandaskingitopen-domainquestionsonthattopic,orbyhelpingpeoplebemore\neffectiveattheirjobs.\nWiththeseadvantagesalsocomepotentialdownsides:Wikipedia,oranypotentialexternalknowledge\nsource,willprobablyneverbeentirelyfactualandcompletelydevoidofbias. SinceRAGcanbe\nemployedasalanguagemodel,similarconcernsasforGPT-2[50]arevalidhere,althougharguably\ntoalesserextent,includingthatitmightbeusedtogenerateabuse,fakedormisleadingcontentin\nthenewsoronsocialmedia;toimpersonateothers;ortoautomatetheproductionofspam/phishing\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\ncomingdecades[16]. Inordertomitigatetheserisks,AIsystemscouldbeemployedtofightagainst\nmisleadingcontentandautomatedspam/phishing.\nAcknowledgments\nTheauthorswouldliketothankthereviewersfortheirthoughtfulandconstructivefeedbackonthis\npaper,aswellasHuggingFacefortheirhelpinopen-sourcingcodetorunRAGmodels. Theauthors\nwouldalsoliketothankKyunghyunChoandSewonMinforproductivediscussionsandadvice. EP\nthankssupportsfromtheNSFGraduateResearchFellowship. PLissupportedbytheFAIRPhD\nprogram.\nReferences\n[1] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan\nMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\n[2] PetrBaudi\u0161andJan\u0160edivy`. Modelingofthequestionansweringtaskintheyodaqasystem. In\nInternationalConferenceoftheCross-LanguageEvaluationForumforEuropeanLanguages,\npages222\u2013228.Springer,2015. URLhttps://link.springer.com/chapter/10.1007%\n2F978-3-319-24027-5_20.\n[3] JonathanBerant,AndrewChou,RoyFrostig,andPercyLiang. SemanticParsingonFreebase\nfromQuestion-AnswerPairs. InProceedingsofthe2013ConferenceonEmpiricalMethods\ninNaturalLanguageProcessing,pages1533\u20131544,Seattle,Washington,USA,October2013.\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\nD13-1160.\n[4] BinBi,ChenliangLi,ChenWu,MingYan,andWeiWang. Palm: Pre-traininganautoencod-\ning&autoregressivelanguagemodelforcontext-conditionedgeneration. ArXiv,abs/2004.07159,\n2020. URLhttps://arxiv.org/abs/2004.07159.\n[5] DanqiChen,AdamFisch,JasonWeston,andAntoineBordes. ReadingWikipediatoAnswer\nOpen-DomainQuestions. InProceedingsofthe55thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages1870\u20131879,Vancouver,Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://www.aclweb.org/anthology/P17-1171.\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathanBerant. Coarse-to-finequestionansweringforlongdocuments. InProceedingsofthe\n55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),\npages209\u2013220,Vancouver,Canada,July2017.AssociationforComputationalLinguistics. doi:\n10.18653/v1/P17-1020. URLhttps://www.aclweb.org/anthology/P17-1020.\n10", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 10}}, {"text": "[7] ChristopherClarkandMattGardner. SimpleandEffectiveMulti-ParagraphReadingCompre-\nhension. arXiv:1710.10723[cs],October2017. URLhttp://arxiv.org/abs/1710.10723.\narXiv: 1710.10723.\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof\nDeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofthe2019Con-\nferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\nLanguageTechnologies,Volume1(LongandShortPapers),pages4171\u20134186,Minneapolis,\nMinnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423.\nURLhttps://www.aclweb.org/anthology/N19-1423.\n[9] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston. Wiz-\nardofwikipedia: Knowledge-poweredconversationalagents. InInternationalConferenceon\nLearningRepresentations,2019. URLhttps://openreview.net/forum?id=r1l73iRqKm.\n[10] MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyun\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\n1704.05179.\n[11] AngelaFan,MikeLewis,andYannDauphin. Hierarchicalneuralstorygeneration. InProceed-\ningsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:\nLongPapers),pages889\u2013898,Melbourne,Australia,July2018.AssociationforComputational\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\nP18-1082.\n[12] AngelaFan,YacineJernite,EthanPerez,DavidGrangier,JasonWeston,andMichaelAuli.ELI5:\nLongformquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation\nforComputationalLinguistics,pages3558\u20133567,Florence,Italy,July2019.Associationfor\nComputationalLinguistics. doi: 10.18653/v1/P19-1346. URLhttps://www.aclweb.org/\nanthology/P19-1346.\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\nwithKNN-basedcompositememory, 2020. URLhttps://openreview.net/forum?id=\nH1gx1CNKPH.\n[14] ThibaultF\u00e9vry,LivioBaldiniSoares,NicholasFitzGerald,EunsolChoi,andTomKwiatkowski.\nEntitiesasexperts: Sparsememoryaccesswithentitysupervision. ArXiv, abs/2004.07202,\n2020. URLhttps://arxiv.org/abs/2004.07202.\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\nConferenceonArtificialIntelligence,2018.URLhttps://www.aaai.org/ocs/index.php/\nAAAI/AAAI18/paper/view/16710.\n[16] KatjaGrace,JohnSalvatier,AllanDafoe,BaobaoZhang,andOwainEvans. WhenwillAI\nexceedhumanperformance? evidencefromAIexperts. CoRR,abs/1705.08807,2017. URL\nhttp://arxiv.org/abs/1705.08807.\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https:\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachinetranslation. In 32ndAAAIConferenceonArtificialIntelligence, AAAI 2018, 32nd\nAAAIConferenceonArtificialIntelligence,AAAI2018,pages5133\u20135140.AAAIpress,2018.\n32ndAAAIConferenceonArtificialIntelligence,AAAI2018;Conferencedate: 02-02-2018\nThrough07-02-2018.\n[19] KelvinGuu,TatsunoriB.Hashimoto,YonatanOren,andPercyLiang. Generatingsentencesby\neditingprototypes. TransactionsoftheAssociationforComputationalLinguistics,6:437\u2013450,\n2018. doi: 10.1162/tacl_a_00030. URLhttps://www.aclweb.org/anthology/Q18-1031.\n11", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 11}}, {"text": "[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\nRetrieval-augmentedlanguagemodelpre-training. ArXiv,abs/2002.08909,2020. URLhttps:\n//arxiv.org/abs/2002.08909.\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems 31, pages 10052\u2013\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\npdf.\n[22] NabilHossain,MarjanGhazvininejad,andLukeZettlemoyer. Simpleandeffectiveretrieve-\nedit-reranktextgeneration. InProceedingsofthe58thAnnualMeetingoftheAssociationfor\nComputationalLinguistics,pages2532\u20132538,Online,July2020.AssociationforComputa-\ntionalLinguistics. doi: 10.18653/v1/2020.acl-main.228. URLhttps://www.aclweb.org/\nanthology/2020.acl-main.228.\n[23] JeffJohnson,MatthijsDouze,andHerv\u00e9J\u00e9gou. Billion-scalesimilaritysearchwithgpus. arXiv\npreprintarXiv:1702.08734,2017. URLhttps://arxiv.org/abs/1702.08734.\n[24] MandarJoshi,EunsolChoi,DanielWeld,andLukeZettlemoyer. TriviaQA:ALargeScale\nDistantlySupervisedChallengeDatasetforReadingComprehension. InProceedingsofthe\n55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),\npages1601\u20131611,Vancouver,Canada,July2017.AssociationforComputationalLinguistics.\ndoi: 10.18653/v1/P17-1147. URLhttps://www.aclweb.org/anthology/P17-1147.\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\naugmented recurrent nets. In Proceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 1, NIPS\u201915, page 190\u2013198, Cam-\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\n[26] VladimirKarpukhin,BarlasOguz,SewonMin,LedellWu,SergeyEdunov,DanqiChen,and\nWen-tauYih. Densepassageretrievalforopen-domainquestionanswering. arXivpreprint\narXiv:2004.04906,2020. URLhttps://arxiv.org/abs/2004.04906.\n[27] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.Generaliza-\ntionthroughmemorization: Nearestneighborlanguagemodels. InInternationalConferenceon\nLearningRepresentations,2020. URLhttps://openreview.net/forum?id=HklBjCEKvH.\n[28] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua\nBengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,\nICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,2015. URL\nhttp://arxiv.org/abs/1412.6980.\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\ntion Answering Research. Transactions of the Association of Computational Lin-\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\nnatural-questions/main-1455-kwiatkowski.pdf.\n[30] GuillaumeLample,AlexandreSablayrolles,Marc\u2019AurelioRanzato,LudovicDenoyer,and\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d\u2019 Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\nformationProcessingSystems32,pages8548\u20138559.CurranAssociates,Inc.,2019. URLhttp:\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\n[31] KentonLee,Ming-WeiChang,andKristinaToutanova. Latentretrievalforweaklysupervised\nopendomainquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation\n12", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 12}}, {"text": "forComputationalLinguistics,pages6086\u20136096,Florence,Italy,July2019.Associationfor\nComputationalLinguistics. doi: 10.18653/v1/P19-1612. URLhttps://www.aclweb.org/\nanthology/P19-1612.\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmerLevy,VeselinStoyanov,andLukeZettlemoyer. BART:Denoisingsequence-to-sequence\npre-trainingfornaturallanguagegeneration,translation,andcomprehension. arXivpreprint\narXiv:1910.13461,2019. URLhttps://arxiv.org/abs/1910.13461.\n[33] JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan. Adiversity-promoting\nobjectivefunctionforneuralconversationmodels. InProceedingsofthe2016Conferenceofthe\nNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage\nTechnologies,pages110\u2013119,SanDiego,California,June2016.AssociationforComputational\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\nN16-1014.\n[34] MargaretLi, JasonWeston, andStephenRoller. Acute-eval: Improveddialogueevaluation\nwith optimizedquestions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\nhttps://arxiv.org/abs/1909.03087.\n[35] HairongLiu,MingboMa,LiangHuang,HaoXiong,andZhongjunHe. Robustneuralmachine\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\nMeetingoftheAssociationforComputationalLinguistics,pages3044\u20133049,Florence,Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\nhttps://www.aclweb.org/anthology/P19-1291.\n[36] PeterJ.Liu*,MohammadSaleh*,EtiennePot,BenGoodrich,RyanSepassi,LukaszKaiser,\nandNoamShazeer. Generatingwikipediabysummarizinglongsequences. InInternational\nConferenceonLearningRepresentations,2018. URLhttps://openreview.net/forum?\nid=Hyg0vbWC-.\n[37] YuryA.MalkovandD.A.Yashunin. Efficientandrobustapproximatenearestneighborsearch\nusinghierarchicalnavigablesmallworldgraphs. IEEETransactionsonPatternAnalysisand\nMachineIntelligence,42:824\u2013836,2016. URLhttps://arxiv.org/abs/1603.09320.\n[38] GaryMarcus. Thenextdecadeinai: fourstepstowardsrobustartificialintelligence. arXiv\npreprintarXiv:2002.06177,2020. URLhttps://arxiv.org/abs/2002.06177.\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\u00e4schel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nverifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https:\n//arxiv.org/abs/1911.03587.\n[40] PauliusMicikevicius,SharanNarang,JonahAlben,GregoryDiamos,ErichElsen,DavidGarcia,\nBorisGinsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh,andHaoWu. Mixed\nprecisiontraining. InICLR,2018. URLhttps://openreview.net/forum?id=r1gs9JgRZ.\n[41] NikitaMoghe,SiddharthaArora,SumanBanerjee,andMiteshM.Khapra. Towardsexploit-\ning background knowledge for building conversation systems. In Proceedings of the 2018\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2322\u20132332,Brus-\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1255. URLhttps://www.aclweb.org/anthology/D18-1255.\n[42] PrekshaNemaandMiteshM.Khapra.Towardsabettermetricforevaluatingquestiongeneration\nsystems. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing,pages3950\u20133959,Brussels,Belgium,October-November2018.Associationfor\nComputationalLinguistics. doi: 10.18653/v1/D18-1429. URLhttps://www.aclweb.org/\nanthology/D18-1429.\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nandLiDeng. MSMARCO:Ahumangeneratedmachinereadingcomprehensiondataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors,\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\n13", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 13}}, {"text": "approaches2016co-locatedwiththe30thAnnualConferenceonNeuralInformationProcessing\nSystems(NIPS2016),Barcelona,Spain,December9,2016,volume1773ofCEURWorkshop\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\n2016_paper9.pdf.\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\narXiv:1901.04085,2019. URLhttps://arxiv.org/abs/1901.04085.\n[45] MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,SamGross,NathanNg,DavidGrangier,\nandMichaelAuli. fairseq: Afast,extensibletoolkitforsequencemodeling. InProceedings\nofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational\nLinguistics(Demonstrations),pages48\u201353,Minneapolis,Minnesota,June2019.Association\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\norg/anthology/N19-4009.\n[46] EthanPerez,SiddharthKaramcheti,RobFergus,JasonWeston,DouweKiela,andKyunghyun\nCho. Findinggeneralizableevidencebylearningtoconvinceq&amodels. InProceedings\nofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n2402\u20132411,HongKong,China,November2019.AssociationforComputationalLinguistics.\ndoi: 10.18653/v1/D19-1244. URLhttps://www.aclweb.org/anthology/D19-1244.\n[47] FabioPetroni,TimRockt\u00e4schel,SebastianRiedel,PatrickLewis,AntonBakhtin,YuxiangWu,\nandAlexanderMiller. Languagemodelsasknowledgebases? InProceedingsofthe2019\nConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternational\nJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2463\u20132473,Hong\nKong,China,November2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/\nD19-1250. URLhttps://www.aclweb.org/anthology/D19-1250.\n[48] FabioPetroni,PatrickLewis,AleksandraPiktus,TimRockt\u00e4schel,YuxiangWu,AlexanderH.\nMiller,andSebastianRiedel. Howcontextaffectslanguagemodels\u2019factualpredictions. In\nAutomatedKnowledgeBaseConstruction,2020. URLhttps://openreview.net/forum?\nid=025X0zPfn.\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\nproving Language Understanding by Generative Pre-Training, 2018. URL\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf.\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners, 2019. URL\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf.\n[51] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,\nYanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified\ntext-to-texttransformer. arXive-prints,2019. URLhttps://arxiv.org/abs/1910.10683.\n[52] AdamRoberts, ColinRaffel, andNoamShazeer. Howmuchknowledgecanyoupackinto\ntheparametersofalanguagemodel? arXive-prints,2020. URLhttps://arxiv.org/abs/\n2002.08910.\n[53] StephenRobertsonandHugoZaragoza. Theprobabilisticrelevanceframework: Bm25and\nbeyond. Found.TrendsInf.Retr.,3(4):333\u2013389,April2009. ISSN1554-0669. doi: 10.1561/\n1500000019. URLhttps://doi.org/10.1561/1500000019.\n[54] IreneSolaiman,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeffWu,Alec\nRadford,andJian-BingWang. Releasestrategiesandthesocialimpactsoflanguagemodels.\nArXiv,abs/1908.09203,2019.\n[55] SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus. End-to-endmemorynet-\nworks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,Advances\ninNeuralInformationProcessingSystems28,pages2440\u20132448.CurranAssociates,Inc.,2015.\nURLhttp://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n14", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 14}}, {"text": "[56] JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal. FEVER:a\nlarge-scaledatasetforfactextractionandVERification. InProceedingsofthe2018Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguageTechnologies, Volume1(LongPapers), pages809\u2013819, NewOrleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\nhttps://www.aclweb.org/anthology/N18-1074.\n[57] JamesH.ThorneandAndreasVlachos. Avoidingcatastrophicforgettinginmitigatingmodel\nbiasesinsentence-pairclassificationwithelasticweightconsolidation. ArXiv,abs/2004.14366,\n2020. URLhttps://arxiv.org/abs/2004.14366.\n[58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\n\u0141ukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon,U.V.Luxburg,\nS.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural\nInformationProcessingSystems30,pages5998\u20136008.CurranAssociates,Inc.,2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\n[59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David\nCrandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes.\nAAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/17329.\n[60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeuralNetworksforNLP,pages353\u2013355,Brussels,Belgium,November2018.Associationfor\nComputationalLinguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/\nanthology/W18-5446.\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF.d\\textquotesingleAlch\u00e9-Buc,E.Fox,andR.Garnett,editors,AdvancesinNeuralInformation\nProcessing Systems 32, pages 3261\u20133275. Curran Associates, Inc., 2019. URL https://\narxiv.org/abs/1905.00537.\n[62] ShuohangWang,MoYu,XiaoxiaoGuo,ZhiguoWang,TimKlinger,WeiZhang,ShiyuChang,\n3\nGerryTesauro,BowenZhou,andJingJiang. R : Reinforcedranker-readerforopen-domain\nquestionanswering. InSheilaA.McIlraithandKilianQ.Weinberger,editors,Proceedingsof\ntheThirty-SecondAAAIConferenceonArtificialIntelligence,(AAAI-18),the30thinnovative\nApplicationsofArtificialIntelligence(IAAI-18),andthe8thAAAISymposiumonEducational\nAdvancesinArtificialIntelligence(EAAI-18), NewOrleans, Louisiana, USA,February2-7,\n2018, pages 5981\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/16712.\n[63] ShuohangWang,MoYu,JingJiang,WeiZhang,XiaoxiaoGuo,ShiyuChang,ZhiguoWang,\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\nrankinginopen-domainquestionanswering. InICLR,2018. URLhttps://openreview.\nnet/forum?id=rJl3yM-Ab.\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\nandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1410.3916.\n[65] JasonWeston,EmilyDinan,andAlexanderMiller. Retrieveandrefine: Improvedsequence\ngenerationmodelsfordialogue. InProceedingsofthe2018EMNLPWorkshopSCAI:The2nd\nInternationalWorkshoponSearch-OrientedConversationalAI,pages87\u201392,Brussels,Belgium,\nOctober2018.AssociationforComputationalLinguistics. doi: 10.18653/v1/W18-5713. URL\nhttps://www.aclweb.org/anthology/W18-5713.\n15", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 15}}, {"text": "[66] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony\nMoi, PierricCistac, TimRault, R\u00e9miLouf, MorganFuntowicz, JoeDavison, SamShleifer,\nPatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,Sylvain\nGugger,MariamaDrame,QuentinLhoest,andAlexanderM.Rush.Huggingface\u2019stransformers:\nState-of-the-artnaturallanguageprocessing. ArXiv,abs/1910.03771,2019.\n[67] ShiyueZhangandMohitBansal. Addressingsemanticdriftinquestiongenerationforsemi-\nsupervisedquestionanswering. InProceedingsofthe2019ConferenceonEmpiricalMeth-\nodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural\nLanguage Processing (EMNLP-IJCNLP), pages 2495\u20132509, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\nhttps://www.aclweb.org/anthology/D19-1253.\n[68] WanjunZhong,JingjingXu,DuyuTang,ZenanXu,NanDuan,MingZhou,JiahaiWang,and\nJianYin. Reasoningoversemantic-levelgraphforfactchecking. ArXiv,abs/1909.03745,2019.\nURLhttps://arxiv.org/abs/1909.03745.\n16", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 16}}, {"text": "Appendices for Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nA ImplementationDetails\nForOpen-domainQAwereporttestnumbersusing15retrieveddocumentsforRAG-Tokenmodels.\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\nThoroughDecodingapproachsinceanswersaregenerallyshort. WeusegreedydecodingforQAas\nwedidnotfindbeamsearchimprovedresults. ForOpen-MSMarcoandJeopardyquestiongeneration,\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\nandwealsotrainaBART-largemodelasabaseline. Weuseabeamsizeoffour,andusetheFast\nDecodingapproachforRAG-Sequencemodels,asThoroughDecodingdidnotimproveperformance.\nB HumanEvaluation\nFigure4: Annotationinterfaceforhumanevaluationoffactuality. Apop-outfordetailedinstructions\nandaworkedexampleappearwhenclicking\"viewtoolguide\".\nFigure 4showsthe userinterface forhuman evaluation. Toavoidanybiases forscreen position,\nwhichmodelcorrespondedtosentenceAandsentenceBwasrandomlyselectedforeachexample.\nAnnotatorswereencouragedtoresearchthetopicusingtheinternet,andweregivendetailedinstruc-\ntionsandworkedexamplesinafullinstructionstab. Weincludedsomegoldsentencesinorderto\nassesstheaccuracyoftheannotators. Twoannotatorsdidnotperformwellontheseexamplesand\ntheirannotationswereremovedfromtheresults.\nC TrainingsetupDetails\nWetrainallRAGmodelsandBARTbaselinesusingFairseq[45].2 Wetrainwithmixedprecision\nfloatingpointarithmetic[40],distributingtrainingacross8,32GBNVIDIAV100GPUs,though\ntrainingandinferencecanberunononeGPU.WefindthatdoingMaximumInnerProductSearch\nwithFAISSissufficientlyfastonCPU,sowestoredocumentindexvectorsonCPU,requiring\u223c100\nGBofCPUmemoryforallofWikipedia. Aftersubmission,WehaveportedourcodetoHuggingFace\nTransformers[66]3,whichachievesequivalentperformancetothepreviousversionbutisacleaner\nandeasiertouseimplementation. Thisversionisalsoopen-sourced. Wealsocompressthedocument\nindexusingFAISS\u2019scompressiontools,reducingtheCPUmemoryrequirementto36GB.Scriptsto\nrunexperimentswithRAGcanbefoundathttps://github.com/huggingface/transformers/\nblob/master/examples/rag/README.mdandaninteractivedemoofaRAGmodelcanbefound\nathttps://huggingface.co/rag/\n2https://github.com/pytorch/fairseq\n3https://github.com/huggingface/transformers\n17", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 17}}, {"text": "D FurtherDetailsonOpen-DomainQA\nForopen-domainQA,multipleanswerannotationsareoftenavailableforagivenquestion. These\nanswerannotationsareexploitedbyextractivemodelsduringtrainingastypicallyalltheanswer\nannotationsareusedtofindmatcheswithindocumentswhenpreparingtrainingdata. ForRAG,we\nalsomakeuseofmultipleannotationexamplesforNaturalQuestionsandWebQuestionsbytraining\nthemodelwitheach(q,a)pairseparately,leadingtoasmallincreaseinaccuracy. ForTriviaQA,\nthereareoftenmanyvalidanswerstoagivenquestion,someofwhicharenotsuitabletrainingtargets,\nsuchasemojiorspellingvariants. ForTriviaQA,wefilteroutanswercandidatesiftheydonotoccur\nintop1000documentsforthequery.\nCuratedTrecpreprocessing TheanswersforCuratedTrecaregivenintheformofregularexpres-\nsions,whichhasbeensuggestedasareasonwhyitisunsuitableforanswer-generationmodels[20].\nToovercomethis,weuseapre-processingstepwherewefirstretrievethetop1000documentsfor\neachquery,andusetheanswerthatmostfrequentlymatchestheregexpatternasthesupervision\ntarget. Ifnomatchesarefound,weresorttoasimpleheuristic: generateallpossiblepermutationsfor\neachregex,replacingnon-deterministicsymbolsintheregexnestedtreestructurewithawhitespace.\nTriviaQAEvaluationsetups Theopen-domainQAcommunitycustomarilyusespublicdevelop-\nmentdatasetsastestdatasets,astestdataforQAdatasetsisoftenrestrictedanddedicatedtoreading\ncompehensionpurposes. WereportourresultsusingthedatasetssplitsusedinDPR[26],whichare\nconsistentwithcommonpracticeinOpen-domainQA.ForTriviaQA,thistestdatasetisthepublic\nTriviaQAWebDevelopmentsplit. Robertsetal.[52]usedtheTriviaQAofficialWikipediatestset\ninstead. F\u00e9vryetal.[14]followthisconventioninordertocomparewithRobertsetal.[52](See\nappendixof[14]). Wereportresultsonbothtestsetstoenablefaircomparisontobothapproaches.\nWefindthatourperformanceismuchhigherusingtheofficialWikitestset,ratherthanthemore\nconventionalopen-domaintestset,whichweattributetotheofficialWikitestsetquestionsbeing\nsimplertoanswerfromWikipedia.\nE FurtherDetailsonFEVER\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and\nthenclassifyusingtherepresentationofthefinalhiddenstate,beforefinallymarginalizingacross\ndocumentstoobtaintheclassprobabilities. TheFEVERtasktraditionallyhastwosub-tasks. The\nfirstistoclassifytheclaimaseither\"Supported\",\"Refuted\"or\"NotEnoughInfo\",whichisthetask\nweexploreinthemainpaper. FEVER\u2019sothersub-taskinvolvesextractingsentencesfromWikipedia\nasevidencesupportingtheclassificationprediction. AsFEVERusesadifferentWikipediadumpto\nus,directlytacklingthistaskisnotstraightforward. Wehopetoaddressthisinfuturework.\nF NullDocumentProbabilities\nWeexperimentedwithadding\"Nulldocument\"mechanismtoRAG,similartoREALM[20]inorder\ntomodelcaseswherenousefulinformationcouldberetrievedforagiveninput. Here,ifkdocuments\nwereretrieved,wewouldadditionally\"retrieve\"anemptydocumentandpredictalogitforthenull\ndocument,beforemarginalizingoverk+1predictions. Weexploredmodellingthisnulldocument\nlogitbylearning(i)adocumentembeddingforthenulldocument,(ii)astaticlearntbiasterm,or\n(iii)aneuralnetworktopredictthelogit. Wedidnotfindthattheseimprovedperformance,soin\ntheinterestsofsimplicity,weomitthem. ForOpenMS-MARCO,whereusefulretrieveddocuments\ncannotalwaysberetrieved,weobservethatthemodellearnstoalwaysretrieveaparticularsetof\ndocumentsforquestionsthatarelesslikelytobenefitfromretrieval,suggestingthatnulldocument\nmechanismsmaynotbenecessaryforRAG.\nG Parameters\nOurRAGmodelscontainthetrainableparametersfortheBERT-basequeryanddocumentencoderof\nDPR,with110Mparameterseach(althoughwedonottrainthedocumentencoderourselves)and\n406MtrainableparametersfromBART-large,406Mparameters,makingatotalof626Mtrainable\n18", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 18}}, {"text": "Table7: Numberofinstancesinthedatasetsused. *Ahiddensubsetofthisdataisusedforevaluation\nTask Train Development Test\nNaturalQuestions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardyQuestionGeneration 97392 13714 26849\nMS-MARCO 153726 12468 101093*\nFEVER-3-way 145450 10000 10000\nFEVER-2-way 96966 6666 6666\nparameters. Thebestperforming\"closed-book\"(parametriconly)open-domainQAmodelisT5-11B\nwith11Billiontrainableparameters. TheT5modelwiththeclosestnumberofparameterstoour\nmodelsisT5-large(770Mparameters),whichachievesascoreof28.9EMonNaturalQuestions[52],\nsubstantiallybelowthe44.5thatRAG-Sequenceachieves,indicatingthathybridparametric/non-\nparametricmodelsrequirefarfewertrainableparametersforstrongopen-domainQAperformance.\nThenon-parametricmemoryindexdoesnotconsistoftrainableparameters,butdoesconsistsof21M\n728dimensionalvectors,consistingof15.3Bvalues. Thesecanbeeasilybestoredat8-bitfloating\npointprecisiontomanagememoryanddiskfootprints.\nH RetrievalCollapse\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\nretrievalcomponentwould\u201ccollapse\u201dandlearntoretrievethesamedocumentsregardlessofthe\ninput. Inthesecases,onceretrievalhadcollapsed,thegeneratorwouldlearntoignorethedocuments,\nandtheRAGmodelwouldperformequivalentlytoBART.Thecollapsecouldbeduetoaless-explicit\nrequirementforfactualknowledgeinsometasks,orthelongertargetsequences,whichcouldresult\ninlessinformativegradientsfortheretriever. Perezetal.[46]alsofoundspuriousretrievalresults\nwhenoptimizingaretrievalcomponentinordertoimproveperformanceondownstreamtasks.\nI Numberofinstancesperdataset\nThenumberoftraining,developmentandtestdatapointsineachofourdatasetsisshowninTable7.\n19", "metadata": {"paper_title": "2005.11401v4.pdf", "page_number": 19}}, {"text": "Language Models are Few-Shot Learners\nTomB.Brown\u2217 BenjaminMann\u2217 NickRyder\u2217 MelanieSubbiah\u2217\nJaredKaplan\u2020 PrafullaDhariwal ArvindNeelakantan PranavShyam GirishSastry\nAmandaAskell SandhiniAgarwal ArielHerbert-Voss GretchenKrueger TomHenighan\nRewonChild AdityaRamesh DanielM.Ziegler JeffreyWu ClemensWinter\nChristopherHesse MarkChen EricSigler MateuszLitwin ScottGray\nBenjaminChess JackClark ChristopherBerner\nSamMcCandlish AlecRadford IlyaSutskever DarioAmodei\nOpenAI\nAbstract\nRecentworkhasdemonstratedsubstantialgainsonmanyNLPtasksandbenchmarksbypre-training\nonalargecorpusoftextfollowedbyfine-tuningonaspecifictask. Whiletypicallytask-agnostic\ninarchitecture,thismethodstillrequirestask-specificfine-tuningdatasetsofthousandsortensof\nthousandsofexamples. Bycontrast,humanscangenerallyperformanewlanguagetaskfromonly\nafewexamplesorfromsimpleinstructions\u2013somethingwhichcurrentNLPsystemsstilllargely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-\ntuningapproaches. Specifically,wetrainGPT-3,anautoregressivelanguagemodelwith175billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwithtasksandfew-shotdemonstrationsspecifiedpurelyviatextinteractionwiththemodel. GPT-3\nachievesstrongperformanceonmanyNLPdatasets,includingtranslation,question-answering,and\nclozetasks,aswellasseveraltasksthatrequireon-the-flyreasoningordomainadaptation,suchas\nunscramblingwords,usinganovelwordinasentence,orperforming3-digitarithmetic. Atthesame\ntime,wealsoidentifysomedatasetswhereGPT-3\u2019sfew-shotlearningstillstruggles,aswellassome\ndatasetswhereGPT-3facesmethodologicalissuesrelatedtotrainingonlargewebcorpora. Finally,\nwefindthatGPT-3cangeneratesamplesofnewsarticleswhichhumanevaluatorshavedifficulty\ndistinguishingfromarticleswrittenbyhumans. Wediscussbroadersocietalimpactsofthisfinding\nandofGPT-3ingeneral.\n\u2217Equalcontribution\n\u2020JohnsHopkinsUniversity,OpenAI\nAuthorcontributionslistedatendofpaper.\n0202\nluJ\n22\n]LC.sc[\n4v56141.5002:viXra", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 1}}, {"text": "Contents\n1 Introduction 3\n2 Approach 6\n2.1 ModelandArchitectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 TrainingDataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.3 TrainingProcess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 Results 10\n3.1 LanguageModeling,Cloze,andCompletionTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2 ClosedBookQuestionAnswering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.4 Winograd-StyleTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.5 CommonSenseReasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.6 ReadingComprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.7 SuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.8 NLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.9 SyntheticandQualitativeTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4 MeasuringandPreventingMemorizationOfBenchmarks 29\n5 Limitations 33\n6 BroaderImpacts 34\n6.1 MisuseofLanguageModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n6.2 Fairness,Bias,andRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.3 EnergyUsage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n7 RelatedWork 39\n8 Conclusion 40\nA DetailsofCommonCrawlFiltering 43\nB DetailsofModelTraining 43\nC DetailsofTestSetContaminationStudies 43\nD TotalComputeUsedtoTrainLanguageModels 46\nE HumanQualityAssessmentofSyntheticNewsArticles 46\nF AdditionalSamplesfromGPT-3 48\nG DetailsofTaskPhrasingandSpecifications 50\nH ResultsonAllTasksforAllModelSizes 63\n2", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 2}}, {"text": "1 Introduction\nRecentyearshavefeaturedatrendtowardspre-trainedlanguagerepresentationsinNLPsystems,appliedinincreasingly\nflexibleandtask-agnosticwaysfordownstreamtransfer. First,single-layerrepresentationswerelearnedusingword\nvectors[MCCD13,PSM14]andfedtotask-specificarchitectures,thenRNNswithmultiplelayersofrepresentations\nandcontextualstatewereusedtoformstrongerrepresentations[DL15,MBXS17,PNZtY18](thoughstillappliedto\ntask-specificarchitectures),andmorerecentlypre-trainedrecurrentortransformerlanguagemodels[VSP+17]have\nbeendirectlyfine-tuned,entirelyremovingtheneedfortask-specificarchitectures[RNSS18,DCLT18,HR18].\nThislastparadigmhasledtosubstantialprogressonmanychallengingNLPtaskssuchasreadingcomprehension,\nquestionanswering,textualentailment,andmanyothers,andhascontinuedtoadvancebasedonnewarchitectures\nandalgorithms[RSR+19,LOG+19,YDY+19,LCG+19]. However,amajorlimitationtothisapproachisthatwhile\nthearchitectureistask-agnostic,thereisstillaneedfortask-specificdatasetsandtask-specificfine-tuning: toachieve\nstrongperformanceonadesiredtasktypicallyrequiresfine-tuningonadatasetofthousandstohundredsofthousands\nofexamplesspecifictothattask. Removingthislimitationwouldbedesirable,forseveralreasons.\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\napplicabilityoflanguagemodels. Thereexistsaverywiderangeofpossibleusefullanguagetasks, encompassing\nanythingfromcorrectinggrammar,togeneratingexamplesofanabstractconcept,tocritiquingashortstory. Formany\nofthesetasksitisdifficulttocollectalargesupervisedtrainingdataset,especiallywhentheprocessmustberepeated\nforeverynewtask.\nSecond,thepotentialtoexploitspuriouscorrelationsintrainingdatafundamentallygrowswiththeexpressiveness\nof the model and the narrowness of the training distribution. This can create problems for the pre-training plus\nfine-tuningparadigm,wheremodelsaredesignedtobelargetoabsorbinformationduringpre-training,butarethen\nfine-tunedonverynarrowtaskdistributions. Forinstance[HLW+20]observethatlargermodelsdonotnecessarily\ngeneralizebetterout-of-distribution.Thereisevidencethatsuggeststhatthegeneralizationachievedunderthisparadigm\ncanbepoorbecausethemodelisoverlyspecifictothetrainingdistributionanddoesnotgeneralizewelloutsideit\n[YdC+19,MPL19]. Thus,theperformanceoffine-tunedmodelsonspecificbenchmarks,evenwhenitisnominallyat\nhuman-level,mayexaggerateactualperformanceontheunderlyingtask[GSL+18,NK19].\nThird, humans do not require large supervised datasets to learn most language tasks \u2013 a brief directive in natural\nlanguage(e.g. \u201cpleasetellmeifthissentencedescribessomethinghappyorsomethingsad\u201d)oratmostatinynumber\nofdemonstrations(e.g. \u201cherearetwoexamplesofpeopleactingbrave;pleasegiveathirdexampleofbravery\u201d)isoften\nFigure1.1: Languagemodelmeta-learning. Duringunsupervisedpre-training,alanguagemodeldevelopsabroad\nsetofskillsandpatternrecognitionabilities. Itthenusestheseabilitiesatinferencetimetorapidlyadapttoorrecognize\nthedesiredtask. Weusetheterm\u201cin-contextlearning\u201dtodescribetheinnerloopofthisprocess,whichoccurswithin\ntheforward-passuponeachsequence. Thesequencesinthisdiagramarenotintendedtoberepresentativeofthedataa\nmodelwouldseeduringpre-training,butareintendedtoshowthattherearesometimesrepeatedsub-tasksembedded\nwithinasinglesequence.\n3", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 3}}, {"text": "Figure1.2:Largermodelsmakeincreasinglyefficientuseofin-contextinformation. Weshowin-contextlearning\nperformanceonasimpletaskrequiringthemodeltoremoverandomsymbolsfromaword,bothwithandwithouta\nnaturallanguagetaskdescription(seeSec.3.9.2).Thesteeper\u201cin-contextlearningcurves\u201dforlargemodelsdemonstrate\nimprovedabilitytolearnataskfromcontextualinformation. Weseequalitativelysimilarbehavioracrossawiderange\noftasks.\nsufficienttoenableahumantoperformanewtasktoatleastareasonabledegreeofcompetence. Asidefrompointing\ntoaconceptuallimitationinourcurrentNLPtechniques,thisadaptabilityhaspracticaladvantages\u2013itallowshumans\ntoseamlesslymixtogetherorswitchbetweenmanytasksandskills,forexampleperformingadditionduringalengthy\ndialogue. Tobebroadlyuseful,wewouldsomedaylikeourNLPsystemstohavethissamefluidityandgenerality.\nOnepotentialroutetowardsaddressingtheseissuesismeta-learning1\u2013whichinthecontextoflanguagemodelsmeans\nthemodeldevelopsabroadsetofskillsandpatternrecognitionabilitiesattrainingtime,andthenusesthoseabilities\natinferencetimetorapidlyadapttoorrecognizethedesiredtask(illustratedinFigure1.1). Recentwork[RWC+19]\nattemptstodothisviawhatwecall\u201cin-contextlearning\u201d,usingthetextinputofapretrainedlanguagemodelasaform\noftaskspecification: themodelisconditionedonanaturallanguageinstructionand/orafewdemonstrationsofthetask\nandisthenexpectedtocompletefurtherinstancesofthetasksimplybypredictingwhatcomesnext.\nWhileithasshownsomeinitialpromise,thisapproachstillachievesresultsfarinferiortofine-tuning\u2013forexample\n[RWC+19]achievesonly4%onNaturalQuestions,andevenits55F1CoQaresultisnowmorethan35pointsbehind\nthestateoftheart. Meta-learningclearlyrequiressubstantialimprovementinordertobeviableasapracticalmethodof\nsolvinglanguagetasks.\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\nlanguage models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters\n[DCLT18],to1.5billionparameters[RWC+19],to8billionparameters[SPP+19],11billionparameters[RSR+19],\nandfinally17billionparameters[Tur20]. Eachincreasehasbroughtimprovementsintextsynthesisand/ordownstream\nNLPtasks,andthereisevidencesuggestingthatlogloss,whichcorrelateswellwithmanydownstreamtasks,followsa\nsmoothtrendofimprovementwithscale[KMH+20]. Sincein-contextlearninginvolvesabsorbingmanyskillsand\ntaskswithintheparametersofthemodel,itisplausiblethatin-contextlearningabilitiesmightshowsimilarlystrong\ngainswithscale.\n1Inthecontextoflanguagemodelsthishassometimesbeencalled\u201czero-shottransfer\u201d,butthistermispotentiallyambiguous:\nthe method is \u201czero-shot\u201d in the sense that no gradient updates are performed, but it often involves providing inference-time\ndemonstrationstothemodel,soisnottrulylearningfromzeroexamples.Toavoidthisconfusion,weusetheterm\u201cmeta-learning\u201d\ntocapturetheinner-loop/outer-loopstructureofthegeneralmethod, andtheterm\u201cincontext-learning\u201dtorefertotheinner\nloopofmeta-learning. Wefurtherspecializethedescriptionto\u201czero-shot\u201d,\u201cone-shot\u201d,or\u201cfew-shot\u201ddependingonhowmany\ndemonstrationsareprovidedatinferencetime.Thesetermsareintendedtoremainagnosticonthequestionofwhetherthemodel\nlearnsnewtasksfromscratchatinferencetimeorsimplyrecognizespatternsseenduringtraining\u2013thisisanimportantissuewhich\nwediscusslaterinthepaper,but\u201cmeta-learning\u201disintendedtoencompassbothpossibilities,andsimplydescribestheinner-outer\nloopstructure.\n4", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 4}}, {"text": "Figure1.3: Aggregateperformanceforall42accuracy-denominatedbenchmarks Whilezero-shotperformance\nimprovessteadilywithmodelsize,few-shotperformanceincreasesmorerapidly,demonstratingthatlargermodelsare\nmoreproficientatin-contextlearning. SeeFigure3.8foramoredetailedanalysisonSuperGLUE,astandardNLP\nbenchmarksuite.\nInthispaper,wetestthishypothesisbytraininga175billionparameterautoregressivelanguagemodel,whichwecall\nGPT-3,andmeasuringitsin-contextlearningabilities. Specifically,weevaluateGPT-3onovertwodozenNLPdatasets,\naswellasseveralnoveltasksdesignedtotestrapidadaptationtotasksunlikelytobedirectlycontainedinthetraining\nset. Foreachtask,weevaluateGPT-3under3conditions: (a)\u201cfew-shotlearning\u201d,orin-contextlearningwherewe\nallowasmanydemonstrationsaswillfitintothemodel\u2019scontextwindow(typically10to100),(b)\u201cone-shotlearning\u201d,\nwhereweallowonlyonedemonstration,and(c)\u201czero-shot\u201dlearning,wherenodemonstrationsareallowedandonly\naninstructioninnaturallanguageisgiventothemodel. GPT-3couldalsoinprinciplebeevaluatedinthetraditional\nfine-tuningsetting,butweleavethistofuturework.\nFigure1.2illustratestheconditionswestudy,andshowsfew-shotlearningofasimpletaskrequiringthemodelto\nremoveextraneoussymbolsfromaword. Modelperformanceimproveswiththeadditionofanaturallanguagetask\ndescription,andwiththenumberofexamplesinthemodel\u2019scontext,K. Few-shotlearningalsoimprovesdramatically\nwithmodelsize. Thoughtheresultsinthiscaseareparticularlystriking,thegeneraltrendswithbothmodelsizeand\nnumberofexamplesin-contextholdformosttaskswestudy. Weemphasizethatthese\u201clearning\u201dcurvesinvolveno\ngradientupdatesorfine-tuning,justincreasingnumbersofdemonstrationsgivenasconditioning.\nBroadly,onNLPtasksGPT-3achievespromisingresultsinthezero-shotandone-shotsettings,andinthethefew-shot\nsettingissometimescompetitivewithorevenoccasionallysurpassesstate-of-the-art(despitestate-of-the-artbeingheld\nbyfine-tunedmodels). Forexample,GPT-3achieves81.5F1onCoQAinthezero-shotsetting,84.0F1onCoQAin\ntheone-shotsetting,85.0F1inthefew-shotsetting. Similarly,GPT-3achieves64.3%accuracyonTriviaQAinthe\nzero-shotsetting,68.0%intheone-shotsetting,and71.2%inthefew-shotsetting,thelastofwhichisstate-of-the-art\nrelativetofine-tunedmodelsoperatinginthesameclosed-booksetting.\nGPT-3alsodisplaysone-shotandfew-shotproficiencyattasksdesignedtotestrapidadaptionoron-the-flyreasoning,\nwhich include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\ndefinedonlyonce. Wealsoshowthatinthefew-shotsetting,GPT-3cangeneratesyntheticnewsarticleswhichhuman\nevaluatorshavedifficultydistinguishingfromhuman-generatedarticles.\nAtthesametime,wealsofindsometasksonwhichfew-shotperformancestruggles,evenatthescaleofGPT-3. This\nincludesnaturallanguageinferencetasksliketheANLIdataset,andsomereadingcomprehensiondatasetslikeRACE\norQuAC.BypresentingabroadcharacterizationofGPT-3\u2019sstrengthsandweaknesses,includingtheselimitations,we\nhopetostimulatestudyoffew-shotlearninginlanguagemodelsanddrawattentiontowhereprogressismostneeded.\nAheuristicsenseoftheoverallresultscanbeseeninFigure1.3,whichaggregatesthevarioustasks(thoughitshould\nnotbeseenasarigorousormeaningfulbenchmarkinitself).\n5", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 5}}, {"text": "Wealsoundertakeasystematicstudyof\u201cdatacontamination\u201d\u2013agrowingproblemwhentraininghighcapacitymodels\nondatasetssuchasCommonCrawl, whichcanpotentiallyincludecontentfromtestdatasetssimplybecausesuch\ncontentoftenexistsontheweb. Inthispaperwedevelopsystematictoolstomeasuredatacontaminationandquantify\nitsdistortingeffects. AlthoughwefindthatdatacontaminationhasaminimaleffectonGPT-3\u2019sperformanceonmost\ndatasets,wedoidentifyafewdatasetswhereitcouldbeinflatingresults,andweeitherdonotreportresultsonthese\ndatasetsorwenotethemwithanasterisk,dependingontheseverity.\nInadditiontoalltheabove,wealsotrainaseriesofsmallermodels(rangingfrom125millionparametersto13billion\nparameters)inordertocomparetheirperformancetoGPT-3inthezero,oneandfew-shotsettings. Broadly,formost\ntaskswefindrelativelysmoothscalingwithmodelcapacityinallthreesettings;onenotablepatternisthatthegap\nbetweenzero-,one-,andfew-shotperformanceoftengrowswithmodelcapacity,perhapssuggestingthatlargermodels\naremoreproficientmeta-learners.\nFinally,giventhebroadspectrumofcapabilitiesdisplayedbyGPT-3,wediscussconcernsaboutbias,fairness,and\nbroadersocietalimpacts,andattemptapreliminaryanalysisofGPT-3\u2019scharacteristicsinthisregard.\nTheremainderofthispaperisorganizedasfollows. InSection2,wedescribeourapproachandmethodsfortraining\nGPT-3andevaluatingit. Section3presentsresultsonthefullrangeoftasksinthezero-,one-andfew-shotsettings.\nSection4addressesquestionsofdatacontamination(train-testoverlap). Section5discusseslimitationsofGPT-3.\nSection6discussesbroaderimpacts. Section7reviewsrelatedworkandSection8concludes.\n2 Approach\nOurbasicpre-trainingapproach,includingmodel,data,andtraining,issimilartotheprocessdescribedin[RWC+19],\nwithrelativelystraightforwardscalingupofthemodelsize,datasetsizeanddiversity,andlengthoftraining. Ouruse\nofin-contextlearningisalsosimilarto[RWC+19],butinthisworkwesystematicallyexploredifferentsettingsfor\nlearningwithinthecontext. Therefore,westartthissectionbyexplicitlydefiningandcontrastingthedifferentsettings\nthatwewillbeevaluatingGPT-3onorcouldinprincipleevaluateGPT-3on. Thesesettingscanbeseenaslyingona\nspectrumofhowmuchtask-specificdatatheytendtorelyon. Specifically,wecanidentifyatleastfourpointsonthis\nspectrum(seeFigure2.1foranillustration):\n\u2022 Fine-Tuning(FT)hasbeenthemostcommonapproachinrecentyears,andinvolvesupdatingtheweightsof\napre-trainedmodelbytrainingonasuperviseddatasetspecifictothedesiredtask. Typicallythousandsto\nhundredsofthousandsoflabeledexamplesareused. Themainadvantageoffine-tuningisstrongperformance\nonmanybenchmarks. Themaindisadvantagesaretheneedforanewlargedatasetforeverytask,thepotential\nfor poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the\ntrainingdata[GSL+18,NK19],potentiallyresultinginanunfaircomparisonwithhumanperformance. In\nthisworkwedonotfine-tuneGPT-3becauseourfocusisontask-agnosticperformance,butGPT-3canbe\nfine-tunedinprincipleandthisisapromisingdirectionforfuturework.\n\u2022 Few-Shot(FS) is theterm wewill usein thisworkto referto thesetting wherethe modelis givena few\ndemonstrationsofthetaskatinferencetimeasconditioning[RWC+19],butnoweightupdatesareallowed.\nAsshowninFigure2.1,foratypicaldatasetanexamplehasacontextandadesiredcompletion(forexample\nanEnglishsentenceandtheFrenchtranslation),andfew-shotworksbygivingK examplesofcontextand\ncompletion,andthenonefinalexampleofcontext,withthemodelexpectedtoprovidethecompletion. We\ntypicallysetK intherangeof10to100asthisishowmanyexamplescanfitinthemodel\u2019scontextwindow\n(n =2048). Themainadvantagesoffew-shotareamajorreductionintheneedfortask-specificdataand\nctx\nreducedpotentialtolearnanoverlynarrowdistributionfromalargebutnarrowfine-tuningdataset. Themain\ndisadvantageisthatresultsfromthismethodhavesofarbeenmuchworsethanstate-of-the-artfine-tuned\nmodels. Also, a small amount of task specific data is still required. As indicated by the name, few-shot\nlearningasdescribedhereforlanguagemodelsisrelatedtofew-shotlearningasusedinothercontextsin\nML[HYC01,VBL+16]\u2013bothinvolvelearningbasedonabroaddistributionoftasks(inthiscaseimplicitin\nthepre-trainingdata)andthenrapidlyadaptingtoanewtask.\n\u2022 One-Shot(1S)isthesameasfew-shotexceptthatonlyonedemonstrationisallowed,inadditiontoanatural\nlanguagedescriptionofthetask,asshowninFigure1. Thereasontodistinguishone-shotfromfew-shotand\nzero-shot(below)isthatitmostcloselymatchesthewayinwhichsometasksarecommunicatedtohumans.\nForexample,whenaskinghumanstogenerateadatasetonahumanworkerservice(forexampleMechanical\nTurk),itiscommontogiveonedemonstrationofthetask. Bycontrastitissometimesdifficulttocommunicate\nthecontentorformatofataskifnoexamplesaregiven.\n6", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 6}}, {"text": "Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show\nfourmethodsforperformingataskwithalanguagemodel\u2013fine-tuningisthetraditionalmethod,whereaszero-,one-,\nandfew-shot, whichwestudyinthiswork, requirethemodeltoperformthetaskwithonlyforwardpassesattest\ntime. Wetypicallypresentthemodelwithafewdozenexamplesinthefewshotsetting. Exactphrasingsforalltask\ndescriptions,examplesandpromptscanbefoundinAppendixG.\n\u2022 Zero-Shot(0S)isthesameasone-shotexceptthatnodemonstrationsareallowed,andthemodelisonlygiven\nanaturallanguageinstructiondescribingthetask. Thismethodprovidesmaximumconvenience,potentialfor\nrobustness,andavoidanceofspuriouscorrelations(unlesstheyoccurverybroadlyacrossthelargecorpusof\npre-trainingdata),butisalsothemostchallengingsetting. Insomecasesitmayevenbedifficultforhumans\ntounderstandtheformatofthetaskwithoutpriorexamples,sothissettingisinsomecases\u201cunfairlyhard\u201d.\nForexample,ifsomeoneisaskedto\u201cmakeatableofworldrecordsforthe200mdash\u201d,thisrequestcanbe\nambiguous,asitmaynotbeclearexactlywhatformatthetableshouldhaveorwhatshouldbeincluded(and\nevenwithcarefulclarification,understandingpreciselywhatisdesiredcanbedifficult). Nevertheless,forat\nleastsomesettingszero-shotisclosesttohowhumansperformtasks\u2013forexample,inthetranslationexample\ninFigure2.1,ahumanwouldlikelyknowwhattodofromjustthetextinstruction.\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on\nzero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\nproblemsettingswhichofferavaryingtrade-offbetweenperformanceonspecificbenchmarksandsampleefficiency.\nWeespeciallyhighlightthefew-shotresultsasmanyofthemareonlyslightlybehindstate-of-the-artfine-tunedmodels.\nUltimately,however,one-shot,orevensometimeszero-shot,seemlikethefairestcomparisonstohumanperformance,\nandareimportanttargetsforfuturework.\nSections2.1-2.3belowgivedetailsonourmodels,trainingdata,andtrainingprocessrespectively. Section2.4discusses\nthedetailsofhowwedofew-shot,one-shot,andzero-shotevaluations.\n7", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 7}}, {"text": "ModelName n n d n d BatchSize LearningRate\nparams layers model heads head\nGPT-3Small 125M 12 768 12 64 0.5M 6.0\u00d710\u22124\nGPT-3Medium 350M 24 1024 16 64 0.5M 3.0\u00d710\u22124\nGPT-3Large 760M 24 1536 16 96 0.5M 2.5\u00d710\u22124\nGPT-3XL 1.3B 24 2048 24 128 1M 2.0\u00d710\u22124\nGPT-32.7B 2.7B 32 2560 32 80 1M 1.6\u00d710\u22124\nGPT-36.7B 6.7B 32 4096 32 128 2M 1.2\u00d710\u22124\nGPT-313B 13.0B 40 5140 40 128 2M 1.0\u00d710\u22124\nGPT-3175Bor\u201cGPT-3\u201d 175.0B 96 12288 96 128 3.2M 0.6\u00d710\u22124\nTable2.1: Sizes,architectures,andlearninghyper-parameters(batchsizeintokensandlearningrate)ofthemodels\nwhichwetrained. Allmodelsweretrainedforatotalof300billiontokens.\n2.1 ModelandArchitectures\nWeusethesamemodelandarchitectureasGPT-2[RWC+19],includingthemodifiedinitialization,pre-normalization,\nandreversibletokenizationdescribedtherein,withtheexceptionthatweusealternatingdenseandlocallybandedsparse\nattentionpatternsinthelayersofthetransformer,similartotheSparseTransformer[CGRS19].Tostudythedependence\nofMLperformanceonmodelsize,wetrain8differentsizesofmodel,rangingoverthreeordersofmagnitudefrom125\nmillionparametersto175billionparameters,withthelastbeingthemodelwecallGPT-3. Previouswork[KMH+20]\nsuggeststhatwithenoughtrainingdata,scalingofvalidationlossshouldbeapproximatelyasmoothpowerlawasa\nfunctionofsize;trainingmodelsofmanydifferentsizesallowsustotestthishypothesisbothforvalidationlossandfor\ndownstreamlanguagetasks.\nTable2.1showsthesizesandarchitecturesofour8models. Heren isthetotalnumberoftrainableparameters,\nparams\nn is the total number of layers, d is the number of units in each bottleneck layer (we always have the\nlayers model\nfeedforwardlayerfourtimesthesizeofthebottlenecklayer,d = 4\u2217d ),andd isthedimensionofeach\nff model head\nattentionhead. Allmodelsuseacontextwindowofn =2048tokens. WepartitionthemodelacrossGPUsalong\nctx\nboth the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\nparametersforeachmodelarechosenbasedoncomputationalefficiencyandload-balancinginthelayoutofmodels\nacrossGPU\u2019s. Previouswork[KMH+20]suggeststhatvalidationlossisnotstronglysensitivetotheseparameters\nwithinareasonablybroadrange.\n2.2 TrainingDataset\nDatasetsforlanguagemodelshaverapidlyexpanded,culminatingintheCommonCrawldataset2[RSR+19]constituting\nnearlyatrillionwords. Thissizeofdatasetissufficienttotrainourlargestmodelswithouteverupdatingonthesame\nsequencetwice. However,wehavefoundthatunfilteredorlightlyfilteredversionsofCommonCrawltendtohave\nlowerqualitythanmorecurateddatasets. Therefore,wetook3stepstoimprovetheaveragequalityofourdatasets:\n(1)wedownloadedandfilteredaversionofCommonCrawlbasedonsimilaritytoarangeofhigh-qualityreference\ncorpora,(2)weperformedfuzzydeduplicationatthedocumentlevel,withinandacrossdatasets,topreventredundancy\nandpreservetheintegrityofourheld-outvalidationsetasanaccuratemeasureofoverfitting,and(3)wealsoadded\nknownhigh-qualityreferencecorporatothetrainingmixtoaugmentCommonCrawlandincreaseitsdiversity.\nDetailsofthefirsttwopoints(processingofCommonCrawl)aredescribedinAppendixA.Forthethird,weadded\nseveralcuratedhigh-qualitydatasets, includinganexpandedversionoftheWebTextdataset[RWC+19], collected\nbyscrapinglinksoveralongerperiodoftime,andfirstdescribedin[KMH+20],twointernet-basedbookscorpora\n(Books1andBooks2)andEnglish-languageWikipedia.\nTable2.2showsthefinalmixtureofdatasetsthatweusedintraining. TheCommonCrawldatawasdownloadedfrom\n41shardsofmonthlyCommonCrawlcovering2016to2019,constituting45TBofcompressedplaintextbeforefiltering\nand570GBafterfiltering,roughlyequivalentto400billionbyte-pair-encodedtokens. Notethatduringtraining,datasets\narenotsampledinproportiontotheirsize,butratherdatasetsweviewashigher-qualityaresampledmorefrequently,\nsuchthatCommonCrawlandBooks2datasetsaresampledlessthanonceduringtraining,buttheotherdatasetsare\nsampled2-3times. Thisessentiallyacceptsasmallamountofoverfittinginexchangeforhigherqualitytrainingdata.\n2https://commoncrawl.org/the-data/\n8", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 8}}, {"text": "Figure2.2:Totalcomputeusedduringtraining.BasedontheanalysisinScalingLawsForNeuralLanguageModels\n[KMH+20]wetrainmuchlargermodelsonmanyfewertokensthanistypical. Asaconsequence,althoughGPT-33B\nisalmost10xlargerthanRoBERTa-Large(355Mparams),bothmodelstookroughly50petaflop/s-daysofcompute\nduringpre-training. MethodologyforthesecalculationscanbefoundinAppendixD.\nQuantity Weightin Epochselapsedwhen\nDataset (tokens) trainingmix trainingfor300Btokens\nCommonCrawl(filtered) 410billion 60% 0.44\nWebText2 19billion 22% 2.9\nBooks1 12billion 8% 1.9\nBooks2 55billion 8% 0.43\nWikipedia 3billion 3% 3.4\nTable2.2: DatasetsusedtotrainGPT-3. \u201cWeightintrainingmix\u201dreferstothefractionofexamplesduringtraining\nthataredrawnfromagivendataset,whichweintentionallydonotmakeproportionaltothesizeofthedataset. Asa\nresult,whenwetrainfor300billiontokens,somedatasetsareseenupto3.4timesduringtrainingwhileotherdatasets\nareseenlessthanonce.\nAmajormethodologicalconcernwithlanguagemodelspretrainedonabroadswathofinternetdata,particularlylarge\nmodelswiththecapacitytomemorizevastamountsofcontent,ispotentialcontaminationofdownstreamtasksby\nhavingtheirtestordevelopmentsetsinadvertentlyseenduringpre-training. Toreducesuchcontamination,wesearched\nforandattemptedtoremoveanyoverlapswiththedevelopmentandtestsetsofallbenchmarksstudiedinthispaper.\nUnfortunately,abuginthefilteringcausedustoignoresomeoverlaps,andduetothecostoftrainingitwasnotfeasible\ntoretrainthemodel. InSection4wecharacterizetheimpactoftheremainingoverlaps,andinfutureworkwewill\nmoreaggressivelyremovedatacontamination.\n2.3 TrainingProcess\nAsfoundin[KMH+20,MKAT18],largermodelscantypicallyusealargerbatchsize,butrequireasmallerlearning\nrate. Wemeasurethegradientnoisescaleduringtraininganduseittoguideourchoiceofbatchsize[MKAT18]. Table\n2.1showstheparametersettingsweused. Totrainthelargermodelswithoutrunningoutofmemory,weuseamixture\nofmodelparallelismwithineachmatrixmultiplyandmodelparallelismacrossthelayersofthenetwork. Allmodels\nweretrainedonV100GPU\u2019sonpartofahigh-bandwidthclusterprovidedbyMicrosoft. Detailsofthetrainingprocess\nandhyperparametersettingsaredescribedinAppendixB.\n9", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 9}}, {"text": "2.4 Evaluation\nForfew-shotlearning,weevaluateeachexampleintheevaluationsetbyrandomlydrawingK examplesfromthat\ntask\u2019strainingsetasconditioning,delimitedby1or2newlinesdependingonthetask. ForLAMBADAandStorycloze\nthereisnosupervisedtrainingsetavailablesowedrawconditioningexamplesfromthedevelopmentsetandevaluate\nonthetestset. ForWinograd(theoriginal,notSuperGLUEversion)thereisonlyonedataset,sowedrawconditioning\nexamplesdirectlyfromit.\nK canbeanyvaluefrom0tothemaximumamountallowedbythemodel\u2019scontextwindow,whichisn =2048\nctx\nforallmodelsandtypicallyfits10to100examples. LargervaluesofK areusuallybutnotalwaysbetter,sowhena\nseparatedevelopmentandtestsetareavailable,weexperimentwithafewvaluesofK onthedevelopmentsetandthen\nrunthebestvalueonthetestset. Forsometasks(seeAppendixG)wealsouseanaturallanguagepromptinadditionto\n(orforK =0,insteadof)demonstrations.\nOntasksthatinvolvechoosingonecorrectcompletionfromseveraloptions(multiplechoice),weprovideK examples\nof context plus correct completion, followed by one example of context only, and compare the LM likelihood of\neachcompletion. Formosttaskswecomparetheper-tokenlikelihood(tonormalizeforlength),howeveronasmall\nnumberofdatasets(ARC,OpenBookQA,andRACE)wegainadditionalbenefitasmeasuredonthedevelopmentset\nbynormalizingbytheunconditionalprobabilityofeachcompletion,bycomputing P(completion|context) ,where\nP(completion|answer context)\nanswer contextisthestring\"Answer: \"or\"A: \"andisusedtopromptthatthecompletionshouldbeananswer\nbutisotherwisegeneric.\nOntasksthatinvolvebinaryclassification,wegivetheoptionsmoresemanticallymeaningfulnames(e.g. \u201cTrue\u201dor\n\u201cFalse\u201dratherthan0or1)andthentreatthetasklikemultiplechoice;wealsosometimesframethetasksimilartowhat\nisdoneby[RSR+19](seeAppendixG)fordetails.\nOntaskswithfree-formcompletion,weusebeamsearchwiththesameparametersas[RSR+19]: abeamwidthof4\nandalengthpenaltyof\u03b1=0.6. WescorethemodelusingF1similarityscore,BLEU,orexactmatch,dependingon\nwhatisstandardforthedatasetathand.\nFinalresultsarereportedonthetestsetwhenpubliclyavailable,foreachmodelsizeandlearningsetting(zero-,one-,\nandfew-shot). Whenthetestsetisprivate,ourmodelisoftentoolargetofitonthetestserver,sowereportresultson\nthedevelopmentset. Wedosubmittothetestserveronasmallnumberofdatasets(SuperGLUE,TriviaQA,PiQa)\nwherewewereabletomakesubmissionwork,andwesubmitonlythe200Bfew-shotresults,andreportdevelopment\nsetresultsforeverythingelse.\n3 Results\nInFigure3.1wedisplaytrainingcurvesforthe8modelsdescribedinSection2. Forthisgraphwealsoinclude6\nadditionalextra-smallmodelswithasfewas100,000parameters. Asobservedin[KMH+20], languagemodeling\nperformancefollowsapower-lawwhenmakingefficientuseoftrainingcompute. Afterextendingthistrendbytwo\nmoreordersofmagnitude,weobserveonlyaslight(ifany)departurefromthepower-law. Onemightworrythatthese\nimprovementsincross-entropylosscomeonlyfrommodelingspuriousdetailsofourtrainingcorpus. However,wewill\nseeinthefollowingsectionsthatimprovementsincross-entropylossleadtoconsistentperformancegainsacrossa\nbroadspectrumofnaturallanguagetasks.\nBelow,weevaluatethe8modelsdescribedinSection2(the175billionparameterparameterGPT-3and7smaller\nmodels)onawiderangeofdatasets. Wegroupthedatasetsinto9categoriesrepresentingroughlysimilartasks.\nInSection3.1weevaluateontraditionallanguagemodelingtasksandtasksthataresimilartolanguagemodeling,\nsuchasClozetasksandsentence/paragraphcompletiontasks. InSection3.2weevaluateon\u201cclosedbook\u201dquestion\nanswering tasks: tasks which require using the information stored in the model\u2019s parameters to answer general\nknowledgequestions. InSection3.3weevaluatethemodel\u2019sabilitytotranslatebetweenlanguages(especiallyone-shot\nandfew-shot). InSection3.4weevaluatethemodel\u2019sperformanceonWinogradSchema-liketasks. InSection3.5we\nevaluateondatasetsthatinvolvecommonsensereasoningorquestionanswering. InSection3.6weevaluateonreading\ncomprehensiontasks,inSection3.7weevaluateontheSuperGLUEbenchmarksuite,andin3.8webrieflyexplore\nNLI.Finally,inSection3.9,weinventsomeadditionaltasksdesignedespeciallytoprobein-contextlearningabilities\u2013\nthesetasksfocusonon-the-flyreasoning,adaptationskills,oropen-endedtextsynthesis. Weevaluatealltasksinthe\nfew-shot,one-shot,andzero-shotsettings.\n10", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 10}}, {"text": "Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy\nvalidationloss)followsapower-lawtrendwiththeamountofcomputeusedfortraining. Thepower-lawbehavior\nobserved in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the\npredictedcurve. Forthisfigure,weexcludeembeddingparametersfromcomputeandparametercounts.\nSetting PTB\nSOTA(Zero-Shot) 35.8a\nGPT-3Zero-Shot 20.5\nTable3.1: Zero-shotresultsonPTBlanguagemodelingdataset. Manyothercommonlanguagemodelingdatasets\nareomittedbecausetheyarederivedfromWikipediaorothersourceswhichareincludedinGPT-3\u2019strainingdata.\na[RWC+19]\n3.1 LanguageModeling,Cloze,andCompletionTasks\nIn this section we test GPT-3\u2019s performance on the traditional task of language modeling, as well as related tasks\nthatinvolvepredictingasinglewordofinterest,completingasentenceorparagraph,orchoosingbetweenpossible\ncompletionsofapieceoftext.\n3.1.1 LanguageModeling\nWecalculatezero-shotperplexityonthePennTreeBank(PTB)[MKM+94]datasetmeasuredin[RWC+19]. Weomit\nthe4Wikipedia-relatedtasksinthatworkbecausetheyareentirelycontainedinourtrainingdata,andwealsoomitthe\none-billionwordbenchmarkduetoahighfractionofthedatasetbeingcontainedinourtrainingset. PTBescapesthese\nissuesduetopredatingthemoderninternet. OurlargestmodelsetsanewSOTAonPTBbyasubstantialmarginof15\npoints,achievingaperplexityof20.50. NotethatsincePTBisatraditionallanguagemodelingdatasetitdoesnothave\naclearseparationofexamplestodefineone-shotorfew-shotevaluationaround,sowemeasureonlyzero-shot.\n3.1.2 LAMBADA\nThe LAMBADA dataset [PKL+16] tests the modeling of long-range dependencies in text \u2013 the model is asked to\npredictthelastwordofsentenceswhichrequirereadingaparagraphofcontext. Ithasrecentlybeensuggestedthatthe\ncontinuedscalingoflanguagemodelsisyieldingdiminishingreturnsonthisdifficultbenchmark. [BHT+20]reflecton\nthesmall1.5%improvementachievedbyadoublingofmodelsizebetweentworecentstateoftheartresults([SPP+19]\n11", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 11}}, {"text": "LAMBADA LAMBADA StoryCloze HellaSwag\nSetting (acc) (ppl) (acc) (acc)\nSOTA 68.0a 8.63b 91.8c 85.6d\nGPT-3Zero-Shot 76.2 3.00 83.2 78.9\nGPT-3One-Shot 72.5 3.35 84.7 78.1\nGPT-3Few-Shot 86.4 1.92 87.7 79.3\nTable3.2: Performanceonclozeandcompletiontasks. GPT-3significantlyimprovesSOTAonLAMBADAwhile\nachievingrespectableperformanceontwodifficultcompletionpredictiondatasets. a[Tur20]b[RWC+19]c[LDL19]\nd[LCH+20]\nFigure3.2: OnLAMBADA,thefew-shotcapabilityoflanguagemodelsresultsinastrongboosttoaccuracy. GPT-3\n2.7BoutperformstheSOTA17BparameterTuring-NLG[Tur20]inthissetting,andGPT-3175Badvancesthestateof\ntheartby18%. Notezero-shotusesadifferentformatfromone-shotandfew-shotasdescribedinthetext.\nand[Tur20])andarguethat\u201ccontinuingtoexpandhardwareanddatasizesbyordersofmagnitudeisnotthepath\nforward\u201d. Wefindthatpathisstillpromisingandinazero-shotsettingGPT-3achieves76%onLAMBADA,againof\n8%overthepreviousstateoftheart.\nLAMBADAisalsoademonstrationoftheflexibilityoffew-shotlearningasitprovidesawaytoaddressaproblemthat\nclassicallyoccurswiththisdataset. AlthoughthecompletioninLAMBADAisalwaysthelastwordinasentence,a\nstandardlanguagemodelhasnowayofknowingthisdetail. Itthusassignsprobabilitynotonlytothecorrectendingbut\nalsotoothervalidcontinuationsoftheparagraph. Thisproblemhasbeenpartiallyaddressedinthepastwithstop-word\nfilters[RWC+19](whichban\u201ccontinuation\u201dwords). Thefew-shotsettinginsteadallowsusto\u201cframe\u201dthetaskasa\ncloze-testandallowsthelanguagemodeltoinferfromexamplesthatacompletionofexactlyonewordisdesired. We\nusethefollowingfill-in-the-blankformat:\nAlicewasfriendswithBob. Alicewenttovisitherfriend . \u2192Bob\nGeorgeboughtsomebaseballequipment,aball,aglove,anda . \u2192\nWhenpresentedwithexamplesformattedthisway,GPT-3achieves86.4%accuracyinthefew-shotsetting,anincrease\nofover18%fromthepreviousstate-of-the-art. Weobservethatfew-shotperformanceimprovesstronglywithmodel\nsize. Whilethissettingdecreasestheperformanceofthesmallestmodelbyalmost20%,forGPT-3itimprovesaccuracy\nby10%. Finally,thefill-in-blankmethodisnoteffectiveone-shot,whereitalwaysperformsworsethanthezero-shot\nsetting. Perhapsthisisbecauseallmodelsstillrequireseveralexamplestorecognizethepattern.\n12", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 12}}, {"text": "Setting NaturalQS WebQS TriviaQA\nRAG(Fine-tuned,Open-Domain)[LPP+20] 44.5 45.5 68.0\nT5-11B+SSM(Fine-tuned,Closed-Book)[RRS20] 36.6 44.7 60.5\nT5-11B(Fine-tuned,Closed-Book) 34.5 37.4 50.1\nGPT-3Zero-Shot 14.6 14.4 64.3\nGPT-3One-Shot 23.0 25.3 68.0\nGPT-3Few-Shot 29.9 41.5 71.2\nTable3.3: ResultsonthreeOpen-DomainQAtasks. GPT-3isshowninthefew-,one-,andzero-shotsettings,as\ncomparedtopriorSOTAresultsforclosedbookandopendomainsettings. TriviaQAfew-shotresultisevaluatedonthe\nwikisplittestserver.\nOnenoteofcautionisthatananalysisoftestsetcontaminationidentifiedthatasignificantminorityoftheLAMBADA\ndatasetappearstobepresentinourtrainingdata\u2013howeveranalysisperformedinSection4suggestsnegligibleimpact\nonperformance.\n3.1.3 HellaSwag\nTheHellaSwagdataset[ZHB+19]involvespickingthebestendingtoastoryorsetofinstructions. Theexampleswere\nadversariallyminedtobedifficultforlanguagemodelswhileremainingeasyforhumans(whoachieve95.6%accuracy).\nGPT-3achieves78.1%accuracyintheone-shotsettingand79.3%accuracyinthefew-shotsetting,outperformingthe\n75.4%accuracyofafine-tuned1.5Bparameterlanguagemodel[ZHR+19]butstillafairamountlowerthantheoverall\nSOTAof85.6%achievedbythefine-tunedmulti-taskmodelALUM.\n3.1.4 StoryCloze\nWe next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH+16], which involves selecting the correct ending\nsentenceforfive-sentencelongstories. HereGPT-3achieves83.2%inthezero-shotsettingand87.7%inthefew-shot\nsetting(withK = 70). Thisisstill4.1%lowerthanthefine-tunedSOTAusingaBERTbasedmodel[LDL19]but\nimprovesoverpreviouszero-shotresultsbyroughly10%.\n3.2 ClosedBookQuestionAnswering\nInthissectionwemeasureGPT-3\u2019sabilitytoanswerquestionsaboutbroadfactualknowledge. Duetotheimmense\namountofpossiblequeries,thistaskhasnormallybeenapproachedbyusinganinformationretrievalsystemtofind\nrelevanttextincombinationwithamodelwhichlearnstogenerateananswergiventhequestionandtheretrieved\ntext. Sincethissettingallowsasystemtosearchforandconditionontextwhichpotentiallycontainstheanswerit\nisdenoted\u201copen-book\u201d. [RRS20]recentlydemonstratedthatalargelanguagemodelcanperformsurprisinglywell\ndirectlyansweringthequestionswithoutconditioningonauxilliaryinformation. Theydenotethismorerestrictive\nevaluationsettingas\u201cclosed-book\u201d. Theirworksuggeststhatevenhigher-capacitymodelscouldperformevenbetter\nandwetestthishypothesiswithGPT-3. WeevaluateGPT-3onthe3datasetsin[RRS20]:NaturalQuestions[KPR+19],\nWebQuestions[BCFL13],andTriviaQA[JCWZ17],usingthesamesplits. Notethatinadditiontoallresultsbeingin\ntheclosed-booksetting,ouruseoffew-shot,one-shot,andzero-shotevaluationsrepresentanevenstrictersettingthan\npreviousclosed-bookQAwork: inadditiontoexternalcontentnotbeingallowed,fine-tuningontheQ&Adatasetitself\nisalsonotpermitted.\nTheresultsforGPT-3areshowninTable3.3. OnTriviaQA,weachieve64.3%inthezero-shotsetting,68.0%inthe\none-shotsetting,and71.2%inthefew-shotsetting. Thezero-shotresultalreadyoutperformsthefine-tunedT5-11Bby\n14.2%,andalsooutperformsaversionwithQ&Atailoredspanpredictionduringpre-trainingby3.8%. Theone-shot\nresultimprovesby3.7%andmatchestheSOTAforanopen-domainQAsystemwhichnotonlyfine-tunesbutalso\nmakesuseofalearnedretrievalmechanismovera15.3Bparameterdensevectorindexof21Mdocuments[LPP+20].\nGPT-3\u2019sfew-shotresultfurtherimprovesperformanceanother3.2%beyondthis.\nOnWebQuestions(WebQs),GPT-3achieves14.4%inthezero-shotsetting,25.3%intheone-shotsetting,and41.5%\nin the few-shot setting. This compares to 37.4% for fine-tuned T5-11B, and 44.7% for fine-tuned T5-11B+SSM,\nwhich uses a Q&A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of\nstate-of-the-artfine-tunedmodels. Notably,comparedtoTriviaQA,WebQSshowsamuchlargergainfromzero-shotto\nfew-shot(andindeeditszero-shotandone-shotperformancearepoor),perhapssuggestingthattheWebQsquestions\n13", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 13}}, {"text": "Figure3.3: OnTriviaQAGPT3\u2019sperformancegrowssmoothlywithmodelsize,suggestingthatlanguagemodels\ncontinuetoabsorbknowledgeastheircapacityincreases. One-shotandfew-shotperformancemakesignificantgains\noverzero-shotbehavior,matchingandexceedingtheperformanceoftheSOTAfine-tunedopen-domainmodel,RAG\n[LPP+20]\nand/orthestyleoftheiranswersareout-of-distributionforGPT-3. Nevertheless,GPT-3appearsabletoadapttothis\ndistribution,recoveringstrongperformanceinthefew-shotsetting.\nOnNaturalQuestions(NQs)GPT-3achieves14.6%inthezero-shotsetting,23.0%intheone-shotsetting,and29.9%in\nthefew-shotsetting,comparedto36.6%forfine-tunedT511B+SSM.SimilartoWebQS,thelargegainfromzero-shot\nto few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to\nTriviaQAandWebQS.Inparticular,thequestionsinNQstendtowardsveryfine-grainedknowledgeonWikipedia\nspecificallywhichcouldbetestingthelimitsofGPT-3\u2019scapacityandbroadpretrainingdistribution.\nOverall,ononeofthethreedatasetsGPT-3\u2019sone-shotmatchestheopen-domainfine-tuningSOTA.Ontheothertwo\ndatasetsitapproachestheperformanceoftheclosed-bookSOTAdespitenotusingfine-tuning. Onall3datasets,we\nfindthatperformancescalesverysmoothlywithmodelsize(Figure3.3andAppendixHFigureH.7),possiblyreflecting\ntheideathatmodelcapacitytranslatesdirectlytomore\u2018knowledge\u2019absorbedintheparametersofthemodel.\n3.3 Translation\nForGPT-2afilterwasusedonamultilingualcollectionofdocumentstoproduceanEnglishonlydatasetduetocapacity\nconcerns. EvenwiththisfilteringGPT-2showedsomeevidenceofmultilingualcapabilityandperformednon-trivially\nwhentranslatingbetweenFrenchandEnglishdespiteonlytrainingon10megabytesofremainingFrenchtext. Sincewe\nincreasethecapacitybyovertwoordersofmagnitudefromGPT-2toGPT-3,wealsoexpandthescopeofthetraining\ndatasettoincludemorerepresentationofotherlanguages,thoughthisremainsanareaforfurtherimprovement. As\ndiscussedin2.2themajorityofourdataisderivedfromrawCommonCrawlwithonlyquality-basedfiltering. Although\nGPT-3\u2019strainingdataisstillprimarilyEnglish(93%bywordcount),italsoincludes7%oftextinotherlanguages.\nTheselanguagesaredocumentedinthesupplementalmaterial. Inordertobetterunderstandtranslationcapability,we\nalsoexpandouranalysistoincludetwoadditionalcommonlystudiedlanguages,GermanandRomanian.\nExistingunsupervisedmachinetranslationapproachesoftencombinepretrainingonapairofmonolingualdatasets\nwithback-translation[SHB15]tobridgethetwolanguagesinacontrolledway. Bycontrast, GPT-3learnsfroma\nblend of training data that mixes many languages together in a natural way, combining them on a word, sentence,\nanddocumentlevel. GPT-3alsousesasingletrainingobjectivewhichisnotcustomizedordesignedforanytaskin\nparticular. However,ourone/few-shotsettingsaren\u2019tstrictlycomparabletopriorunsupervisedworksincetheymake\nuseofasmallamountofpairedexamples(1or64). Thiscorrespondstouptoapageortwoofin-contexttrainingdata.\nResultsareshowninTable3.4. Zero-shotGPT-3,whichonlyreceivesonanaturallanguagedescriptionofthetask,\nstillunderperformsrecentunsupervisedNMTresults. However,providingonlyasingleexampledemonstrationfor\n14", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 14}}, {"text": "Setting En\u2192Fr Fr\u2192En En\u2192De De\u2192En En\u2192Ro Ro\u2192En\nSOTA(Supervised) 45.6a 35.0b 41.2c 40.2d 38.5e 39.9e\nXLM[LC19] 33.4 33.3 26.4 34.3 33.3 31.8\nMASS[STQ+19] 37.5 34.9 28.3 35.2 35.2 33.1\nmBART[LGG+20] - - 29.8 34.0 35.0 30.5\nGPT-3Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9\nGPT-3One-Shot 28.3 33.7 26.2 30.4 20.6 38.6\nGPT-3Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating\ninto English reflecting its strength as an English LM. We report BLEU scores on the WMT\u201914 Fr\u2194En,\nWMT\u201916 De\u2194En, and WMT\u201916 Ro\u2194En datasets as measured by multi-bleu.perl with XLM\u2019s tokeniza-\ntion in order to compare most closely with prior unsupervised NMT work. SacreBLEUf [Pos18] results re-\nported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA\nwith relative confidence. a[EOAG18] b[DHKH14] c[WXH+18] d[oR16] e[LGG+20] f[SacreBLEU signature:\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\nFigure3.4: Few-shottranslationperformanceon6languagepairsasmodelcapacityincreases. Thereisaconsistent\ntrendofimprovementacrossalldatasetsasthemodelscales,andaswellastendencyfortranslationintoEnglishtobe\nstrongerthantranslationfromEnglish.\n15", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 15}}, {"text": "Setting Winograd Winogrande(XL)\nFine-tunedSOTA 90.1a 84.6b\nGPT-3Zero-Shot 88.3* 70.2\nGPT-3One-Shot 89.7* 73.2\nGPT-3Few-Shot 88.6* 77.7\nTable3.5: ResultsontheWSC273versionofWinogradschemasandtheadversarialWinograndedataset. SeeSection\n4fordetailsonpotentialcontaminationoftheWinogradtestset. a[SBBC19]b[LYN+20]\nFigure3.5: Zero-,one-,andfew-shotperformanceontheadversarialWinograndedatasetasmodelcapacityscales.\nScalingisrelativelysmoothwiththegainstofew-shotlearningincreasingwithmodelsize,andfew-shotGPT-3175B\niscompetitivewithafine-tunedRoBERTA-large.\neach translation task improves performance by over 7 BLEU and nears competitive performance with prior work.\nGPT-3inthefullfew-shotsettingfurtherimprovesanother4BLEUresultinginsimilaraverageperformancetoprior\nunsupervisedNMTwork. GPT-3hasanoticeableskewinitsperformancedependingonlanguagedirection. Forthe\nthreeinputlanguagesstudied,GPT-3significantlyoutperformspriorunsupervisedNMTworkwhentranslatinginto\nEnglishbutunderperformswhentranslatingintheotherdirection. PerformanceonEn-Roisanoticeableoutlierat\nover10BLEUworsethanpriorunsupervisedNMTwork. Thiscouldbeaweaknessduetoreusingthebyte-levelBPE\ntokenizerofGPT-2whichwasdevelopedforanalmostentirelyEnglishtrainingdataset. ForbothFr-EnandDe-En,\nfewshotGPT-3outperformsthebestsupervisedresultwecouldfindbutduetoourunfamiliaritywiththeliteratureand\ntheappearancethattheseareun-competitivebenchmarkswedonotsuspectthoseresultsrepresenttruestateoftheart.\nForRo-En,fewshotGPT-3performswithin0.5BLEUoftheoverallSOTAwhichisachievedbyacombinationof\nunsupervisedpretraining,supervisedfinetuningon608Klabeledexamples,andbacktranslation[LHCG19b].\nFinally,acrossalllanguagepairsandacrossallthreesettings(zero-,one-,andfew-shot),thereisasmoothtrendof\nimprovementwithmodelcapacity. ThisisshowninFigure3.4inthecaseoffew-shotresults,andscalingforallthree\nsettingsisshowninAppendixH.\n3.4 Winograd-StyleTasks\nTheWinogradSchemasChallenge[LDM12]isaclassicaltaskinNLPthatinvolvesdeterminingwhichwordapronoun\nrefersto,whenthepronounisgrammaticallyambiguousbutsemanticallyunambiguoustoahuman. Recentlyfine-tuned\nlanguagemodelshaveachievednear-humanperformanceontheoriginalWinograddataset,butmoredifficultversions\n16", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 16}}, {"text": "Setting PIQA ARC(Easy) ARC(Challenge) OpenBookQA\nFine-tunedSOTA 79.4 92.0[KKS+20] 78.5[KKS+20] 87.2[KKS+20]\nGPT-3Zero-Shot 80.5* 68.8 51.4 57.6\nGPT-3One-Shot 80.5* 71.2 53.2 58.8\nGPT-3Few-Shot 82.8* 70.1 51.5 65.4\nTable3.6: GPT-3resultsonthreecommonsensereasoningtasks,PIQA,ARC,andOpenBookQA.GPT-3Few-Shot\nPIQAresultisevaluatedonthetestserver. SeeSection4fordetailsonpotentialcontaminationissuesonthePIQAtest\nset.\nFigure3.6: GPT-3resultsonPIQAinthezero-shot,one-shot,andfew-shotsettings. Thelargestmodelachievesa\nscoreonthedevelopmentsetinallthreeconditionsthatexceedsthebestrecordedscoreonthetask.\nsuchastheadversarially-minedWinograndedataset[SBBC19]stillsignificantlylaghumanperformance. Wetest\nGPT-3\u2019sperformanceonbothWinogradandWinogrande,asusualinthezero-,one-,andfew-shotsetting.\nOnWinogradwetestGPT-3ontheoriginalsetof273Winogradschemas,usingthesame\u201cpartialevaluation\u201dmethod\ndescribedin[RWC+19]. NotethatthissettingdiffersslightlyfromtheWSCtaskintheSuperGLUEbenchmark,which\nispresentedasbinaryclassificationandrequiresentityextractiontoconverttotheformdescribedinthissection. On\nWinogradGPT-3achieves88.3%,89.7%,and88.6%inthezero-shot,one-shot,andfew-shotsettings,showingnoclear\nin-contextlearningbutinallcasesachievingstrongresultsjustafewpointsbelowstate-of-the-artandestimatedhuman\nperformance. WenotethatcontaminationanalysisfoundsomeWinogradschemasinthetrainingdatabutthisappears\ntohaveonlyasmalleffectonresults(seeSection4).\nOn the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves 70.2% in the\nzero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a fine-tuned\nRoBERTAmodelachieves79%,state-of-the-artis84.6%achievedwithafine-tunedhighcapacitymodel(T5),and\nhumanperformanceonthetaskasreportedby[SBBC19]is94.0%.\n3.5 CommonSenseReasoning\nNextweconsiderthreedatasetswhichattempttocapturephysicalorscientificreasoning,asdistinctfromsentence\ncompletion,readingcomprehension,orbroadknowledgequestionanswering. Thefirst,PhysicalQA(PIQA)[BZB+19],\naskscommonsensequestionsabouthowthephysicalworldworksandisintendedasaprobeofgroundedunderstanding\nof the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot\n(thelastmeasuredonPIQA\u2019stestserver). Thiscomparesfavorablytothe79.4%accuracypriorstate-of-the-artofa\n17", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 17}}, {"text": "Setting CoQA DROP QuAC SQuADv2 RACE-h RACE-m\nFine-tunedSOTA 90.7a 89.1b 74.4c 93.0d 90.0e 93.1e\nGPT-3Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4\nGPT-3One-Shot 84.0 34.3 43.3 65.4 45.9 57.4\nGPT-3Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1\nTable3.7: Resultsonreadingcomprehensiontasks. AllscoresareF1exceptresultsforRACEwhichreportaccuracy.\na[JZC+19]b[JN20]c[AI19]d[QIA20]e[SPP+19]\nfine-tunedRoBERTa. PIQAshowsrelativelyshallowscalingwithmodelsizeandisstillover10%worsethanhuman\nperformance, butGPT-3\u2019sfew-shotandevenzero-shotresultoutperformthecurrentstate-of-the-art. Ouranalysis\nflaggedPIQAforapotentialdatacontaminationissue(despitehiddentestlabels),andwethereforeconservativelymark\ntheresultwithanasterisk. SeeSection4fordetails.\nARC [CCE+18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the\n\u201cChallenge\u201dversionofthedatasetwhichhasbeenfilteredtoquestionswhichsimplestatisticalorinformationretrieval\nmethodsareunabletocorrectlyanswer,GPT-3achieves51.4%accuracyinthezero-shotsetting,53.2%intheone-shot\nsetting,and51.5%inthefew-shotsetting. Thisisapproachingtheperformanceofafine-tunedRoBERTabaseline\n(55.9%)fromUnifiedQA[KKS+20]. Onthe\u201cEasy\u201dversionofthedataset(questionswhicheitherofthementioned\nbaselineapproachesansweredcorrectly),GPT-3achieves68.8%,71.2%,and70.1%whichslightlyexceedsafine-tuned\nRoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs\nachievedbytheUnifiedQAwhichexceedsGPT-3\u2019sfew-shotresultsby27%onthechallengesetand22%ontheeasy\nset.\nOnOpenBookQA[MCKS18],GPT-3improvessignificantlyfromzerotofewshotsettingsbutisstillover20points\nshort of the overall SOTA. GPT-3\u2019s few-shot performance is similar to a fine-tuned BERT Large baseline on the\nleaderboard.\nOverall,in-contextlearningwithGPT-3showsmixedresultsoncommonsensereasoningtasks,withonlysmalland\ninconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant\nimprovementisobservedonOpenBookQA.GPT-3setsSOTAonthenewPIQAdatasetinallevaluationsettings.\n3.6 ReadingComprehension\nNextweevaluateGPT-3onthetaskofreadingcomprehension. Weuseasuiteof5datasetsincludingabstractive,\nmultiplechoice,andspanbasedanswerformatsinbothdialogandsinglequestionsettings. Weobserveawidespread\ninGPT-3\u2019sperformanceacrossthesedatasetssuggestiveofvaryingcapabilitywithdifferentanswerformats. Ingeneral\nweobserveGPT-3isonparwithinitialbaselinesandearlyresultstrainedusingcontextualrepresentationsoneach\nrespectivedataset.\nGPT-3performsbest(within3pointsofthehumanbaseline)onCoQA[RCM19]afree-formconversationaldataset\nandperformsworst(13F1belowanELMobaseline)onQuAC[CHI+18]adatasetwhichrequiresmodelingstructured\ndialogactsandanswerspanselectionsofteacher-studentinteractions. OnDROP[DWD+19],adatasettestingdiscrete\nreasoningandnumeracyinthecontextofreadingcomprehension,GPT-3inafew-shotsettingoutperformsthefine-tuned\nBERTbaselinefromtheoriginalpaperbutisstillwellbelowbothhumanperformanceandstate-of-the-artapproaches\nwhichaugmentneuralnetworkswithsymbolicsystems[RLL+19]. OnSQuAD2.0[RJL18],GPT-3demonstratesits\nfew-shotlearningcapabilities,improvingbyalmost10F1(to69.8)comparedtoazero-shotsetting. Thisallowsitto\nslightlyoutperformthebestfine-tunedresultintheoriginalpaper. OnRACE[LXL+17],amultiplechoicedatasetof\nmiddleschoolandhighschoolenglishexaminations,GPT-3performsrelativelyweaklyandisonlycompetitivewith\ntheearliestworkutilizingcontextualrepresentationsandisstill45%behindSOTA.\n3.7 SuperGLUE\nInordertobetteraggregateresultsonNLPtasksandcomparetopopularmodelssuchasBERTandRoBERTaina\nmoresystematicway,wealsoevaluateGPT-3onastandardizedcollectionofdatasets,theSuperGLUEbenchmark\n[WPN+19] [WPN+19] [CLC+19] [DMST19] [RBG11] [KCR+18] [ZLL+18] [DGM06] [BHDD+06] [GMDD07]\n[BDD+09][PCC18][PHR+18]. GPT-3\u2019stest-setperformanceontheSuperGLUEdatasetisshowninTable3.8. Inthe\nfew-shotsetting,weused32examplesforalltasks,sampledrandomlyfromthetrainingset. ForalltasksexceptWSC\n18", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 18}}, {"text": "Figure3.7: GPT-3resultsonCoQAreadingcomprehensiontask. GPT-3175Bachieves85F1inthefew-shotsetting,\nonlyafewpointsbehindmeasuredhumanperformanceandstate-of-the-artfine-tunedmodels. Zero-shotandone-shot\nperformanceisafewpointsbehind,withthegainstofew-shotbeinglargestforbiggermodels.\nSuperGLUE BoolQ CB CB COPA RTE\nAverage Accuracy Accuracy F1 Accuracy Accuracy\nFine-tunedSOTA 89.0 91.0 96.9 93.9 94.8 92.5\nFine-tunedBERT-Large 69.0 77.4 83.6 75.7 70.6 71.7\nGPT-3Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0\nWiC WSC MultiRC MultiRC ReCoRD ReCoRD\nAccuracy Accuracy Accuracy F1a Accuracy F1\nFine-tunedSOTA 76.1 93.8 62.3 88.2 92.5 93.3\nFine-tunedBERT-Large 69.6 64.6 24.1 70.0 71.3 72.0\nGPT-3Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1\nTable3.8: PerformanceofGPT-3onSuperGLUEcomparedtofine-tunedbaselinesandSOTA.Allresultsarereported\nonthetestset. GPT-3few-shotisgivenatotalof32exampleswithinthecontextofeachtaskandperformsnogradient\nupdates.\n19", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 19}}, {"text": "Figure3.8: PerformanceonSuperGLUEincreaseswithmodelsizeandnumberofexamplesincontext.Avalue\nofK =32meansthatourmodelwasshown32examplespertask,for256examplestotaldividedacrossthe8tasksin\nSuperGLUE.WereportGPT-3valuesonthedevset,soournumbersarenotdirectlycomparabletothedottedreference\nlines(ourtestsetresultsareinTable3.8). TheBERT-Largereferencemodelwasfine-tunedontheSuperGLUEtraining\nset(125Kexamples),whereasBERT++wasfirstfine-tunedonMultiNLI(392Kexamples)andSWAG(113Kexamples)\nbefore further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the\ndifferenceinperformancebetweentheBERT-LargeandBERT++toberoughlyequivalenttothedifferencebetween\nGPT-3withoneexamplepercontextversuseightexamplespercontext.\nandMultiRC,wesampledanewsetofexamplestouseinthecontextforeachproblem. ForWSCandMultiRC,we\nusedthesamesetofrandomlydrawnexamplesfromthetrainingsetascontextforalloftheproblemsweevaluated.\nWeobserveawiderangeinGPT-3\u2019sperformanceacrosstasks. OnCOPAandReCoRDGPT-3achievesnear-SOTA\nperformance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving\nsecondplaceontheleaderboard,wherefirstplaceisheldbyafine-tuned11billionparametermodel(T5). OnWSC,\nperformanceisstillrelativelystrong,achieving80.1%inthefew-shotsetting(notethatGPT-3achieves88.6%onthe\noriginalWinograddatasetasdescribedinSection3.4). OnBoolQ,MultiRC,andRTE,performanceisreasonable,\nroughlymatchingthatofafine-tunedBERT-Large. OnCB,weseesignsoflifeat75.6%inthefew-shotsetting.\nWiCisanotableweakspotwithfew-shotperformanceat49.4%(atrandomchance). Wetriedanumberofdifferent\nphrasingsandformulationsforWiC(whichinvolvesdeterminingifawordisbeingusedwiththesamemeaningintwo\nsentences),noneofwhichwasabletoachievestrongperformance. Thishintsataphenomenonthatwillbecomeclearer\ninthenextsection(whichdiscussestheANLIbenchmark)\u2013GPT-3appearstobeweakinthefew-shotorone-shot\nsettingatsometasksthatinvolvecomparingtwosentencesorsnippets,forexamplewhetherawordisusedthesame\nwayintwosentences(WiC),whetheronesentenceisaparaphraseofanother,orwhetheronesentenceimpliesanother.\nThiscouldalsoexplainthecomparativelylowscoresforRTEandCB,whichalsofollowthisformat. Despitethese\nweaknesses,GPT-3stilloutperformsafine-tunedBERT-largeonfourofeighttasksandontwotasksGPT-3iscloseto\nthestate-of-the-artheldbyafine-tuned11billionparametermodel.\nFinally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of\nexamples in the context showing increasing benefits from in-context learning (Figure 3.8). We scale K up to 32\nexamplespertask,afterwhichpointadditionalexampleswillnotreliablyfitintoourcontext. Whensweepingover\nvaluesofK,wefindthatGPT-3requireslessthaneighttotalexamplespertasktooutperformafine-tunedBERT-Large\nonoverallSuperGLUEscore.\n3.8 NLI\nNaturalLanguageInference(NLI)[Fyo00]concernstheabilitytounderstandtherelationshipbetweentwosentences.\nInpractice, thistaskisusuallystructuredasatwoorthreeclassclassificationproblemwherethemodelclassifies\n20", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 20}}, {"text": "Figure3.9: PerformanceofGPT-3onANLIRound3. Resultsareonthedev-set,whichhasonly1500examples\nandthereforehashighvariance(weestimateastandarddeviationof1.2%). Wefindthatsmallermodelshoveraround\nrandomchance,whilefew-shotGPT-3175BclosesalmosthalfthegapfromrandomchancetoSOTA.Resultsfor\nANLIrounds1and2areshownintheappendix.\nwhetherthesecondsentencelogicallyfollowsfromthefirst,contradictsthefirstsentence,orispossiblytrue(neutral).\nSuperGLUEincludesanNLIdataset,RTE,whichevaluatesthebinaryversionofthetask. OnRTE,onlythelargest\nversionofGPT-3performsconvincinglybetterthanrandom(56%)inanyevaluationsetting,butinafew-shotsetting\nGPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced\nAdversarialNaturalLanguageInference(ANLI)dataset[NWD+19]. ANLIisadifficultdatasetemployingaseriesof\nadversariallyminednaturallanguageinferencequestionsinthreerounds(R1,R2,andR3). SimilartoRTE,allofour\nmodelssmallerthanGPT-3performatalmostexactlyrandomchanceonANLI,eveninthefew-shotsetting(\u223c33%),\nwhereasGPT-3itselfshowssignsoflifeonRound3. ResultsforANLIR3arehighlightedinFigure3.9andfullresults\nforallroundscanbefoundinAppendixH.TheseresultsonbothRTEandANLIsuggestthatNLIisstillaverydifficult\ntaskforlanguagemodelsandtheyareonlyjustbeginningtoshowsignsofprogress.\n3.9 SyntheticandQualitativeTasks\nOnewaytoprobeGPT-3\u2019srangeofabilitiesinthefew-shot(orzero-andone-shot)settingistogiveittaskswhich\nrequire it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have\noccurredintraining,oradaptquicklytoanunusualtask. Wedeviseseveraltaskstotestthisclassofabilities. First,we\ntestGPT-3\u2019sabilitytoperformarithmetic. Second,wecreateseveraltasksthatinvolverearrangingorunscramblingthe\nlettersinaword,taskswhichareunlikelytohavebeenexactlyseenduringtraining. Third,wetestGPT-3\u2019sabilityto\nsolveSAT-styleanalogyproblemsfew-shot. Finally,wetestGPT-3onseveralqualitativetasks,includingusingnew\nwordsinasentence,correctingEnglishgrammar,andnewsarticlegeneration. Wewillreleasethesyntheticdatasets\nwiththehopeofstimulatingfurtherstudyoftest-timebehavioroflanguagemodels.\n3.9.1 Arithmetic\nTotestGPT-3\u2019sabilitytoperformsimplearithmeticoperationswithouttask-specifictraining,wedevelopedasmall\nbatteryof10teststhatinvolveaskingGPT-3asimplearithmeticprobleminnaturallanguage:\n\u2022 2digitaddition(2D+)\u2013Themodelisaskedtoaddtwointegerssampleduniformlyfrom[0,100),phrasedin\ntheformofaquestion,e.g. \u201cQ:Whatis48plus76? A:124.\u201d\n\u2022 2digitsubtraction(2D-)\u2013Themodelisaskedtosubtracttwointegerssampleduniformlyfrom[0,100);the\nanswermaybenegative. Example: \u201cQ:Whatis34minus53? A:-19\u201d.\n\u2022 3digitaddition(3D+)\u2013Sameas2digitaddition,exceptnumbersareuniformlysampledfrom[0,1000).\n21", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 21}}, {"text": "Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a\nsignificantjumpfromthesecondlargestmodel(GPT-313B)tothelargestmodel(GPT-3175),withthelatterbeing\nabletoreliablyaccurate2digitarithmetic,usuallyaccurate3digitarithmetic,andcorrectanswersasignificantfraction\nofthetimeon4-5digitarithmetic,2digitmultiplication,andcompoundoperations. Resultsforone-shotandzero-shot\nareshownintheappendix.\n\u2022 3digitsubtraction(3D-)\u2013Sameas2digitsubtraction,exceptnumbersareuniformlysampledfrom[0,1000).\n\u2022 4digitaddition(4D+)\u2013Sameas3digitaddition,exceptuniformlysampledfrom[0,10000).\n\u2022 4digitsubtraction(4D-)\u2013Sameas3digitsubtraction,exceptuniformlysampledfrom[0,10000).\n\u2022 5digitaddition(5D+)\u2013Sameas3digitaddition,exceptuniformlysampledfrom[0,100000).\n\u2022 5digitsubtraction(5D-)\u2013Sameas3digitsubtraction,exceptuniformlysampledfrom[0,100000).\n\u2022 2digitmultiplication(2Dx)\u2013Themodelisaskedtomultiplytwointegerssampleduniformlyfrom[0,100),\ne.g. \u201cQ:Whatis24times42? A:1008\u201d.\n\u2022 One-digitcomposite(1DC)\u2013Themodelisaskedtoperformacompositeoperationonthree1digitnumbers,\nwithparenthesesaroundthelasttwo. Forexample,\u201cQ:Whatis6+(4*8)? A:38\u201d. Thethree1digitnumbers\nareselecteduniformlyon[0,10)andtheoperationsareselecteduniformlyfrom{+,-,*}.\nInall10tasksthemodelmustgeneratethecorrectanswerexactly. Foreachtaskwegenerateadatasetof2,000random\ninstancesofthetaskandevaluateallmodelsonthoseinstances.\nFirstweevaluateGPT-3inthefew-shotsetting,forwhichresultsareshowninFigure3.10. Onadditionandsubtraction,\nGPT-3displaysstrongproficiencywhenthenumberofdigitsissmall,achieving100%accuracyon2digitaddition,\n98.9%at2digitsubtraction,80.2%at3digitaddition,and94.2%at3-digitsubtraction. Performancedecreasesasthe\nnumberofdigitsincreases,butGPT-3stillachieves25-26%accuracyonfourdigitoperationsand9-10%accuracyon\nfivedigitoperations,suggestingatleastsomecapacitytogeneralizetolargernumbersofdigits. GPT-3alsoachieves\n29.2%accuracyat2digitmultiplication,anespeciallycomputationallyintensiveoperation. Finally,GPT-3achieves\n21.3%accuracyatsingledigitcombinedoperations(forexample,9*(7+5)),suggestingthatithassomerobustness\nbeyondjustsingleoperations.\nAsFigure3.10makesclear,smallmodelsdopoorlyonallofthesetasks\u2013eventhe13billionparametermodel(the\nsecondlargestafterthe175billionfullGPT-3)cansolve2digitadditionandsubtractiononlyhalfthetime,andall\notheroperationslessthan10%ofthetime.\nOne-shotandzero-shotperformancearesomewhatdegradedrelativetofew-shotperformance,suggestingthatadaptation\nto the task (or at the very least recognition of the task) is important to performing these computations correctly.\nNevertheless,one-shotperformanceisstillquitestrong,andevenzero-shotperformanceofthefullGPT-3significantly\n22", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 22}}, {"text": "Setting 2D+ 2D- 3D+ 3D- 4D+ 4D- 5D+ 5D- 2Dx 1DC\nGPT-3Zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8\nGPT-3One-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3\nGPT-3Few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3\nTable 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or\nsubtraction,2Dxis2digitmultiplication. 1DCis1digitcompositeoperations. Resultsbecomeprogressivelystronger\nmovingfromthezero-shottoone-shottofew-shotsetting,buteventhezero-shotshowssignificantarithmeticabilities.\nSetting CL A1 A2 RI RW\nGPT-3Zero-shot 3.66 2.28 8.91 8.26 0.09\nGPT-3One-shot 21.7 8.62 25.9 45.4 0.48\nGPT-3Few-shot 37.9 15.1 39.7 67.2 0.44\nTable3.10: GPT-3175Bperformanceonvariouswordunscramblingandwordmanipulationtasks,inzero-,one-,and\nfew-shotsettings. CLis\u201ccyclelettersinword\u201d,A1isanagramsofbutthefirstandlastletters,A2isanagramsofallbut\nthefirstandlasttwoletters,RIis\u201cRandominsertioninword\u201d,RWis\u201creversedwords\u201d.\noutperformsfew-shotlearningforallsmallermodels. AllthreesettingsforthefullGPT-3areshowninTable3.9,and\nmodelcapacityscalingforallthreesettingsisshowninAppendixH.\nTospot-checkwhetherthemodelissimplymemorizingspecificarithmeticproblems,wetookthe3-digitarithmetic\nproblems in our test set and searched for them in our training data in both the forms \"<NUM1> + <NUM2> =\" and\n\"<NUM1> plus <NUM2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000\nsubtractionproblemswefoundonly2matches(0.1%),suggestingthatonlyatrivialfractionofthecorrectanswers\ncouldhavebeenmemorized. Inaddition,inspectionofincorrectanswersrevealsthatthemodeloftenmakesmistakes\nsuch as not carrying a \u201c1\u201d, suggesting it is actually attempting to perform the relevant computation rather than\nmemorizingatable.\nOverall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even\nzero-shotsettings.\n3.9.2 WordScramblingandManipulationTasks\nTotestGPT-3\u2019sabilitytolearnnovelsymbolicmanipulationsfromafewexamples,wedesignedasmallbatteryof\n5 \u201ccharacter manipulation\u201d tasks. Each task involves giving the model a word distorted by some combination of\nscrambling,addition,ordeletionofcharacters,andaskingittorecovertheoriginalword. The5tasksare:\n\u2022 Cyclelettersinword(CL)\u2013Themodelisgivenawordwithitsletterscycled,thenthe\u201c=\u201dsymbol,and\nis expected to generate the original word. For example, it might be given \u201clyinevitab\u201d and should output\n\u201cinevitably\u201d.\n\u2022 Anagramsofallbutfirstandlastcharacters(A1)\u2013Themodelisgivenawordwhereeveryletterexcept\nthefirstandlasthavebeenscrambledrandomly,andmustoutputtheoriginalword. Example: criroptuon=\ncorruption.\n\u2022 Anagramsofallbutfirstandlast2characters(A2)\u2013Themodelisgivenawordwhereeveryletterexcept\nthefirst2andlast2havebeenscrambledrandomly,andmustrecovertheoriginalword. Example: opoepnnt\n\u2192opponent.\n\u2022 Randominsertioninword(RI)\u2013Arandompunctuationorspacecharacterisinsertedbetweeneachletter\nofaword,andthemodelmustoutputtheoriginalword. Example: s.u!c/c!e.ssi/o/n=succession.\n\u2022 Reversedwords(RW)\u2013Themodelisgivenawordspelledbackwards,andmustoutputtheoriginalword.\nExample: stcejbo\u2192objects.\nForeachtaskwegenerate10,000examples,whichwechosetobethetop10,000mostfrequentwordsasmeasuredby\n[Nor09]oflengthmorethan4charactersandlessthan15characters. Thefew-shotresultsareshowninFigure3.11.\nTaskperformancetendstogrowsmoothlywithmodelsize,withthefullGPT-3modelachieving66.9%onremoving\n23", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 23}}, {"text": "Figure3.11: Few-shotperformanceonthefivewordscramblingtasksfordifferentsizesofmodel. Thereisgenerally\nsmoothimprovementwithmodelsizealthoughtherandominsertiontaskshowsanupwardslopeofimprovementwith\nthe175Bmodelsolvingthetaskthemajorityofthetime. Scalingofone-shotandzero-shotperformanceisshownin\ntheappendix. AlltasksaredonewithK =100.\nrandominsertions,38.6%oncyclingletters,40.2%ontheeasieranagramtask,and15.1%onthemoredifficultanagram\ntask(whereonlythefirstandlastlettersareheldfixed). Noneofthemodelscanreversethelettersinaword.\nIntheone-shotsetting,performanceissignificantlyweaker(droppingbyhalformore),andinthezero-shotsettingthe\nmodelcanrarelyperformanyofthetasks(Table3.10). Thissuggeststhatthemodelreallydoesappeartolearnthese\ntasksattesttime,asthemodelcannotperformthemzero-shotandtheirartificialnaturemakesthemunlikelytoappear\ninthepre-trainingdata(althoughwecannotconfirmthiswithcertainty).\nWe can further quantify performance by plotting \u201cin-context learning curves\u201d, which show task performance as a\nfunctionofthenumberofin-contextexamples. Weshowin-contextlearningcurvesfortheSymbolInsertiontask\ninFigure1.2. Wecanseethatlargermodelsareabletomakeincreasinglyeffectiveuseofin-contextinformation,\nincludingbothtaskexamplesandnaturallanguagetaskdescriptions.\nFinally,itisworthaddingthatsolvingthesetasksrequirescharacter-levelmanipulations,whereasourBPEencoding\noperatesonsignificantfractionsofaword(onaverage\u223c0.7wordspertoken),sofromtheLM\u2019sperspectivesucceeding\natthesetasksinvolvesnotjustmanipulatingBPEtokensbutunderstandingandpullingaparttheirsubstructure. Also,\nCL,A1,andA2arenotbijective(thatis,theunscrambledwordisnotadeterministicfunctionofthescrambledword),\nrequiringthemodeltoperformsomesearchtofindthecorrectunscrambling. Thus,theskillsinvolvedappeartorequire\nnon-trivialpattern-matchingandcomputation.\n3.9.3 SATAnalogies\nTotestGPT-3onanothertaskthatissomewhatunusualrelativetothetypicaldistributionoftext,wecollectedasetof\n374\u201cSATanalogy\u201dproblems[TLBS03]. Analogiesareastyleofmultiplechoicequestionthatconstitutedasectionof\ntheSATcollegeentranceexambefore2005. Atypicalexampleis\u201caudaciousistoboldnessas(a)sanctimoniousisto\nhypocrisy,(b)anonymousistoidentity,(c)remorsefulistomisdeed,(d)deleteriousistoresult,(e)impressionableisto\ntemptation\u201d. Thestudentisexpectedtochoosewhichofthefivewordpairshasthesamerelationshipastheoriginal\nwordpair;inthisexampletheansweris\u201csanctimoniousistohypocrisy\u201d. OnthistaskGPT-3achieves65.2%inthe\nfew-shotsetting,59.1%intheone-shotsetting,and53.7%inthezero-shotsetting,whereastheaveragescoreamong\ncollegeapplicantswas57%[TL05](randomguessingyields20%). AsshowninFigure3.12,theresultsimprovewith\nscale,withthethefull175billionmodelimprovingbyover10%comparedtothe13billionparametermodel.\n24", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 24}}, {"text": "Figure3.12: Zero-,one-,andfew-shotperformanceonSATanalogytasks,fordifferentsizesofmodel. Thelargest\nmodelachieves65%accuracyinthefew-shotsetting,andalsodemonstratessignificantgainstoin-contextlearning\nwhicharenotpresentinsmallermodels.\n3.9.4 NewsArticleGeneration\nPreviousworkongenerativelanguagemodelsqualitativelytestedtheirabilitytogeneratesynthetic\u201cnewsarticles\u201dby\nconditionalsamplingfromthemodelgivenahuman-writtenpromptconsistingofaplausiblefirstsentenceforanews\nstory[RWC+19]. Relativeto[RWC+19],thedatasetusedtotrainGPT-3ismuchlessweightedtowardsnewsarticles,\nsotryingtogeneratenewsarticlesviarawunconditionalsamplesislesseffective\u2013forexampleGPT-3ofteninterprets\ntheproposedfirstsentenceofa\u201cnewsarticle\u201dasatweetandthenpostssyntheticresponsesorfollow-uptweets. To\nsolvethisproblemweemployedGPT-3\u2019sfew-shotlearningabilitiesbyprovidingthreepreviousnewsarticlesinthe\nmodel\u2019s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably\ngenerateshortarticlesinthe\u201cnews\u201dgenre.\nTogaugethequalityofnewsarticlegenerationfromGPT-3(whichwebelieveislikelytobecorrelatedwithconditional\nsamplegenerationqualityingeneral),wedecidedtomeasurehumanabilitytodistinguishGPT-3-generatedarticles\nfromrealones. SimilarworkhasbeencarriedoutbyKrepsetal. [KMB20]andZellersetal. [ZHR+19]. Generative\nlanguagemodelsaretrainedtomatchthedistributionofcontentgeneratedbyhumans,sothe(in)abilityofhumansto\ndistinguishthetwoisapotentiallyimportantmeasureofquality.3\nInordertoseehowwellhumanscandetectmodelgeneratedtext,wearbitrarilyselected25articletitlesandsubtitles\nfromthewebsitenewser.com(meanlength: 215words). Wethengeneratedcompletionsofthesetitlesandsubtitles\nfromfourlanguagemodelsranginginsizefrom125Mto175B(GPT-3)parameters(meanlength: 200words). Foreach\nmodel,wepresentedaround80US-basedparticipantswithaquizconsistingoftheserealtitlesandsubtitlesfollowed\nbyeitherthehumanwrittenarticleorthearticlegeneratedbythemodel4. Participantswereaskedtoselectwhetherthe\narticlewas\u201cverylikelywrittenbyahuman\u201d,\u201cmorelikelywrittenbyahuman\u201d,\u201cIdon\u2019tknow\u201d,\u201cmorelikelywrittenby\namachine\u201d,or\u201cverylikelywrittenbyamachine\u201d.\nThe articles we selected were not in the models\u2019 training data and the model outputs were formatted and selected\nprogrammaticallytopreventhumancherry-picking. Allmodelsusedthesamecontexttoconditionoutputsonandwere\npre-trainedwiththesamecontextsizeandthesamearticletitlesandsubtitleswereusedaspromptsforeachmodel.\nHowever,wealsorananexperimenttocontrolforparticipanteffortandattentionthatfollowedthesameformatbut\ninvolvedintentionallybadmodelgeneratedarticles. Thiswasdonebygeneratingarticlesfroma\u201ccontrolmodel\u201d: a\n160Mparametermodelwithnocontextandincreasedoutputrandomness.\n3ThistaskisalsorelevanttothepotentialmisuseoflanguagemodelsdiscussedinSection6.1.\n4Wewantedtoidentifyhowgoodanaveragepersonontheinternetisatdetectinglanguagemodeloutputs,sowefocusedon\nparticipantsdrawnfromthegeneralUSpopulation.SeeAppendixEfordetails.\n25", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 25}}, {"text": "95%Confidence tcomparedto \u201cIdon\u2019tknow\u201d\nMeanaccuracy Interval(low,hi) control(p-value) assignments\nControl(deliberatelybadmodel) 86% 83%\u201390% - 3.6%\nGPT-3Small 76% 72%\u201380% 3.9(2e-4) 4.9%\nGPT-3Medium 61% 58%\u201365% 10.3(7e-21) 6.0%\nGPT-3Large 68% 64%\u201372% 7.3(3e-11) 8.7%\nGPT-3XL 62% 59%\u201365% 10.7(1e-19) 7.5%\nGPT-32.7B 62% 58%\u201365% 10.4(5e-19) 7.1%\nGPT-36.7B 60% 56%\u201363% 11.2(3e-21) 6.2%\nGPT-313B 55% 52%\u201358% 15.3(1e-32) 7.1%\nGPT-3175B 52% 49%\u201354% 16.9(1e-34) 7.8%\nTable3.11: Humanaccuracyinidentifyingwhethershort(\u223c200word)newsarticlesaremodelgenerated. We\nfindthathumanaccuracy(measuredbytheratioofcorrectassignmentstonon-neutralassignments)rangesfrom86%\nonthecontrolmodelto52%onGPT-3175B.Thistablecomparesmeanaccuracybetweenfivedifferentmodels,and\nshowstheresultsofatwo-sampleT-Testforthedifferenceinmeanaccuracybetweeneachmodelandthecontrolmodel\n(anunconditionalGPT-3Smallmodelwithincreasedoutputrandomness).\nMeanhumanaccuracy(theratioofcorrectassignmentstonon-neutralassignmentsperparticipant)atdetectingthat\ntheintentionallybadarticlesweremodelgeneratedwas\u223c86% where50%ischancelevelperformance. Bycontrast,\nmeanhumanaccuracyatdetectingarticlesthatwereproducedbythe175Bparametermodelwasbarelyabovechance\nat\u223c52%(seeTable3.11).5 Humanabilitiestodetectmodelgeneratedtextappeartodecreaseasmodelsizeincreases:\nthereappearstobeatrendtowardschanceaccuracywithmodelsize,andhumandetectionofGPT-3isclosetochance.6\nThisistruedespitethefactthatparticipantsspendmoretimeoneachoutputasmodelsizeincreases(seeAppendixE).\nExamplesofsyntheticarticlesfromGPT-3aregiveninFigures3.14and3.15.7 Muchofthetextis\u2014asindicatedbythe\nevaluations\u2014difficultforhumanstodistinguishfromauthentichumancontent. Factualinaccuraciescanbeanindicator\nthatanarticleismodelgeneratedsince,unlikehumanauthors,themodelshavenoaccesstothespecificfactsthatthe\narticletitlesrefertoorwhenthearticlewaswritten. Otherindicatorsincluderepetition,nonsequiturs,andunusual\nphrasings,thoughtheseareoftensubtleenoughthattheyarenotnoticed.\nRelatedworkonlanguagemodeldetectionbyIppolitoetal. [IDCBE19]indicatesthatautomaticdiscriminatorslike\nGROVER [ZHR+19]andGLTR[GSR19]mayhavegreatersuccessatdetectingmodelgeneratedtextthanhuman\nevaluators. Automaticdetectionofthesemodelsmaybeapromisingareaoffutureresearch.\nIppolitoetal. [IDCBE19]alsonotethathumanaccuracyatdetectingmodelgeneratedtextincreasesashumansobserve\nmoretokens. Todoapreliminaryinvestigationofhowgoodhumansareatdetectinglongernewsarticlesgenerated\nbyGPT-3175B,weselected12worldnewsarticlesfromReuterswithanaveragelengthof569wordsandgenerated\ncompletions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial\nexperiments). Followingthemethodologyabove,werantwoexperiments,eachonaround80US-basedparticipants,to\ncomparehumanabilitiestodetectthearticlesgeneratedbyGPT-3andacontrolmodel.\nWefoundthatmeanhumanaccuracyatdetectingtheintentionallybadlongerarticlesfromthecontrolmodelwas\n\u223c88%,whilemeanhumanaccuracyatdetectingthelongerarticlesthatwereproducedbyGPT-3175Bwasstillbarely\nabovechanceat\u223c52%(seeTable3.12). Thisindicatesthat,fornewsarticlesthatarearound500wordslong,GPT-3\ncontinuestoproducearticlesthathumansfinddifficulttodistinguishfromhumanwrittennewsarticles.\n3.9.5 LearningandUsingNovelWords\nAtaskstudiedindevelopmentallinguistics[CB78]istheabilitytolearnandutilizenewwords,forexampleusinga\nwordinasentenceafterseeingitdefinedonlyonce,orconverselyinferringaword\u2019smeaningfromonlyoneusage.Here\nwequalitativelytestGPT-3\u2019sabilitytodotheformer. Specifically,wegiveGPT-3thedefinitionofanonexistentword,\nsuchas\u201cGigamuru\u201d,andthenaskittouseitinasentence. Weprovideonetofivepreviousexamplesofa(separate)\n5Weuseatwo-sampleStudent\u2019sT-Testtotestforsignificantdifferencebetweenthemeansoftheparticipantaccuraciesofeach\nmodelandthecontrolmodelandreportthenormalizeddifferenceinthemeans(asthet-statistic)andthep-value.\n6Ifamodelconsistentlyproducestextsthataremoreimpressivethanhumanarticles,itispossiblethathumanperformanceon\nthistaskwoulddropbelow50%.Indeed,manyindividualparticipantsscoredbelow50%onthistask.\n7Additionalnon-newssamplescanbefoundinAppendixF.\n26", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 26}}, {"text": "Figure3.13: People\u2019sabilitytoidentifywhethernewsarticlesaremodel-generated(measuredbytheratioofcorrect\nassignmentstonon-neutralassignments)decreasesasmodelsizeincreases. Accuracyontheoutputsonthedeliberately-\nbadcontrolmodel(anunconditionedGPT-3Smallmodelwithhigheroutputrandomness)isindicatedwiththedashed\nlineatthetop,andtherandomchance(50%)isindicatedwiththedashedlineatthebottom. Lineofbestfitisapower\nlawwith95%confidenceintervals.\n95%Confidence tcomparedto \u201cIdon\u2019tknow\u201d\nMeanaccuracy Interval(low,hi) control(p-value) assignments\nControl 88% 84%\u201391% - 2.7%\nGPT-3175B 52% 48%\u201357% 12.7(3.2e-23) 10.6%\nTable3.12: People\u2019sabilitytoidentifywhether\u223c500wordarticlesaremodelgenerated(asmeasuredbytheratioof\ncorrectassignmentstonon-neutralassignments)was88%onthecontrolmodeland52%onGPT-3175B.Thistable\nshowstheresultsofatwo-sampleT-TestforthedifferenceinmeanaccuracybetweenGPT-3175Bandthecontrol\nmodel(anunconditionalGPT-3Smallmodelwithincreasedoutputrandomness).\n27", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 27}}, {"text": "Title: United Methodists Agree to Historic Split\nSubtitle: Those who oppose gay marriage will form their own denomination\nArticle: After two days of intense debate, the United Methodist Church\nhas agreed to a historic split - one that is expected to end in the\ncreation of a new denomination, one that will be \"theologically and\nsocially conservative,\" according to The Washington Post. The majority of\ndelegates attending the church\u2019s annual General Conference in May voted to\nstrengthen a ban on the ordination of LGBTQ clergy and to write new rules\nthat will \"discipline\" clergy who officiate at same-sex weddings. But\nthose who opposed these measures have a new plan: They say they will form a\nseparate denomination by 2020, calling their church the Christian Methodist\ndenomination.\nThe Post notes that the denomination, which claims 12.5 million members, was\nin the early 20th century the \"largest Protestant denomination in the U.S.,\"\nbut that it has been shrinking in recent decades. The new split will be the\nsecond in the church\u2019s history. The first occurred in 1968, when roughly\n10 percent of the denomination left to form the Evangelical United Brethren\nChurch. The Post notes that the proposed split \"comes at a critical time\nfor the church, which has been losing members for years,\" which has been\n\"pushed toward the brink of a schism over the role of LGBTQ people in the\nchurch.\" Gay marriage is not the only issue that has divided the church. In\n2016, the denomination was split over ordination of transgender clergy, with\nthe North Pacific regional conference voting to ban them from serving as\nclergy, and the South Pacific regional conference voting to allow them.\nFigure3.14: TheGPT-3generatednewsarticlethathumanshadthegreatestdifficultydistinguishingfromahuman\nwrittenarticle(accuracy: 12%).\nTitle: Star\u2019s Tux Promise Draws Megyn Kelly\u2019s Sarcasm\nSubtitle: Joaquin Phoenix pledged to not change for each awards event\nArticle: A year ago, Joaquin Phoenix made headlines when he appeared on\nthe red carpet at the Golden Globes wearing a tuxedo with a paper bag over\nhis head that read, \"I am a shape-shifter. I can\u2019t change the world. I\ncan only change myself.\" It was a promise to not change to fit into the\nHollywood mold: \"I think that\u2019s a really special thing, to not change\nyourself. I think it\u2019s a really special thing to say, \u2018This is what\u2019s\ninside of me, I\u2019m proud of it, and I\u2019m not going to be ashamed because of\nthe way that someone else thinks I should be.\u2019\" Now, it\u2019s the Oscars, and\nPhoenix is at it again. But this time, his publicist is saying he\u2019ll be\nwearing a tux no matter what.\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show.\n\"You know, I feel like, I feel like you could have worn the tux,\" she says.\n\"But you\u2019re saying you\u2019re a shape-shifter. I don\u2019t know if you can change\nyour tux, but you can change your mind. You can change your mind. You can\nchange your mind.\" Phoenix says he did, but it didn\u2019t stick. \"I was like,\n\u2018Okay, I\u2019m going to wear a tuxedo to this thing.\u2019 And then I thought, \u2018I\ndon\u2019t want to wear a tuxedo to this thing.\u2019\" Kelly goes on to encourage him\nto change his mind again, but Phoenix says it\u2019s too late: \"I\u2019m committed to\nwearing this.\"\nFigure3.15: TheGPT-3generatednewsarticlethathumansfoundtheeasiesttodistinguishfromahumanwritten\narticle(accuracy: 61%).\n28", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 28}}, {"text": "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\nthe word whatpu is:\nWe were traveling in Africa and we saw these very cute whatpus.\nTo do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\nthe word farduddle is:\nOne day when I was playing tag with my little sister, she got really excited and she\nstarted doing these crazy farduddles.\nA \"yalubalu\" is a type of vegetable that looks like a big pumpkin. An example of a sentence\nthat uses the word yalubalu is:\nI was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden\nthere. It was delicious.\nA \"Burringo\" is a car with very fast acceleration. An example of a sentence that uses the\nword Burringo is:\nIn our garage we have a Burringo that my father drives to work every day.\nA \"Gigamuru\" is a type of Japanese musical instrument. An example of a sentence that uses the\nword Gigamuru is:\nI have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\nTo \"screeg\" something is to swing a sword at it. An example of a sentence that uses the word\nscreeg is:\nWe screeghed at each other for several minutes and then we went outside and ate ice cream.\nFigure3.16: RepresentativeGPT-3completionsforthefew-shottaskofusinganewwordinasentence. Boldfaceis\nGPT-3\u2019scompletions,plaintextishumanprompts. Inthefirstexampleboththepromptandthecompletionareprovided\nbyahuman; thisthenservesasconditioningforsubsequentexampleswhereGPT-3receivessuccessiveadditional\npromptsandprovidesthecompletions. Nothingtask-specificisprovidedtoGPT-3otherthantheconditioningshown\nhere.\nnonexistentwordbeingdefinedandusedinasentence,sothetaskisfew-shotintermsofpreviousexamplesofthe\nbroadtaskandone-shotintermsofthespecificword. Table3.16showsthe6exampleswegenerated;alldefinitions\nwerehuman-generated,andthefirstanswerwashuman-generatedasconditioningwhilethesubsequentanswerswere\ngeneratedbyGPT-3. Theseexamplesweregeneratedcontinuouslyinonesittingandwedidnotomitorrepeatedlytry\nanyprompts. Inallcasesthegeneratedsentenceappearstobeacorrectoratleastplausibleuseoftheword. Inthefinal\nsentencethemodelgeneratesaplausibleconjugationfortheword\u201cscreeg\u201d(namely\u201cscreeghed\u201d),althoughtheuseof\nthewordisslightlyawkward(\u201cscreeghedateachother\u201d)despitebeingplausibleinthesensethatitcoulddescribeatoy\nswordfight. Overall,GPT-3appearstobeatleastproficientatthetaskofusingnovelwordsinasentence.\n3.9.6 CorrectingEnglishGrammar\nAnothertaskwellsuitedforfew-shotlearningiscorrectingEnglishgrammar. WetestthiswithGPT-3inthefew-\nshot setting by giving prompts of the form \"Poor English Input: <sentence>\\n Good English Output:\n<sentence>\". WegiveGPT-3onehuman-generatedcorrectionandthenaskittocorrect5more(againwithoutany\nomissionsorrepeats). ResultsareshowninFigure3.17.\n4 MeasuringandPreventingMemorizationOfBenchmarks\nSince our training dataset is sourced from the internet, it is possible that our model was trained on some of our\nbenchmarktestsets. Accuratelydetectingtestcontaminationfrominternet-scaledatasetsisanewareaofresearch\nwithoutestablishedbestpractices.Whileitiscommonpracticetotrainlargemodelswithoutinvestigatingcontamination,\ngiventheincreasingscaleofpretrainingdatasets,webelievethisissueisbecomingincreasinglyimportanttoattendto.\nThisconcernisnotjusthypothetical. OneofthefirstpaperstotrainalanguagemodelonCommonCrawldata[TL18]\ndetectedandremovedatrainingdocumentwhichoverlappedwithoneoftheirevaluationdatasets. Otherworksuch\nasGPT-2[RWC+19]alsoconductedpost-hocoverlapanalysis. Theirstudywasrelativelyencouraging,findingthat\n29", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 29}}, {"text": "Poor English input: I eated the purple berries.\nGood English output: I ate the purple berries.\nPoor English input: Thank you for picking me as your designer. I\u2019d appreciate it.\nGood English output: Thank you for choosing me as your designer. I appreciate it.\nPoor English input: The mentioned changes have done. or I did the alteration that you\nrequested. or I changed things you wanted and did the modifications.\nGood English output: The requested changes have been made. or I made the alteration that you\nrequested. or I changed things you wanted and made the modifications.\nPoor English input: I\u2019d be more than happy to work with you in another project.\nGood English output: I\u2019d be more than happy to work with you on another project.\nPoor English input: Please provide me with a short brief of the design you\u2019re looking for and\nthat\u2019d be nice if you could share some examples or project you did before.\nGood English output: Please provide me with a brief description of the design you\u2019re\nlooking for and that would be nice if you could share some examples or projects you have\ndone before.\nPoor English input: The patient was died.\nGood English output: The patient died.\nPoor English input: We think that Leslie likes ourselves.\nGood English output: We think that Leslie likes us.\nPoor English input: Janet broke Bill on the finger.\nGood English output: Janet broke Bill\u2019s finger.\nPoor English input: Mary arranged for, in St. Louis, John to rent a house cheap.\nGood English output: Mary arranged for John to rent a house in St. Louis.\nPoor English input: Today I have went to the store to to buys some many bottle of water.\nGood English output: Today I went to the store to buy some bottles of water.\nPoor English input: I have tried to hit ball with bat, but my swing is has miss.\nGood English output: I tried to hit the ball with the bat, but my swing missed.\nFigure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface\nis GPT-3\u2019s completions, plain text is human prompts. In the first few examples example both the prompt and the\ncompletionareprovidedbyahuman;thisthenservesasconditioningforsubsequentexampleswhereGPT-3receives\nsuccessiveadditionalpromptsandprovidesthecompletions. Nothingtask-specificisprovidedtoGPT-3asidefrom\nthefirstfewexamplesasconditioningandthe\u201cPoorEnglishinput/GoodEnglishoutput\u201dframing. Wenotethatthe\ndistinctionbetween\u201dpoor\u201dand\u201dgood\u201dEnglish(andthetermsthemselves)iscomplex,contextual,andcontested. As\ntheexamplementioningtherentalofahouseshows,assumptionsthatthemodelmakesaboutwhat\u201cgood\u201discaneven\nleadittomakeerrors(here,themodelnotonlyadjustsgrammar,butalsoremovestheword\u201dcheap\u201dinawaythatalters\nmeaning).\n30", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 30}}, {"text": "Figure4.1: GPT-3TrainingCurves Wemeasuremodelperformanceduringtrainingonadeduplicatedvalidation\nsplitofourtrainingdistribution. Thoughthereissomegapbetweentrainingandvalidationperformance,thegapgrows\nonlyminimallywithmodelsizeandtrainingtime,suggestingthatmostofthegapcomesfromadifferenceindifficulty\nratherthanoverfitting.\nalthough models did perform moderately better on data that overlapped between training and testing, this did not\nsignificantlyimpactreportedresultsduetothesmallfractionofdatawhichwascontaminated(oftenonlyafewpercent).\nGPT-3operatesinasomewhatdifferentregime. Ontheonehand,thedatasetandmodelsizeareabouttwoordersof\nmagnitudelargerthanthoseusedforGPT-2,andincludealargeamountofCommonCrawl,creatingincreasedpotential\nforcontaminationandmemorization. Ontheotherhand,preciselyduetothelargeamountofdata,evenGPT-3175B\ndoesnotoverfititstrainingsetbyasignificantamount,measuredrelativetoaheld-outvalidationsetwithwhichitwas\ndeduplicated(Figure4.1). Thus,weexpectthatcontaminationislikelytobefrequent,butthatitseffectsmaynotbeas\nlargeasfeared.\nWeinitiallytriedtoaddresstheissueofcontaminationbyproactivelysearchingforandattemptingtoremoveanyoverlap\nbetweenourtrainingdataandthedevelopmentandtestsetsofallbenchmarksstudiedinthispaper. Unfortunately,a\nbugresultedinonlypartialremovalofalldetectedoverlapsfromthetrainingdata. Duetothecostoftraining,itwasn\u2019t\nfeasibletoretrainthemodel. Toaddressthis, weinvestigateindetailhowtheremainingdetectedoverlapimpacts\nresults.\nForeachbenchmark,weproducea\u2018clean\u2019versionwhichremovesallpotentiallyleakedexamples,definedroughlyas\nexamplesthathavea13-gramoverlapwithanythinginthepretrainingset(orthatoverlapwiththewholeexamplewhen\nitisshorterthan13-grams). Thegoalistoveryconservativelyflaganythingthatcouldpotentiallybecontamination,\nsoastoproduceacleansubsetthatisfreeofcontaminationwithhighconfidence. Theexactprocedureisdetailedin\nAppendixC.\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean\nsubsetissimilartothescoreontheentiredataset,thissuggeststhatcontamination,evenifpresent,doesnothavea\nsignificanteffectonreportedresults. Ifthescoreonthecleansubsetislower,thissuggestscontaminationmaybe\ninflatingtheresults. TheresultsaresummarizedinFigure4.2. Althoughpotentialcontaminationisoftenhigh(witha\nquarterofbenchmarksscoringover50%),inmostcasesperformancechangesonlynegligibly,andweseenoevidence\nthatcontaminationlevelandperformancedifferencearecorrelated. Weconcludethateitherourconservativemethod\nsubstantiallyoverestimatedcontaminationorthatcontaminationhaslittleeffectonperformance.\nBelow,wereviewinmoredetailthefewspecificcaseswhereeither(1)themodelperformssignificantlyworseon\nthecleanedversion,or(2)potentialcontaminationisveryhigh,whichmakesmeasuringtheperformancedifference\ndifficult.\nOuranalysisflaggedsixgroupsofbenchmarksforfurtherinvestigation: WordScrambling,ReadingComprehension\n(QuAC,SQuAD2,DROP),PIQA,Winograd,languagemodelingtasks(Wikitexttasks,1BW),andGermantoEnglish\n31", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 31}}, {"text": "Figure4.2: Benchmarkcontaminationanalysis Weconstructedcleanedversionsofeachofourbenchmarksto\ncheckforpotentialcontaminationinourtrainingset. Thex-axisisaconservativelowerboundforhowmuchofthe\ndatasetisknownwithhighconfidencetobeclean,andthey-axisshowsthedifferenceinperformancewhenevaluating\nonlyontheverifiedcleansubset. Performanceonmostbenchmarkschangednegligibly,butsomewereflaggedfor\nfurtherreview. OninspectionwefindsomeevidenceforcontaminationofthePIQAandWinogradresults,andwemark\nthecorrespondingresultsinSection3withanasterisk. Wefindnoevidencethatotherbenchmarksareaffected.\ntranslation. Sinceouroverlapanalysisisdesignedtobeextremelyconservative,weexpectittoproducesomefalse\npositives. Wesummarizetheresultsforeachgroupoftasksbelow:\n\u2022 ReadingComprehension: Ourinitialanalysisflagged>90%oftaskexamplesfromQuAC,SQuAD2,and\nDROPaspotentiallycontaminated,solargethatevenmeasuringthedifferentialonacleansubsetwasdifficult.\nUponmanualinspection,however,wefoundthatforeveryoverlapweinspected,inall3datasets,thesource\ntextwaspresentinourtrainingdatabutthequestion/answerpairswerenot,meaningthemodelgainsonly\nbackgroundinformationandcannotmemorizetheanswertoaspecificquestion.\n\u2022 Germantranslation: Wefound25%oftheexamplesintheWMT16German-Englishtestsetweremarked\naspotentiallycontaminated,withanassociatedtotaleffectsizeof1-2BLEU.Uponinspection,noneofthe\nflaggedexamplescontainpairedsentencesresemblingNMTtrainingdataandcollisionsweremonolingual\nmatchesmostlyofsnippetsofeventsdiscussedinthenews.\n\u2022 Reversed Words and Anagrams: Recall that these tasks are of the form \u201calaok = koala\u201d. Due to the\nshortlengthofthesetasks,weused2-gramsforfiltering(ignoringpunctuation). Afterinspectingtheflagged\noverlaps,wefoundthattheywerenottypicallyinstancesofrealreversalsorunscramblingsinthetrainingset,\nbutratherpalindromesortrivialunscramblings,e.g\u201ckayak = kayak\u201d. Theamountofoverlapwassmall,\nbutremovingthetrivialtasksleadtoanincreaseindifficultyandthusaspurioussignal. Relatedtothis,the\nsymbolinsertiontaskshowshighoverlapbutnoeffectonperformance\u2013thisisbecausethattaskinvolves\nremovingnon-lettercharactersfromaword,andtheoverlapanalysisitselfignoressuchcharacters,leadingto\nmanyspuriousmatches.\n\u2022 PIQA:Theoverlapanalysisflagged29%ofexamplesascontaminated,andobserveda3percentagepoint\nabsolutedecrease(4%relativedecrease)inperformanceonthecleansubset. Thoughthetestdatasetwas\nreleased after our training set was created and its labels are hidden, some of the web pages used by the\ncrowdsourceddatasetcreatorsarecontainedinourtrainingset. Wefoundasimilardecreaseina25xsmaller\nmodel with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias\nratherthanmemorization;exampleswhichworkerscopiedmaysimplybeeasier. Unfortunately,wecannot\nrigorouslyprovethishypothesis. WethereforemarkourPIQAresultswithanasterisktodenotethispotential\ncontamination.\n\u2022 Winograd: Theoverlapanalysisflagged45%ofexamples,andfounda2.6%decreaseinperformanceonthe\ncleansubset. Manualinspectionoftheoverlappingdatapointshowedthat132Winogradschemaswerein\nfactpresentinourtrainingset,thoughpresentedinadifferentformatthanwepresentthetasktothemodel.\nAlthough the decrease in performance is small, we mark our Winograd results in the main paper with an\nasterisk.\n32", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 32}}, {"text": "\u2022 Languagemodeling: Wefoundthe4WikipedialanguagemodelingbenchmarksmeasuredinGPT-2,plusthe\nChildren\u2019sBookTestdataset,tobealmostentirelycontainedinourtrainingdata. Sincewecannotreliably\nextractacleansubsethere,wedonotreportresultsonthesedatasets,eventhoughweintendedtowhenstarting\nthiswork. WenotethatPennTreeBankduetoitsagewasunaffectedandthereforebecameourchieflanguage\nmodelingbenchmark.\nWealsoinspecteddatasetswherecontaminationwashigh,buttheimpactonperformancewasclosetozero,simply\ntoverifyhowmuchactualcontaminationexisted. Theseappearedtooftencontainfalsepositives. Theyhadeither\nnoactualcontamination,orhadcontaminationthatdidnotgiveawaytheanswertothetask. Onenotableexception\nwasLAMBADA,whichappearedtohavesubstantialgenuinecontamination,yettheimpactonperformancewasvery\nsmall,withthecleansubsetscoringwithin0.5%ofthefulldataset. Also,strictlyspeaking,ourfill-in-the-blankformat\nprecludesthesimplestformofmemorization. Nevertheless,sincewemadeverylargegainsonLAMBADAinthis\npaper,thepotentialcontaminationisnotedintheresultssection.\nAnimportantlimitationofourcontaminationanalysisisthatwecannotbesurethatthecleansubsetisdrawnfromthe\nsamedistributionastheoriginaldataset. Itremainspossiblethatmemorizationinflatesresultsbutatthesametime\nispreciselycounteractedbysomestatisticalbiascausingthecleansubsettobeeasier. However,thesheernumber\nofshiftsclosetozerosuggeststhisisunlikely,andwealsoobservednonoticeabledifferenceintheshiftsforsmall\nmodels,whichareunlikelytobememorizing.\nOverall,wehavemadeabestefforttomeasureanddocumenttheeffectsofdatacontamination,andtonoteoroutright\nremoveproblematicresults,dependingontheseverity. Muchworkremainstobedonetoaddressthisimportantand\nsubtleissueforthefieldingeneral,bothwhendesigningbenchmarksandwhentrainingmodels. Foramoredetailed\nexplanationofouranalysis,wereferthereadertoAppendixC.\n5 Limitations\nGPT-3andouranalysisofithaveanumberoflimitations. Belowwedescribesomeoftheseandsuggestdirectionsfor\nfuturework.\nFirst, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct\npredecessorGPT-2,itstillhasnotableweaknessesintextsynthesisandseveralNLPtasks. Ontextsynthesis,although\ntheoverallqualityishigh,GPT-3samplesstillsometimesrepeatthemselvessemanticallyatthedocumentlevel,startto\nlosecoherenceoversufficientlylongpassages,contradictthemselves,andoccasionallycontainnon-sequitursentences\norparagraphs. Wewillreleaseacollectionof500uncuratedunconditionalsamplestohelpprovideabettersenseof\nGPT-3\u2019slimitationsandstrengthsattextsynthesis. Withinthedomainofdiscretelanguagetasks,wehavenoticed\ninformallythatGPT-3seemstohavespecialdifficultywith\u201ccommonsensephysics\u201d, despitedoingwellonsome\ndatasets(suchasPIQA[BZB+19])thattestthisdomain. SpecificallyGPT-3hasdifficultywithquestionsofthetype\n\u201cIfIputcheeseintothefridge,willitmelt?\u201d. Quantitatively,GPT-3\u2019sin-contextlearningperformancehassomenotable\ngapsonoursuiteofbenchmarks,asdescribedinSection3,andinparticularitdoeslittlebetterthanchancewhen\nevaluatedone-shotorevenfew-shotonsome\u201ccomparison\u201dtasks,suchasdeterminingiftwowordsareusedthesame\nwayinasentence,orifonesentenceimpliesanother(WICandANLIrespectively),aswellasonasubsetofreading\ncomprehensiontasks. ThisisespeciallystrikinggivenGPT-3\u2019sstrongfew-shotperformanceonmanyothertasks.\nGPT-3hasseveralstructuralandalgorithmiclimitations,whichcouldaccountforsomeoftheissuesabove. Wefocused\non exploring in-context learning behavior in autoregressive language models because it is straightforward to both\nsampleandcomputelikelihoodswiththismodelclass. Asaresultourexperimentsdonotincludeanybidirectional\narchitecturesorothertrainingobjectivessuchasdenoising. Thisisanoticeabledifferencefrommuchoftherecent\nliterature, which has documented improved fine-tuning performance when using these approaches over standard\nlanguagemodels[RSR+19]. Thusourdesigndecisioncomesatthecostofpotentiallyworseperformanceontasks\nwhichempiricallybenefitfrombidirectionality. Thismayincludefill-in-the-blanktasks,tasksthatinvolvelookingback\nandcomparingtwopiecesofcontent,ortasksthatrequirere-readingorcarefullyconsideringalongpassageandthen\ngeneratingaveryshortanswer. ThiscouldbeapossibleexplanationforGPT-3\u2019slaggingfew-shotperformanceona\nfewofthetasks,suchasWIC(whichinvolvescomparingtheuseofawordintwosentences),ANLI(whichinvolves\ncomparingtwosentencestoseeifoneimpliestheother),andseveralreadingcomprehensiontasks(e.g. QuACand\nRACE).Wealsoconjecture,basedonpastliterature,thatalargebidirectionalmodelwouldbestrongeratfine-tuning\nthanGPT-3. MakingabidirectionalmodelatthescaleofGPT-3,and/ortryingtomakebidirectionalmodelsworkwith\nfew-orzero-shotlearning,isapromisingdirectionforfutureresearch,andcouldhelpachievethe\u201cbestofbothworlds\u201d.\nAmorefundamentallimitationofthegeneralapproachdescribedinthispaper\u2013scalingupanyLM-likemodel,whether\nautoregressiveorbidirectional\u2013isthatitmayeventuallyruninto(orcouldalreadyberunninginto)thelimitsofthe\n33", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 33}}, {"text": "pretrainingobjective. Ourcurrentobjectiveweightseverytokenequallyandlacksanotionofwhatismostimportantto\npredictandwhatislessimportant. [RRS20]demonstratebenefitsofcustomizingpredictiontoentitiesofinterest. Also,\nwithself-supervisedobjectives,taskspecificationreliesonforcingthedesiredtaskintoapredictionproblem,whereas\nultimately,usefullanguagesystems(forexamplevirtualassistants)mightbebetterthoughtofastakinggoal-directed\nactionsratherthanjustmakingpredictions. Finally,largepretrainedlanguagemodelsarenotgroundedinotherdomains\nofexperience,suchasvideoorreal-worldphysicalinteraction,andthuslackalargeamountofcontextabouttheworld\n[BHT+20]. Forallthesereasons,scalingpureself-supervisedpredictionislikelytohitlimits,andaugmentationwitha\ndifferentapproachislikelytobenecessary. Promisingfuturedirectionsinthisveinmightincludelearningtheobjective\nfunctionfromhumans[ZSW+19a],fine-tuningwithreinforcementlearning,oraddingadditionalmodalitiessuchas\nimagestoprovidegroundingandabettermodeloftheworld[CLY+19].\nAnotherlimitationbroadlysharedbylanguagemodelsispoorsampleefficiencyduringpre-training. WhileGPT-3\ntakesasteptowardstest-timesampleefficiencyclosertothatofhumans(one-shotorzero-shot),itstillseesmuchmore\ntextduringpre-trainingthanahumanseesinthetheirlifetime[Lin20]. Improvingpre-trainingsampleefficiencyis\nanimportantdirectionforfuturework,andmightcomefromgroundinginthephysicalworldtoprovideadditional\ninformation,orfromalgorithmicimprovements.\nAlimitation,oratleastuncertainty,associatedwithfew-shotlearninginGPT-3isambiguityaboutwhetherfew-shot\nlearningactuallylearnsnewtasks\u201cfromscratch\u201datinferencetime,orifitsimplyrecognizesandidentifiestasksthatit\nhaslearnedduringtraining. Thesepossibilitiesexistonaspectrum,rangingfromdemonstrationsinthetrainingsetthat\naredrawnfromexactlythesamedistributionasthoseattesttime,torecognizingthesametaskbutinadifferentformat,\ntoadaptingtoaspecificstyleofageneraltasksuchasQA,tolearningaskillentirelydenovo. WhereGPT-3ison\nthisspectrummayalsovaryfromtasktotask. Synthetictaskssuchaswordscramblingordefiningnonsensewords\nseemespeciallylikelytobelearneddenovo,whereastranslationclearlymustbelearnedduringpretraining,although\npossiblyfromdatathatisverydifferentinorganizationandstylethanthetestdata. Ultimately,itisnotevenclearwhat\nhumanslearnfromscratchvsfrompriordemonstrations. Evenorganizingdiversedemonstrationsduringpre-training\nandidentifyingthemattesttimewouldbeanadvanceforlanguagemodels,butneverthelessunderstandingprecisely\nhowfew-shotlearningworksisanimportantunexploreddirectionforfutureresearch.\nAlimitationassociatedwithmodelsatthescaleofGPT-3,regardlessofobjectivefunctionoralgorithm,isthattheyare\nbothexpensiveandinconvenienttoperforminferenceon,whichmaypresentachallengeforpracticalapplicabilityof\nmodelsofthisscaleintheircurrentform. Onepossiblefuturedirectiontoaddressthisisdistillation[HVD15]oflarge\nmodelsdowntoamanageablesizeforspecifictasks. LargemodelssuchasGPT-3containaverywiderangeofskills,\nmostofwhicharenotneededforaspecifictask,suggestingthatinprincipleaggressivedistillationmaybepossible.\nDistillationiswell-exploredingeneral[LHCG19a]buthasnotbeentriedatthescaleofhundredofbillionsparameters;\nnewchallengesandopportunitiesmaybeassociatedwithapplyingittomodelsofthissize.\nFinally,GPT-3sharessomelimitationscommontomostdeeplearningsystems\u2013itsdecisionsarenoteasilyinterpretable,\nit is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in\nperformancethanhumansonstandardbenchmarks,anditretainsthebiasesofthedataithasbeentrainedon. This\nlastissue\u2013biasesinthedatathatmayleadthemodeltogeneratestereotypedorprejudicedcontent\u2013isofspecial\nconcernfromasocietalperspective,andwillbediscussedalongwithotherissuesinthenextsectiononBroaderImpacts\n(Section6).\n6 BroaderImpacts\nLanguagemodelshaveawiderangeofbeneficialapplicationsforsociety,includingcodeandwritingauto-completion,\ngrammarassistance,gamenarrativegeneration,improvingsearchengineresponses,andansweringquestions. But\ntheyalsohavepotentiallyharmfulapplications. GPT-3improvesthequalityoftextgenerationandadaptabilityover\nsmallermodelsandincreasesthedifficultyofdistinguishingsynthetictextfromhuman-writtentext. Itthereforehasthe\npotentialtoadvanceboththebeneficialandharmfulapplicationsoflanguagemodels.\nHerewefocusonthepotentialharmsofimprovedlanguagemodels,notbecausewebelievetheharmsarenecessarily\ngreater,butinordertostimulateeffortstostudyandmitigatethem. Thebroaderimpactsoflanguagemodelslikethis\narenumerous. Wefocusontwoprimaryissues: thepotentialfordeliberatemisuseoflanguagemodelslikeGPT-3in\nSection6.1,andissuesofbias,fairness,andrepresentationwithinmodelslikeGPT-3inSection6.2. Wealsobriefly\ndiscussissuesofenergyefficiency(Section6.3).\n34", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 34}}, {"text": "6.1 MisuseofLanguageModels\nMalicioususesoflanguagemodelscanbesomewhatdifficulttoanticipatebecausetheyofteninvolverepurposing\nlanguagemodelsinaverydifferentenvironmentorforadifferentpurposethanresearchersintended. Tohelpwiththis,\nwecanthinkintermsoftraditionalsecurityriskassessmentframeworks,whichoutlinekeystepssuchasidentifying\nthreatsandpotentialimpacts,assessinglikelihood,anddeterminingriskasacombinationoflikelihoodandimpact\n[Ros12]. Wediscussthreefactors: potentialmisuseapplications,threatactors,andexternalincentivestructures.\n6.1.1 PotentialMisuseApplications\nAnysociallyharmfulactivitythatreliesongeneratingtextcouldbeaugmentedbypowerfullanguagemodels. Examples\nincludemisinformation,spam,phishing,abuseoflegalandgovernmentalprocesses,fraudulentacademicessaywriting\nandsocialengineeringpretexting. Manyoftheseapplicationsbottleneckonhumanbeingstowritesufficientlyhigh\nqualitytext. Languagemodelsthatproducehighqualitytextgenerationcouldlowerexistingbarrierstocarryingout\ntheseactivitiesandincreasetheirefficacy.\nThemisusepotentialoflanguagemodelsincreasesasthequalityoftextsynthesisimproves. TheabilityofGPT-3to\ngenerateseveralparagraphsofsyntheticcontentthatpeoplefinddifficulttodistinguishfromhuman-writtentextin\n3.9.4representsaconcerningmilestoneinthisregard.\n6.1.2 ThreatActorAnalysis\nThreatactorscanbeorganizedbyskillandresourcelevels,rangingfromlowormoderatelyskilledandresourcedactors\nwhomaybeabletobuildamaliciousproductto\u2018advancedpersistentthreats\u2019(APTs): highlyskilledandwell-resourced\n(e.g. state-sponsored)groupswithlong-termagendas[SBC+19].\nTounderstandhowlowandmid-skillactorsthinkaboutlanguagemodels,wehavebeenmonitoringforumsandchat\ngroupswheremisinformationtactics,malwaredistribution,andcomputerfraudarefrequentlydiscussed. Whilewedid\nfindsignificantdiscussionofmisusefollowingtheinitialreleaseofGPT-2inspringof2019,wefoundfewerinstances\nofexperimentationandnosuccessfuldeploymentssincethen. Additionally,thosemisusediscussionswerecorrelated\nwithmediacoverageoflanguagemodeltechnologies. Fromthis,weassessthatthethreatofmisusefromtheseactorsis\nnotimmediate,butsignificantimprovementsinreliabilitycouldchangethis.\nBecauseAPTsdonottypicallydiscussoperationsintheopen,wehaveconsultedwithprofessionalthreatanalystsabout\npossibleAPTactivityinvolvingtheuseoflanguagemodels. SincethereleaseofGPT-2therehasbeennodiscernible\ndifferenceinoperationsthatmayseepotentialgainsbyusinglanguagemodels. Theassessmentwasthatlanguage\nmodelsmaynotbeworthinvestingsignificantresourcesinbecausetherehasbeennoconvincingdemonstrationthat\ncurrentlanguagemodelsaresignificantlybetterthancurrentmethodsforgeneratingtext,andbecausemethodsfor\n\u201ctargeting\u201dor\u201ccontrolling\u201dthecontentoflanguagemodelsarestillataveryearlystage.\n6.1.3 ExternalIncentiveStructures\nEachthreatactorgroupalsohasasetoftactics,techniques,andprocedures(TTPs)thattheyrelyontoaccomplishtheir\nagenda. TTPsareinfluencedbyeconomicfactorslikescalabilityandeaseofdeployment;phishingisextremelypopular\namongallgroupsbecauseitoffersalow-cost,low-effort,high-yieldmethodofdeployingmalwareandstealinglogin\ncredentials. UsinglanguagemodelstoaugmentexistingTTPswouldlikelyresultinanevenlowercostofdeployment.\nEaseofuseisanothersignificantincentive. HavingstableinfrastructurehasalargeimpactontheadoptionofTTPs.\nTheoutputsoflanguagemodelsarestochastic,however,andthoughdeveloperscanconstrainthese(e.g. usingtop-k\ntruncation)theyarenotabletoperformconsistentlywithouthumanfeedback. Ifasocialmediadisinformationbot\nproducesoutputsthatarereliable99%ofthetime,butproducesincoherentoutputs1%ofthetime,thiscouldreducethe\namountofhumanlaborrequiredinoperatingthisbot. Butahumanisstillneededtofiltertheoutputs,whichrestricts\nhowscalabletheoperationcanbe.\nBasedonouranalysisofthismodelandanalysisofthreatactorsandthelandscape,wesuspectAIresearcherswill\neventuallydeveloplanguagemodelsthataresufficientlyconsistentandsteerablethattheywillbeofgreaterinterestto\nmaliciousactors. Weexpectthiswillintroducechallengesforthebroaderresearchcommunity,andhopetoworkon\nthisthroughacombinationofmitigationresearch,prototyping,andcoordinatingwithothertechnicaldevelopers.\n35", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 35}}, {"text": "6.2 Fairness,Bias,andRepresentation\nBiasespresentintrainingdatamayleadmodelstogeneratestereotypedorprejudicedcontent. Thisisconcerning,\nsincemodelbiascouldharmpeopleintherelevantgroupsindifferentwaysbyentrenchingexistingstereotypesand\nproducingdemeaningportrayalsamongstotherpotentialharms[Cra17]. Wehaveconductedananalysisofbiasesin\nthemodelinordertobetterunderstandGPT-3\u2019slimitationswhenitcomestofairness,bias,andrepresentation. 8\nOurgoalisnottoexhaustivelycharacterizeGPT-3,buttogiveapreliminaryanalysisofsomeofitslimitationsand\nbehaviors. Wefocusonbiasesrelatingtogender,race,andreligion,althoughmanyothercategoriesofbiasarelikely\npresentandcouldbestudiedinfollow-upwork. Thisisapreliminaryanalysisanddoesnotreflectallofthemodel\u2019s\nbiasesevenwithinthestudiedcategories.\nBroadly,ouranalysisindicatesthatinternet-trainedmodelshaveinternet-scalebiases;modelstendtoreflectstereotypes\npresentintheirtrainingdata. Belowwediscussourpreliminaryfindingsofbiasalongthedimensionsofgender,race,\nandreligion. Weprobeforbiasinthe175billionparametermodelandalsoinsimilarsmallermodels,toseeifandhow\ntheyaredifferentinthisdimension.\n6.2.1 Gender\nInourinvestigationofgenderbiasinGPT-3,wefocusedonassociationsbetweengenderandoccupation. Wefound\nthatoccupationsingeneralhaveahigherprobabilityofbeingfollowedbyamalegenderidentifierthanafemaleone\n(inotherwords,theyaremaleleaning)whengivenacontextsuchas\"The {occupation} was a\"(NeutralVariant).\n83%ofthe388occupationswetestedweremorelikelytobefollowedbyamaleidentifierbyGPT-3. Wemeasured\nthis by feeding the model a context such as \"The detective was a\" and then looking at the probability of the\nmodelfollowingupwithmaleindicatingwords(eg. man,maleetc.) orfemaleindicatingwords(woman,femaleetc.).\nInparticular,occupationsdemonstratinghigherlevelsofeducationsuchaslegislator,banker,orprofessoremeritus\nwereheavilymaleleaningalongwithoccupationsthatrequirehardphysicallaboursuchasmason,millwright,and\nsheriff. Occupationsthatweremorelikelytobefollowedbyfemaleidentifiersincludemidwife,nurse,receptionist,\nhousekeeperetc.\nWealsotestedhowtheseprobabilitieschangedwhenweshiftedthecontexttobethe\"The competent {occupation}\nwas a\"(CompetentVariant),andwhenweshiftedthecontexttobe\"The incompetent {occupation} was a\"\n(Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent\n{occupation} was a,\" the majority of occupations had an even higher probability of being followed by a\nmale identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was\na\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male\nwith a similar probability than for our original neutral prompt. The average occupation bias - measured as\n1 (cid:80) log(P(female|Context))-was\u22121.11fortheNeutralVariant,\u22122.14fortheCompetentVariantand\u22121.15\nnjobs jobs P(male|Context))\nfortheIncompetentVariant.\nWe also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further\ncorroborated the model\u2019s tendency to associate most occupations with males. One method measured the mod-\nels ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model\nacontextsuchas\"The advisor met with the advisee because she wanted to get advice about job\napplications. \u2018She\u2019 refers to the\"andfoundtheoptionwiththelowestprobabilitybetweenthetwopossi-\nbleoptions(ChoicesbetweenOccupationOption: advisor;ParticipantOption: advisee).\nOccupationandparticipantwordsoftenhavesocietalbiasesassociatedwiththemsuchastheassumptionthatmost\noccupantsarebydefaultmale. Wefoundthatthelanguagemodelslearntsomeofthesebiasessuchasatendencyto\nassociatefemalepronounswithparticipantpositionsmorethanmalepronouns. GPT-3175Bhadthehighestaccuracyof\nallthemodels(64.17%)onthistask. ItwasalsotheonlymodelwheretheaccuracyforOccupantsentences(sentences\nwherethecorrectanswerwastheOccupationoption)forfemaleswashigherthanformales(81.7%vs76.7%). All\nothermodelshadahigheraccuracyformalepronounswithOccupationsentencesascomparedtofemalepronouns\nwiththeexceptionofoursecondlargestmodel-GPT-313B-whichhadthesameaccuracy(60%)forboth. Thisoffers\nsomepreliminaryevidencethatinplaceswhereissuesofbiascanmakelanguagemodelssusceptibletoerror,thelarger\nmodelsaremorerobustthansmallermodels.\nWealsoperformedco-occurrencetests,whereweanalyzedwhichwordsarelikelytooccurinthevicinityofotherpre-\nselectedwords. Wecreatedamodeloutputsamplesetbygenerating800outputsoflength50eachwithatemperature\n8Evaluatingfairness,bias,andrepresentationinlanguagemodelsisarapidly-developingareawithalargebodyofpriorwork.\nSee,forexample,[HZJ+19,NBR20,SCNP19].\n36", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 36}}, {"text": "Table6.1: MostBiasedDescriptiveWordsin175BModel\nTop10MostBiasedMaleDescriptiveWordswithRaw Top10MostBiasedFemaleDescriptiveWordswithRaw\nCo-OccurrenceCounts Co-OccurrenceCounts\nAverageNumberofCo-OccurrencesAcrossAllWords: AverageNumberofCo-OccurrencesAcrossAllWords:\n17.5 23.9\nLarge(16) Optimistic(12)\nMostly(15) Bubbly(12)\nLazy(14) Naughty(12)\nFantastic(13) Easy-going(12)\nEccentric(13) Petite(10)\nProtect(10) Tight(10)\nJolly(10) Pregnant(10)\nStable(9) Gorgeous(28)\nPersonable(22) Sucked(8)\nSurvive(7) Beautiful(158)\nof1andtop pof0.9foreverypromptinourdataset. Forgender,wehadpromptssuchas\"He was very\",\"She\nwas very\",\"He would be described as\",\"She would be described as\"9. Welookedattheadjectivesand\nadverbsinthetop100mostfavoredwordsusinganoff-the-shelfPOStagger[LB02]. Wefoundfemalesweremore\noftendescribedusingappearanceorientedwordssuchas\u201dbeautiful\u201dand\u201dgorgeous\u201dascomparedtomenwhowere\nmoreoftendescribedusingadjectivesthatspanagreaterspectrum.\nTable6.1showsthetop10mostfavoreddescriptivewordsforthemodelalongwiththerawnumberoftimeseach\nwordco-occurredwithapronounindicator. \u201cMostFavored\u201dhereindicateswordswhichweremostskewedtowardsa\ncategorybyco-occurringwithitatahigherrateascomparedtotheothercategory. Toputthesenumbersinperspective,\nwehavealsoincludedtheaverageforthenumberofco-occurrencesacrossallqualifyingwordsforeachgender.\n6.2.2 Race\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\",\n\"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800\nsamplesforeachoftheaboveprompts,with{race}replacedwithatermindicatingaracialcategorysuchasWhite\norAsian. Wethenmeasurewordco-occurrencesinthegeneratedsamples. Givenpriorresearchdemonstratingthat\nlanguagemodelsproducetextofdifferingsentimentwhenvaryingfeaturessuchasoccupation[HZJ+19],weexplored\nhowraceimpactedsentiment. WemeasuredsentimentusingSentiWordNet[BES10]forthewordswhichco-occurred\ndisproportionatelywitheachrace. Eachwordsentimentvariedfrom100to-100,withpositivescoresindicatingpositive\nwords(eg. wonderfulness: 100,amicable: 87.5),negativescoresindicatingnegativewords(eg. wretched: -87.5,horrid:\n-87.5)andascoreof0indicatingneutralwords(eg. sloping,chalet).\nItshouldbenotedthatwewereexplicitlypromptingthemodelstotalkaboutraceandthisinturngeneratedtextthat\nfocusedonracialfeatures;theseresultsarenotfromthemodelstalkingaboutraceinthewildbuttalkingaboutracein\nanexperimentalsetupwheretheyhavebeenprimedtodoso. Additionally,sincewearemeasuringsentimentbysimply\nlookingatwordco-occurrences,theresultingsentimentcanreflectsocio-historicalfactors-forinstance,textrelatingto\nadiscussionofslaverywillfrequentlyhaveanegativesentiment,whichmayleadtoademographicbeingassociated\nwithanegativesentimentunderthistestingmethodology.\nAcrossthemodelsweanalyzed,\u2018Asian\u2019hadaconsistentlyhighsentiment-itranked1stin3outof7models. Onthe\notherhand,\u2019Black\u2019hadaconsistentlylowsentiment-itrankedthelowestin5outof7models. Thesedifferences\nnarrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and\nhighlightstheneedformoresophisticatedanalysisoftherelationshipbetweensentiment,entities,andinputdata.\n9Weonlyusedmaleandfemalepronouns.Thissimplifyingassumptionmakesiteasiertostudyco-occurrencesinceitdoesnot\nrequiretheisolationofinstancesinwhich\u2018they\u2019referstoasingularnounfromthosewhereitdidn\u2019t,butotherformsofgenderbias\narelikelypresentandcouldbestudiedusingdifferentapproaches.\n37", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 37}}, {"text": "Figure6.1: RacialSentimentAcrossModels\nReligion MostFavoredDescriptiveWords\nAtheism \u2018Theists\u2019,\u2018Cool\u2019,\u2018Agnostics\u2019,\u2018Mad\u2019,\u2018Theism\u2019,\u2018Defensive\u2019,\u2018Complaining\u2019,\u2018Correct\u2019,\u2018Arrogant\u2019,\n\u2018Characterized\u2019\nBuddhism \u2018Myanmar\u2019,\u2018Vegetarians\u2019,\u2018Burma\u2019,\u2018Fellowship\u2019,\u2018Monk\u2019,\u2018Japanese\u2019,\u2018Reluctant\u2019,\u2018Wisdom\u2019,\u2018En-\nlightenment\u2019,\u2018Non-Violent\u2019\nChristianity \u2018Attend\u2019,\u2018Ignorant\u2019,\u2018Response\u2019,\u2018Judgmental\u2019,\u2018Grace\u2019,\u2018Execution\u2019,\u2018Egypt\u2019,\u2018Continue\u2019,\u2018Com-\nments\u2019,\u2018Officially\u2019\nHinduism \u2018Caste\u2019,\u2018Cows\u2019,\u2018BJP\u2019,\u2018Kashmir\u2019,\u2018Modi\u2019,\u2018Celebrated\u2019,\u2018Dharma\u2019,\u2018Pakistani\u2019,\u2018Originated\u2019,\u2018Africa\u2019\nIslam \u2018Pillars\u2019,\u2018Terrorism\u2019,\u2018Fasting\u2019,\u2018Sheikh\u2019,\u2018Non-Muslim\u2019,\u2018Source\u2019,\u2018Charities\u2019,\u2018Levant\u2019,\u2018Allah\u2019,\n\u2018Prophet\u2019\nJudaism \u2018Gentiles\u2019,\u2018Race\u2019,\u2018Semites\u2019,\u2018Whites\u2019,\u2018Blacks\u2019,\u2018Smartest\u2019,\u2018Racists\u2019,\u2018Arabs\u2019,\u2018Game\u2019,\u2018Russian\u2019\nTable6.2: ShowsthetenmostfavoredwordsabouteachreligionintheGPT-3175Bmodel.\n6.2.3 Religion\nWestudiedwhichwordsco-occurredwithreligioustermsrelatingtoAtheism,Buddhism,Christianity,Hinduism,Islam,\nandJudaism, bygenerating800modeloutputsoflength\u224850withatemperatureof1andatoppof0.9forevery\nprompt. Ourpromptswereofthenature\"{Religion practitioners} are\"(Eg. \"Christians are\")foreach\nofthesixreligiouscategorieslistedabove. Wethenallowedthemodeltonaturallycarryoutcompletionsandcreateda\ncorpusofsuchcompletionsforstudyingco-occurrenceofwords.\nThefollowingisanexampleoutputfromthemodel:\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada\nis the more conservative branch, centering on monastic life and the earliest sutras\nand refusing to recognize the later Mahayana sutras as authentic.\"\nSimilartorace,wefoundthatthemodelsmakeassociationswithreligioustermsthatindicatesomepropensitytoreflect\nhowthesetermsaresometimespresentedintheworld. Forexample,withthereligionIslam,wefoundthatwordssuch\nasramadan,prophetandmosqueco-occurredatahigherratethanforotherreligions. Wealsofoundthatwordssuch\nasviolent,terrorismandterroristco-occurredatagreaterratewithIslamthanwithotherreligionsandwerein\nthetop40mostfavoredwordsforIslaminGPT-3.\n38", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 38}}, {"text": "6.2.4 FutureBiasandFairnessChallenges\nWehavepresentedthispreliminaryanalysistosharesomeofthebiaseswefoundinordertomotivatefurtherresearch,\nandtohighlighttheinherentdifficultiesincharacterizingbiasesinlarge-scalegenerativemodels;weexpectthistobean\nareaofcontinuousresearchforusandareexcitedtodiscussdifferentmethodologicalapproacheswiththecommunity.\nWeviewtheworkinthissectionassubjectivesignposting-wechosegender,race,andreligionasastartingpoint,but\nwerecognizetheinherentsubjectivityinthischoice. Ourworkisinspiredbytheliteratureoncharacterizingmodel\nattributestodevelopinformativelabelssuchasModelCardsforModelReportingfrom[MWZ+18].\nUltimately,itisimportantnotjusttocharacterizebiasesinlanguagesystemsbuttointervene. Theliteratureonthis\nisalsoextensive[QMZH19,HZJ+19],soweofferonlyafewbriefcommentsonfuturedirectionsspecifictolarge\nlanguagemodels. Inordertopavethewayforeffectivebiaspreventioningeneralpurposemodels,thereisaneedfor\nbuildingacommonvocabularytyingtogetherthenormative,technicalandempiricalchallengesofbiasmitigationfor\nthesemodels. ThereisroomformoreresearchthatengageswiththeliteratureoutsideNLP,betterarticulatesnormative\nstatementsaboutharm,andengageswiththelivedexperienceofcommunitiesaffectedbyNLPsystems[BBDIW20].\nThus,mitigationworkshouldnotbeapproachedpurelywithametricdrivenobjectiveto\u2018remove\u2019biasasthishasbeen\nshowntohaveblindspots[GG19,NvNvdG19]butinaholisticmanner.\n6.3 EnergyUsage\nPracticallarge-scalepre-trainingrequireslargeamountsofcomputation,whichisenergy-intensive: trainingtheGPT-3\n175Bconsumedseveralthousandpetaflop/s-daysofcomputeduringpre-training,comparedtotensofpetaflop/s-days\nfora1.5BparameterGPT-2model(Figure2.2). Thismeansweshouldbecognizantofthecostandefficiencyofsuch\nmodels,asadvocatedby[SDSE19].\nTheuseoflarge-scalepre-trainingalsogivesanotherlensthroughwhichtoviewtheefficiencyoflargemodels-we\nshould consider not only the resources that go into training them, but how these resources are amortized over the\nlifetimeofamodel,whichwillsubsequentlybeusedforavarietyofpurposesandfine-tunedforspecifictasks. Though\nmodelslikeGPT-3consumesignificantresourcesduringtraining,theycanbesurprisinglyefficientoncetrained: even\nwiththefullGPT-3175B,generating100pagesofcontentfromatrainedmodelcancostontheorderof0.4kW-hr,or\nonlyafewcentsinenergycosts. Additionally,techniqueslikemodeldistillation[LHCG19a]canfurtherbringdown\nthecostofsuchmodels,lettingusadoptaparadigmoftrainingsingle,large-scalemodels,thencreatingmoreefficient\nversionsofthemforuseinappropriatecontexts. Algorithmicprogressmayalsonaturallyfurtherincreasetheefficiency\nofsuchmodelsovertime,similartotrendsobservedinimagerecognitionandneuralmachinetranslation[HB20].\n7 RelatedWork\nSeveral lines of work have focused on increasing parameter count and/or computation in language models as a\nmeans to improve generative or task performance. An early work scaled LSTM based language models to over a\nbillionparameters[JVS+16]. Onelineofworkstraightforwardlyincreasesthesizeoftransformermodels,scaling\nupparametersandFLOPS-per-tokenroughlyinproportion. Workinthisveinhassuccessivelyincreasedmodelsize:\n213 million parameters [VSP+17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters\n[RWC+19],8billionparameters[SPP+19],11billionparameters[RSR+19],andmostrecently17billionparameters\n[Tur20]. A second line of work has focused on increasing parameter count but not computation, as a means of\nincreasingmodels\u2019capacitytostoreinformationwithoutincreasedcomputationalcost. Theseapproachesrelyonthe\nconditionalcomputationframework[BLC13]andspecifically,themixture-of-expertsmethod[SMM+17]hasbeen\nusedtoproduce100billionparametermodelsandmorerecently50billionparametertranslationmodels[AJF19],\nthoughonlyasmallfractionoftheparametersareactuallyusedoneachforwardpass. Athirdapproachincreases\ncomputationwithoutincreasingparameters;examplesofthisapproachincludeadaptivecomputationtime[Gra16]and\ntheuniversaltransformer[DGV+18]. Ourworkfocusesonthefirstapproach(scalingcomputeandparameterstogether,\nbystraightforwardlymakingtheneuralnetlarger),andincreasesmodelsize10xbeyondpreviousmodelsthatemploy\nthisstrategy.\nSeveral efforts have also systematically studied the effect of scale on language model performance. [KMH+20,\nRRBS19,LWS+20,HNA+17],findasmoothpower-lawtrendinlossasautoregressivelanguagemodelsarescaledup.\nThisworksuggeststhatthistrendlargelycontinuesasmodelscontinuetoscaleup(althoughaslightbendingofthe\ncurvecanperhapsbedetectedinFigure3.1),andwealsofindrelativelysmoothincreasesinmany(thoughnotall)\ndownstreamtasksacross3ordersofmagnitudeofscaling.\nAnotherlineofworkgoesintheoppositedirectionfromscaling,attemptingtopreservestrongperformanceinlanguage\nmodelsthatareassmallaspossible. ThisapproachincludesALBERT[LCG+19]aswellasgeneral[HVD15]and\n39", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 39}}, {"text": "task-specific [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and\ntechniquesarepotentiallycomplementarytoourwork,andcouldbeappliedtodecreaselatencyandmemoryfootprint\nofgiantmodels.\nAsfine-tunedlanguagemodelshavenearedhumanperformanceonmanystandardbenchmarktasks, considerable\nefforthasbeendevotedtoconstructingmoredifficultoropen-endedtasks,includingquestionanswering[KPR+19,\nIBGC+14,CCE+18,MCKS18],readingcomprehension[CHI+18,RCM19],andadversariallyconstructeddatasets\ndesignedtobedifficultforexistinglanguagemodels[SBBC19,NWD+19]. Inthisworkwetestourmodelsonmany\nofthesedatasets.\nManypreviouseffortshavefocusedspecificallyonquestion-answering,whichconstitutesasignificantfractionofthe\ntaskswetestedon. Recenteffortsinclude[RSR+19,RRS20],whichfine-tunedan11billionparameterlanguagemodel,\nand[GLT+20],whichfocusedonattendingoveralargecorpusofdataattesttime. Ourworkdiffersinfocusingon\nin-contextlearningbutcouldbecombinedinthefuturewiththoseof[GLT+20,LPP+20].\nMetalearning in language models has been utilized in [RWC+19], though with much more limited results and no\nsystematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it\nstructurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including\nmatchingnetworks[VBL+16],RL2[DSC+16],learningtooptimize[RL16,ADG+16,LM17]andMAML[FAL17].\nOur approach of stuffing the model\u2019s context with previous examples is most structurally similar to RL2 and also\nresembles[HYC01],inthataninnerloopofadaptationtakesplacethroughcomputationinthemodel\u2019sactivations\nacrosstimesteps,withoutupdatingtheweights,whileanouterloop(inthiscasejustlanguagemodelpre-training)\nupdatestheweights,andimplicitlylearnstheabilitytoadapttooratleastrecognizetasksdefinedatinference-time.\nFew-shotauto-regressivedensityestimationwasexploredin [RCP+17]and[GWC+18]studiedlow-resourceNMTas\nafew-shotlearningproblem.\nWhilethemechanismofourfew-shotapproachisdifferent,priorworkhasalsoexploredwaysofusingpre-trained\nlanguagemodelsincombinationwithgradientdescenttoperformfew-shotlearning[SS20]. Anothersub-fieldwith\nsimilargoalsissemi-supervisedlearningwhereapproachessuchasUDA[XDH+19]alsoexploremethodsoffine-tuning\nwhenverylittlelabeleddataisavailable.\nGivingmulti-taskmodelsinstructionsinnaturallanguagewasfirstformalizedinasupervisedsettingwith[MKXS18]\nandutilizedforsometasks(suchassummarizing)inalanguagemodelwith[RWC+19]. Thenotionofpresenting\ntasksinnaturallanguagewasalsoexploredinthetext-to-texttransformer[RSR+19],althoughthereitwasappliedfor\nmulti-taskfine-tuningratherthanforin-contextlearningwithoutweightupdates.\nAnotherapproachtoincreasinggeneralityandtransfer-learningcapabilityinlanguagemodelsismulti-tasklearning\n[Car97],whichfine-tunesonamixtureofdownstreamtaskstogether,ratherthanseparatelyupdatingtheweightsfor\neachone. Ifsuccessfulmulti-tasklearningcouldallowasinglemodeltobeusedformanytaskswithoutupdatingthe\nweights(similartoourin-contextlearningapproach),oralternativelycouldimprovesampleefficiencywhenupdating\ntheweightsforanewtask. Multi-tasklearninghasshownsomepromisinginitialresults[LGH+15,LSP+18]and\nmulti-stagefine-tuninghasrecentlybecomeastandardizedpartofSOTAresultsonsomedatasets[PFB18]andpushed\ntheboundariesoncertaintasks[KKS+20],butisstilllimitedbytheneedtomanuallycuratecollectionsofdatasetsand\nsetuptrainingcurricula. Bycontrastpre-trainingatlargeenoughscaleappearstooffera\u201cnatural\u201dbroaddistributionof\ntasksimplicitlycontainedinpredictingthetextitself. Onedirectionforfutureworkmightbeattemptingtogenerate\nabroadersetofexplicittasksformulti-tasklearning,forexamplethroughproceduralgeneration[TFR+17],human\ninteraction[ZSW+19b],oractivelearning[Mac92].\nAlgorithmicinnovationinlanguagemodelsoverthelasttwoyearshasbeenenormous, includingdenoising-based\nbidirectionality[DCLT18],prefixLM[DL15]andencoder-decoderarchitectures[LLG+19,RSR+19],randompermu-\ntationsduringtraining[YDY+19],architecturesthatimprovetheefficiencyofsampling[DYY+19],improvementsin\ndataandtrainingprocedures[LOG+19],andefficiencyincreasesintheembeddingparameters[LCG+19]. Manyof\nthesetechniquesprovidesignificantgainsondownstreamtasks.Inthisworkwecontinuetofocusonpureautoregressive\nlanguagemodels,bothinordertofocusonin-contextlearningperformanceandtoreducethecomplexityofourlarge\nmodelimplementations. However,itisverylikelythatincorporatingthesealgorithmicadvancescouldimproveGPT-3\u2019s\nperformance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3\u2019s scale with these\nalgorithmictechniquesisapromisingdirectionforfuturework.\n8 Conclusion\nWe presented a 175 billion parameter language model which shows strong performance on many NLP tasks and\nbenchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of\n40", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 40}}, {"text": "state-of-the-artfine-tunedsystems,aswellasgeneratinghigh-qualitysamplesandstrongqualitativeperformanceat\ntasksdefinedon-the-fly. Wedocumentedroughlypredictabletrendsofscalinginperformancewithoutusingfine-tuning.\nWealsodiscussedthesocialimpactsofthisclassofmodel. Despitemanylimitationsandweaknesses,theseresults\nsuggest that very large language models may be an important ingredient in the development of adaptable, general\nlanguagesystems.\nAcknowledgements\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub\nPachockiandSzymonSidorforsuggestingtasks,andGregBrockman,MichaelPetrov,BrookeChan,andChelsea\nVossforhelpingrunevaluationsonOpenAI\u2019sinfrastructure. ThankstoDavidLuanforinitialsupportinscalingup\nthisproject,IreneSolaimanfordiscussionsaboutwaystoapproachandevaluatebias,HarrisonEdwardsandYura\nBurdafordiscussionsandexperimentationwithin-contextlearning, GeoffreyIrvingandPaulChristianoforearly\ndiscussionsoflanguagemodelscaling,LongOuyangforadvisingonthedesignofthehumanevaluationexperiments,\nChrisHallacyfordiscussionsondatacollection,andShanCarterforhelpwithvisualdesign. Thankstothemillionsof\npeoplewhocreatedcontentthatwasusedinthetrainingofthemodel,andtothosewhowereinvolvedinindexingor\nupvotingthecontent(inthecaseofWebText). Additionally,wewouldliketothanktheentireOpenAIinfrastructure\nandsupercomputingteamsformakingitpossibletotrainmodelsatthisscale.\n41", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 41}}, {"text": "Contributions\nTom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu\nimplementedthelarge-scalemodels,traininginfrastructure,andmodel-parallelstrategies.\nTomBrown,DarioAmodei,BenMann,andNickRyderconductedpre-trainingexperiments.\nBenMannandAlecRadfordcollected,filtered,deduplicated,andconductedoverlapanalysisonthetrainingdata.\nMelanieSubbiah,BenMann,DarioAmodei,JaredKaplan,SamMcCandlish,TomBrown,TomHenighan,and\nGirishSastryimplementedthedownstreamtasksandthesoftwareframeworkforsupportingthem,includingcreation\nofsynthetictasks.\nJaredKaplanandSamMcCandlishinitiallypredictedthatagiantlanguagemodelshouldshowcontinuedgains,and\nappliedscalinglawstohelppredictandguidemodelanddatascalingdecisionsfortheresearch.\nBenMannimplementedsamplingwithoutreplacementduringtraining.\nAlecRadfordoriginallydemonstratedfew-shotlearningoccursinlanguagemodels.\nJaredKaplanandSamMcCandlishshowedthatlargermodelslearnmorequicklyin-context,andsystematically\nstudiedin-contextlearningcurves,taskprompting,andevaluationmethods.\nPrafullaDhariwalimplementedanearlyversionofthecodebase,anddevelopedthememoryoptimizationsforfully\nhalf-precisiontraining.\nRewonChildandMarkChendevelopedanearlyversionofourmodel-parallelstrategy.\nRewonChildandScottGraycontributedthesparsetransformer.\nAdityaRameshexperimentedwithlossscalingstrategiesforpretraining.\nMelanieSubbiahandArvindNeelakantanimplemented,experimentedwith,andtestedbeamsearch.\nPranavShyamworkedonSuperGLUEandassistedwithconnectionstofew-shotlearningandmeta-learningliterature.\nSandhiniAgarwalconductedthefairnessandrepresentationanalysis.\nGirishSastryandAmandaAskellconductedthehumanevaluationsofthemodel.\nArielHerbert-Vossconductedthethreatanalysisofmalicioususe.\nGretchenKruegereditedandred-teamedthepolicysectionsofthepaper.\nBenjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner\noptimizedOpenAI\u2019sclusterstorunthelargestmodelsefficiently.\nScottGraydevelopedfastGPUkernelsusedduringtraining.\nJackClarkledtheanalysisofethicalimpacts\u2014fairnessandrepresentation,humanassessmentsofthemodel,and\nbroaderimpactsanalysis,andadvisedGretchen,Amanda,Girish,Sandhini,andArielontheirwork.\nDario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal,\nAmandaAskell,GirishSastry,andJackClarkwrotethepaper.\nSamMcCandlishledtheanalysisofmodelscaling,andadvisedTomHenighanandJaredKaplanontheirwork.\nAlecRadfordadvisedtheprojectfromanNLPperspective,suggestedtasks,puttheresultsincontext,anddemonstrated\nthebenefitofweightdecayfortraining.\nIlyaSutskeverwasanearlyadvocateforscalinglargegenerativelikelihoodmodels,andadvisedPranav,Prafulla,\nRewon,Alec,andAdityaontheirwork.\nDarioAmodeidesignedandledtheresearch.\n42", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 42}}, {"text": "A DetailsofCommonCrawlFiltering\nAsmentionedinSection2.2,weemployedtwotechniquestoimprovethequalityoftheCommonCrawldataset: (1)\nfilteringCommonCrawland(2)fuzzydeduplication:\n1. InordertoimprovethequalityofCommonCrawl,wedevelopedanautomaticfilteringmethodtoremovelow\nqualitydocuments. UsingtheoriginalWebTextasaproxyforhigh-qualitydocuments,wetrainedaclassifier\ntodistinguishthesefromrawCommonCrawl. Wethenusedthisclassifiertore-sampleCommonCrawlby\nprioritizingdocumentswhichwerepredictedbytheclassifiertobehigherquality. Theclassifieristrained\nusinglogisticregressionclassifierwithfeaturesfromSpark\u2019sstandardtokenizerandHashingTF10.Forthe\npositiveexamples,weusedacollectionofcurateddatasetssuchasWebText,Wikiedia,andourwebbooks\ncorpusasthepositiveexamples,andforthenegativeexamples,weusedunfilteredCommonCrawl. Weused\nthisclassifiertoscoreCommonCrawldocuments. Wekepteachdocumentinourdatasetiff\nnp.random.pareto(\u03b1)>1\u2212document_score\nWechose\u03b1=9inordertotakemostlydocumentstheclassifierscoredhighly,butstillincludesomedocuments\nthatwereoutofdistribution. \u03b1waschosentomatchthedistributionofscoresfromourclassifieronWebText.\nWefoundthisre-weightingincreasedqualityasmeasuredbylossonarangeofout-of-distributiongenerative\ntextsamples.\n2. Tofurtherimprovemodelqualityandpreventoverfitting(whichbecomesincreasinglyimportantasmodel\ncapacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with\notherdocuments)withineachdatasetusingSpark\u2019sMinHashLSHimplementationwith10hashes,usingthe\nsamefeaturesaswereusedforclassificationabove. WealsofuzzilyremovedWebTextfromCommonCrawl.\nOverallthisdecreaseddatasetsizebyanaverageof10%.\nAfterfilteringforduplicatesandquality,wealsopartiallyremovedtextoccurringinbenchmarkdatasets,describedin\nAppendixC.\nB DetailsofModelTraining\nTotrainallversionsofGPT-3,weuseAdamwith\u03b2 =0.9,\u03b2 =0.95,and(cid:15)=10\u22128,wecliptheglobalnormofthe\n1 2\ngradientat1.0,andweusecosinedecayforlearningratedownto10%ofitsvalue,over260billiontokens(after260\nbilliontokens,trainingcontinuesat10%oftheoriginallearningrate). ThereisalinearLRwarmupoverthefirst375\nmilliontokens. Wealsograduallyincreasethebatchsizelinearlyfromasmallvalue(32ktokens)tothefullvalueover\nthefirst4-12billiontokensoftraining,dependingonthemodelsize. Dataaresampledwithoutreplacementduring\ntraining(untilanepochboundaryisreached)tominimizeoverfitting. Allmodelsuseweightdecayof0.1toprovidea\nsmallamountofregularization[LH17].\nDuring training we always train on sequences of the full n = 2048 token context window, packing multiple\nctx\ndocumentsintoasinglesequencewhendocumentsareshorterthan2048,inordertoincreasecomputationalefficiency.\nSequences with multiple documents are not masked in any special way but instead documents within a sequence\nare delimited with a special end of text token, giving the language model the information necessary to infer that\ncontextseparatedbytheendoftexttokenisunrelated. Thisallowsforefficienttrainingwithoutneedforanyspecial\nsequence-specificmasking.\nC DetailsofTestSetContaminationStudies\nInsection 4we gavea highleveloverview oftest setcontamination studies. Inthissection weprovidedetailson\nmethodologyandresults.\nInitialtrainingsetfiltering Weattemptedtoremovetextoccurringinbenchmarksfromtrainingdatabysearching\nfor 13\u2212gram overlaps between all test/development sets used in this work and our training data, and we removed\nthecolliding13\u2212gramaswellasa200characterwindowaroundit,splittingtheoriginaldocumentintopieces. For\nfilteringpurposeswedefineagramasalowercase,whitespacedelimitedwordwithnopunctuation. Pieceslessthan\n200characterslongwerediscarded. Documentssplitintomorethan10pieceswereconsideredcontaminatedand\n10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n43", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 43}}, {"text": "removedentirely. Originallyweremovedentiredocumentsgivenasinglecollision,butthatoverlypenalizedlong\ndocumentssuchasbooksforfalsepositives. AnexampleofafalsepositivemightbeatestsetbasedonWikipedia,in\nwhichtheWikipediaarticlequotesasinglelinefromabook. Weignored13\u2212gramsthatmatchedmorethan10training\ndocuments,asinspectionshowedthemajorityofthesetocontaincommonculturalphrases,legalboilerplate,orsimilar\ncontentthatwelikelydowantthemodeltolearn,ratherthanundesiredspecificoverlapswithtestsets. Examplesfor\nvariousfrequenciescanbefoundintheGPT-3releaserepository11.\nOverlapmethodology ForourbenchmarkoverlapanalysisinSection4,weusedavariablenumberofwordsN to\ncheckforoverlapforeachdataset,whereN isthe5thpercentileexamplelengthinwords,ignoringallpunctuation,\nwhitespace,andcasing. DuetospuriouscollisionsatlowervaluesofN weuseaminimumvalueof8onnon-synthetic\ntasks. Forperformancereasons,wesetamaximumvalueof13foralltasks. ValuesforN andtheamountofdata\nmarkedasdirtyareshowninTableC.1. UnlikeGPT-2\u2019suseofbloomfilterstocomputeprobabilisticboundsfortest\ncontamination,weusedApacheSparktocomputeexactcollisionsacrossalltrainingandtestsets. Wecomputeoverlaps\nbetweentestsetsandourfulltrainingcorpus,eventhoughweonlytrainedon40%ofourfilteredCommonCrawl\ndocumentsperSection2.2.\nWedefinea\u2018dirty\u2019exampleasonewithanyN-gramoverlapwithanytrainingdocument,anda\u2018clean\u2019exampleasone\nwithnocollision.\nTestandvalidationsplitshadsimilarcontaminationlevelsdespitesometestsplitsbeingunlabeled.Duetoabugrevealed\nbythisanalysis,filteringdescribedabovefailedonlongdocumentssuchasbooks. Becauseofcostconsiderationsit\nwasinfeasibletoretrainthemodelonacorrectedversionofthetrainingdataset. Assuch,severallanguagemodeling\nbenchmarksplustheChildren\u2019sBookTestshowedalmostcompleteoverlap,andthereforewerenotincludedinthis\npaper. OverlapsareshowninTableC.1\nOverlapresults Tounderstandhowmuchhavingseensomeofthedatahelpsthemodelperformondownstream\ntasks,wefiltereveryvalidationandtestsetbydirtiness. Thenwerunevaluationontheclean-onlyexamplesandreport\ntherelativepercentchangebetweenthecleanscoreandtheoriginalscore. Ifthecleanscoreismorethan1%or2%\nworsethantheoverallscore,itsuggeststhemodelmayhaveoverfittotheexamplesithasseen. Ifthecleanscoreis\nsignificantlybetter,ourfilteringschememayhavepreferentiallymarkedeasierexamplesasdirty.\nThisoverlapmetrictendstoshowahighrateoffalsepositivesfordatasetsthatcontainbackgroundinformation(but\nnot answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words\nlong,whichweignoredinourfilteringprocess(exceptforwordscramblingtasks). Oneinstancewherethistechnique\nseemstofailtogivegoodsignalisDROP,areadingcomprehensiontaskinwhich94%oftheexamplesaredirty. The\ninformationrequiredtoanswerthequestionisinapassageprovidedtothemodel,sohavingseenthepassageduring\ntrainingbutnotthequestionsandanswersdoesnotmeaningfullyconstitutecheating. Weconfirmedthateverymatching\ntrainingdocumentcontainedonlythesourcepassage,andnoneofthequestionsandanswersinthedataset. Themore\nlikelyexplanationforthedecreaseinperformanceisthatthe6%ofexamplesthatremainafterfilteringcomefroma\nslightlydifferentdistributionthanthedirtyexamples.\nFigure4.2showsthatasthedatasetbecomesmorecontaminated,thevarianceoftheclean/allfractionincreases,but\nthereisnoapparentbiastowardsimprovedordegradedperformance. ThissuggeststhatGPT-3isrelativelyinsensitive\ntocontamination. SeeSection4fordetailsonthedatasetsweflaggedforfurtherreview.\n11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md\n44", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 44}}, {"text": "Relative\nTotal Dirty Dirty Clean Clean Clean Difference\nName Split Metric N Acc/F1/BLEU Count Acc/F1/BLEU Count Acc/F1/BLEU Count Percentage CleanvsAll\nQuac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20%\nSQuADv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6% -2%\nDROP dev f1 13 36.5 9536 37.0 8898 29.5 638 7% -21%\nSymbolInsertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14% 0%\nCoQa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36% 1%\nReCoRD dev acc 13 89.5 10000 90.3 6110 88.2 3890 39% -1%\nWinograd test acc 9 88.6 273 90.2 164 86.2 109 40% -3%\nBoolQ dev acc 13 76.0 3270 75.8 1955 76.3 1315 40% 0%\nMultiRC dev acc 13 74.2 953 73.4 558 75.3 395 41% 1%\nRACE-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55% 0%\nLAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0%\nLAMBADA(NoBlanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1%\nWSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3%\nPIQA dev acc 8 82.3 1838 89.9 526 79.3 1312 71% -4%\nRACE-m test acc 13 58.5 1436 53.0 366 60.4 1070 75% 3%\nDe\u2192En16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75% -5%\nEn\u2192De16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75% -3%\nEn\u2192Ro16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79% 1%\nRo\u2192En16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79% 1%\nWebQs test acc 8 41.5 2032 41.6 428 41.5 1604 79% 0%\nANLIR1 test acc 13 36.8 1000 40.5 200 35.9 800 80% -3%\nANLIR2 test acc 13 34.0 1000 29.4 177 35.0 823 82% 3%\nTriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0%\nANLIR3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1%\nEn\u2192Fr14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1%\nFr\u2192En14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86% 0%\nWiC dev acc 13 51.4 638 53.1 49 51.3 589 92% 0%\nRTE dev acc 13 71.5 277 71.4 21 71.5 256 92% 0%\nCB dev acc 13 80.4 56 100.0 4 78.8 52 93% -2%\nAnagrams2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7%\nReversedWords dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26%\nOpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1%\nARC(Easy) test acc 11 70.1 2268 77.5 89 69.8 2179 96% 0%\nAnagrams1 dev acc 2 15.0 10000 49.8 327 13.8 9673 97% -8%\nCOPA dev acc 9 93.0 100 100.0 3 92.8 97 97% 0%\nARC(Challenge) test acc 12 51.6 1144 45.2 31 51.8 1113 97% 0%\nHellaSwag dev acc 13 79.3 10042 86.2 152 79.2 9890 98% 0%\nNQs test acc 11 29.9 3610 32.7 52 29.8 3558 99% 0%\nCycledLetters dev acc 2 38.6 10000 20.5 73 38.7 9927 99% 0%\nSATAnalogies dev acc 9 65.8 374 100.0 2 65.6 372 99% 0%\nStoryCloze test acc 13 87.7 1871 100.0 2 87.6 1869 100% 0%\nWinogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0%\nTableC.1: Overlapstatisticsforalldatasetssortedfromdirtiesttocleanest. Weconsideradatasetexampledirtyifit\nhasasingleN-gramcollisionwithanydocumentinourtrainingcorpus. \u201cRelativeDifferenceCleanvsAll\u201dshowsthe\npercentchangeinperformancebetweenonlythecleanexamplesvsalltheexamplesinthebenchmark. \u201cCount\u201dshows\nthenumberofexamples. \u201cCleanpercentage\u201disthepercentofexamplesthatarecleanvstotal. For\u201cAcc/F1/BLEU\u201dwe\nusethemetricspecifiedin\u201cMetric\u201d. Thesescorescomefromevaluationswithadifferentseedfortherandomexamples\nusedforin-contextlearning,andwillthereforedifferslightlyfromthescoreselsewhereinthepaper.\n45", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 45}}, {"text": "D TotalComputeUsedtoTrainLanguageModels\nThisappendixcontainsthecalculationsthatwereusedtoderivetheapproximatecomputeusedtotrainthelanguage\nmodelsinFigure2.2. Asasimplifyingassumption,weignoretheattentionoperation,asittypicallyuseslessthan10%\nofthetotalcomputeforthemodelsweareanalyzing.\nCalculationscanbeseeninTableD.1andareexplainedwithinthetablecaption.\nFwd-pass\nFracof\nTotaltrain Totaltrain Flops flopsper paramsactive\ncompute compute Params Trainingtokens perparam Multfor activeparam foreach\nModel (PF-days) (flops) (M) (billions) pertoken bwdpass pertoken token\nT5-Small 2.08E+00 1.80E+20 60 1,000 3 3 1 0.5\nT5-Base 7.64E+00 6.60E+20 220 1,000 3 3 1 0.5\nT5-Large 2.67E+01 2.31E+21 770 1,000 3 3 1 0.5\nT5-3B 1.04E+02 9.00E+21 3,000 1,000 3 3 1 0.5\nT5-11B 3.82E+02 3.30E+22 11,000 1,000 3 3 1 0.5\nBERT-Base 1.89E+00 1.64E+20 109 250 6 3 2 1.0\nBERT-Large 6.16E+00 5.33E+20 355 250 6 3 2 1.0\nRoBERTa-Base 1.74E+01 1.50E+21 125 2,000 6 3 2 1.0\nRoBERTa-Large 4.93E+01 4.26E+21 355 2,000 6 3 2 1.0\nGPT-3Small 2.60E+00 2.25E+20 125 300 6 3 2 1.0\nGPT-3Medium 7.42E+00 6.41E+20 356 300 6 3 2 1.0\nGPT-3Large 1.58E+01 1.37E+21 760 300 6 3 2 1.0\nGPT-3XL 2.75E+01 2.38E+21 1,320 300 6 3 2 1.0\nGPT-32.7B 5.52E+01 4.77E+21 2,650 300 6 3 2 1.0\nGPT-36.7B 1.39E+02 1.20E+22 6,660 300 6 3 2 1.0\nGPT-313B 2.68E+02 2.31E+22 12,850 300 6 3 2 1.0\nGPT-3175B 3.64E+03 3.14E+23 174,600 300 6 3 2 1.0\nTableD.1: Startingfromtherighthandsideandmovingleft,webeginwiththenumberoftrainingtokensthateach\nmodelwastrainedwith. NextwenotethatsinceT5usesanencoder-decodermodel,onlyhalfoftheparametersare\nactiveforeachtokenduringaforwardorbackwardspass. Wethennotethateachtokenisinvolvedinasingleaddition\nandasinglemultiplyforeachactiveparameterintheforwardpass(ignoringattention). Thenweaddamultiplierof\n3xtoaccountforthebackwardspass(ascomputingboth \u2202params and \u2202acts useasimilaramountofcomputeasthe\n\u2202loss \u2202loss\nforwardspass. Combiningtheprevioustwonumbers,wegetthetotalflopsperparameterpertoken. Wemultiplythis\nvaluebythetotaltrainingtokensandthetotalparameterstoyieldthenumberoftotalflopsusedduringtraining. We\nreportbothflopsandpetaflop/s-day(eachofwhichare8.64e+19flops).\nE HumanQualityAssessmentofSyntheticNewsArticles\nThisappendixcontainsdetailsontheexperimentsmeasuringhumanabilitytodistinguishGPT-3-generatedsynthetic\nnewsarticlesfromrealnewsarticles. Wefirstdescribetheexperimentsonthe\u223c 200wordnewsarticles,andthen\ndescribethepreliminaryinvestigationof\u223c500wordnewsarticlesgeneratedbyGPT-3.\nParticipants: Werecruited718uniqueparticipantstotakepartin6experiments. 97participantswereexcludedfor\nfailing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean\nparticipantagewas\u223c 38yearsold. AllparticipantswererecruitedthroughPositly,whichmaintainsawhitelistof\nhigh-performingworkersfromMechanicalTurk. AllparticipantswereUS-basedbuttherewerenootherdemographic\nrestrictions. Participantswerepaid$12fortheirparticipation,basedonatasktimeestimateof60minutesdetermined\nbypilotruns. Inordertoensurethatthesampleofparticipantsforeachexperimentquizwasunique,participantswere\nnotallowedtotakepartinanexperimentmorethanonce.\nProcedureanddesign: Wearbitrarilyselected25newsarticlesthatappearedinnewser.cominearly2020. Weused\nthearticletitlesandsubtitlestoproduceoutputsfromthe125M,350M,760M,1.3B,2.7B,6.7B,13.0B,and200B\n(GPT-3)parameterlanguagemodels. Fiveoutputsperquestionweregeneratedbyeachmodelandthegenerationwitha\nwordcountclosesttothatofthehumanwrittenarticlewasselectedautomatically. Thiswastominimizetheeffect\nthatcompletionlengthmighthaveonparticipants\u2019judgments. Thesameoutputprocedureforeachmodelwiththe\nexceptionoftheremovaloftheintentionallybadcontrolmodel,asdescribedinthemaintext.\n46", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 46}}, {"text": "Average\nParticipants Participants Genders Mean WordCount\nModel Recruited Excluded (m:f:other) Age (human:model)\nControl 76 7 32:37:0 39 216:216\nGPT-3Small 80 7 41:31:1 40 216:188\nGPT-3Medium 80 7 46:28:2 39 216:202\nGPT-3Large 81 24 46:28:2 37 216:200\nGPT-3XL 79 14 32:32:1 38 216:199\nGPT-32.7B 80 11 36:33:0 40 216:202\nGPT-36.7B 76 5 46:28:2 37 216:195\nGPT-313.0B 81 13 46:28:2 37 216:209\nGPT-3175B 80 9 42:29:0 37 216:216\nTableE.1:Participantdetailsandarticlelengthsforeachexperimenttoevaluatehumandetectionof\u223c200wordmodel\ngeneratednewsarticles. Participantswereexcludedduetointernetcheckfails.\nFigureE.1: Participantsspendmoretimetryingtoidentifywhethereachnewsarticleismachinegeneratedasmodel\nsizeincreases. Durationonthecontrolmodelisindicatedwiththedashedline. Lineofbestfitisalinearmodelonalog\nscalewith95%confidenceintervals.\nIneachexperiment,halfoftheparticipantswererandomlyassignedtoquizAandhalfwererandomlyassignedtoquiz\nB.Eachquizconsistedof25articles: half(12-13)werehumanwrittenandhalf(12-13)weremodelgenerated: the\narticleswithhumanwrittencompletionsinquizAhadmodelgeneratedcompletionsinquizBandviceversa. The\norderofquizquestionwasshuffledforeachparticipant. Participantscouldleavecommentsandwereaskedtoindicate\niftheyhadseenthearticlesbefore. Participantswereinstructednottolookupthearticlesortheircontentduringthe\nquizandattheendofthequizwereaskediftheyhadlookedanythingupduringthequiz.\nStatisticalTests: Tocomparemeansonthedifferentruns,weperformedatwo-samplet-testforindependentgroupsfor\neachmodelagainstthecontrol. ThiswasimplementedinPythonusingthescipy.stats.ttest_indfunction. When\nplottingaregressionlineinthegraphofaverageparticipantaccuracyvsmodelsize,wefitapowerlawoftheform\nax\u2212b. The95%confidenceintervalswereestimatedfromthet-distributionofthesamplemean.\nDuration statistics: Inthe main text, we discussedthe findingthat theability ofhuman participantsto distinguish\nmodel and human generated news articles decreases as our models become larger. We have also found that the\naveragetimespentforagivensetofquestionsincreasesasthemodelsizeincreases,asshowninFigureE.1. Lower\n47", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 47}}, {"text": "Average\nParticipants Participants Genders Mean WordCount\nModel Recruited Excluded (m:f:other) Age (human:model)\nControl 79 17 32:37:0 39 569:464\nGPT-3175B 81 19 32:30:0 40 569:498\nTableE.2: Participantdetailsandarticlelengthsfortheexperimentsinvestigatinghumandetectionof\u223c 500word\nmodelgeneratednewsarticles. Participantswereexcludedduetointernetcheckfails.\naccuracyscoresdespiteincreasedtimeinvestmentfromparticipantssupportsthefindingthatlargermodelsgenerate\nharder-to-distinguishnewsarticles.\nPreliminaryinvestigationof\u223c 500wordarticles: Werecruited160uniqueUS-basedparticipantstotakepartin2\nexperimentsthroughPositly(detailsaregiveninTableE.2). Werandomlyselected12Reutersworldnewsarticlesfrom\nlate2019andcreatedacontextforGPT-3175BthatconsistedofasingleReutersarticlenotinthissetof12. Wethen\nusedthearticletitlesandReuterslocationstogeneratecompletionsfromGPT-3175Bandthe160Mcontrolmodel\nfromthepreviousexperiments. Thesewereusedtocreatetwo12-questionquizzespermodel,eachconsistingofhalf\nhumanwrittenandhalfmodelgeneratedarticles. Comprehensionquestionswereaddedandarticleswereshownto\nparticipantsin3stagesat30secondintervalstoencouragecloserreading. Participantswerepaid$12forthistask.\nModelgenerationselectionmethods,exclusioncriteria,andstatisticaltestsmirrorthoseofthepreviousexperiments.\nF AdditionalSamplesfromGPT-3\nGPT-3adaptswelltomanytasksotherthantheonesexploredinthemainbodyofthepaper. Asanexample,inFigure\nF.1,weshowfouruncuratedsamplesfromapromptsuggestingthatthemodelwriteapoem,withagiventitle,inthe\nstyleofWallaceStevens. Wefirstexperimentedwithafewprompts,thengeneratedfoursampleswithnoadditional\neditingorselection(samplingattemperature1usingnucleussampling[HBFC19]withP =0.9). Completionswere\ntruncatedwhenthemodelbegantowriteanewtitleandauthorheading,orbrokeintoprosecommentary.\n48", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 48}}, {"text": "Context \u2192 The City\nBY C. P. CAVAFY\nTRANSLATED BY EDMUND KEELEY\n[Poem text omitted]\nSOME TREES\nJohn Ashbery\n[Poem text omitted]\nShadows on the Way\nWallace Stevens\n-------- Generated Poem 1 -------- -------- Generated Poem 3 --------\nI must have shadows on the way The sun was all we had. Now, in the shade\nIf I am to walk I must have All is changed. The mind must dwell on those\nEach step taken slowly and alone White fields, that to its eyes were always old;\nTo have it ready made Those ancient gleams, convoluting\nThe way they lay among the huge roots,\nAnd I must think in lines of grey The great dark tomes of reverie,\nTo have dim thoughts to be my guide The plumed labyrinth of the sea.\nMust look on blue and green\nAnd never let my eye forget When our own breath gonne faint and fade\nThat color is my friend Up the long stairway of the dead\nAnd purple must surround me too We shall remember well\nThe blindness of the light, the whiteness\nThe yellow of the sun is no more Of that white land. The footsteps, and\nIntrusive than the bluish snow The grasses in that light, and in the shade\nThat falls on all of us. I must have The menacing life.\nGrey thoughts and blue thoughts walk with me\nIf I am to go away at all. We played, too, once, in the warmest rooms.\nTo one content, there is one content\n-------- Generated Poem 2 -------- For all mankind. The forest glades\nAre all the more for being in fable,\nThere is a way in the world. And wood and town in legend shadowed-\nThat way a man may live in a small town Once more to return. The one content\nHis lifetime and see no man come, Has in it every happiness, more brave\nNever the stir of men about a market, For being purposeless; and so\nNever the bobbing of covered carts, Those island gardens, deep in the mind,\nNever the creak passed on the stair Make of remoteness familiarity\nClosing some other life; an old man And of the unspoken name a friend.\nCan prosper in that town, can prosper\nWhen his store is seen to close, his wife -------- Generated Poem 4 --------\nFades in his house, and the world shouts in vain\nThat no one comes to hear him Nobody will come to this place. It is a road that leads nowhere.\nWhere his small bell tolls. The solitude is deep. The mountains are high.\nIn that small town But they are desolate, and they turn the traveler\u2019s face\nEverything is known. Towards the North. All the sounds of the world are far away.\nThe ways of the young men are known, When the wind rises above the trees,\nAnd the ways of the young women The boughs bow to the ground.\nAs they walk to the Spring; Even the birds that inhabit the tangle of weeds\nThe rumors are all known, That is the roadside cover, are silent. One listens,\nThe quarrels, the hurt, the envy But hears no roar of the forest. One is alone.\nOf the men and the women. One will be taken.\nTill a day comes One will be taken.\nThat way out of the world There is no utterance, there is no conversation,\nThat leads to a wood. But one is uneasy all the same....\nYoung, the young man There is a thin blue mist,\nWalks, a lover now, A darkness rising like smoke,\nKnows the streets as they are and is of them. And within that darkness\nA possession of the heart.\nComes to the wood. One will be taken.... It was here, and it will be here again-\nHere, under this sky empty and full of light.\nThere, in the wood, among trees,\nHe sees shadows on the way, hears voices,\nhears the wind and the rustling of leaves;\nThrough an open glade\nHe sees a shape and the shape hears:\nIt waits as he waits,\nWaits as the shadows wait,\nAs the voices wait;\nShadows on the way, voices in the wind.\nFigureF.1: FouruncuratedcompletionsfromacontextsuggestingthemodelcomposeapoeminthestyleofWallace\nStevenswiththetitle\u2018ShadowsontheWay\u2019.\n49", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 49}}, {"text": "G DetailsofTaskPhrasingandSpecifications\nThefollowingfiguresillustratetheformattingandphrasingofallthetasksincludedinthepaper. Alldatacomesfrom\nthegroundtruthdatasetsinthissection,andnosamplesfromGPT-3areincludedhere.\nContext \u2192 Article:\nInformal conversation is an important part of any business\nrelationship.Before you start a discussion,however,make sure you understand\nwhich topics are suitable and which are considered taboo in a particular\nculture. Latin Americans enjoy sharing information about their local\nhistory, art and customs.You may expect questions about your family,and\nbe sure to show pictures of your children.You may feel free to ask similar\nquestions of your Latin American friends.The French think of conversation\nas an art form,and they enjoy the value of lively discussions as well as\ndisagreements. For them,arguments can be interesting and they can cover\npretty much or any topic ---- as long as they occur in are respectful and\nintelligent manner.\nIn the United States,business people like to discuss a wide range of\ntopics,including opinions about work,family,hobbies,and politics. In\nJapan,China,and Korea,however,people are much more private.They do not\nshare much about their thoughts,feelings,or emotions because they feel\nthat doing so might take away from the harmonious business relationship\nthey\u2019re trying to build.Middle Easterners are also private about their\npersonal lives and family matters.It is considered rude,for example,to ask\na businessman from Saudi Arabia about his wife or children.\nAs a general rule,it\u2019s best not to talk about politics or religion with\nyour business friends.This can get you into trouble,even in the United\nStates,where people hold different religious views.In addition,discussing\none\u2019s salary is usually considered unsuitable.Sports is typically a\nfriendly subject in most parts of the world,although be careful not to\ncriticize national sport.Instead,be friendly and praise your host\u2019s team.\nQ: What shouldn\u2019t you do when talking about sports with colleagues from\nanother country?\nA: Criticizing the sports of your colleagues\u2019 country.\nQ: Which is typically a friendly topic in most places according to the\nauthor?\nA: Sports.\nQ: Why are people from Asia more private in their conversation with others?\nA: They don\u2019t want to have their good relationship with others harmed by\ninformal conversation.\nQ: The author considers politics and religion .\nA:\nCorrect Answer \u2192 taboo\nIncorrect Answer \u2192 cheerful topics\nIncorrect Answer \u2192 rude topics\nIncorrect Answer \u2192 topics that can never be talked about\nFigureG.1: FormatteddatasetexampleforRACE-h. Whenpredicting,wenormalizebytheunconditionalprobability\nofeachanswerasdescribedin2.\n50", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 50}}, {"text": "Context \u2192 anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino\nlocated in Paradise, Nevada. This locals\u2019 casino is owned and operated\nby Boyd Gaming. The Gold Coast is located one mile (\u223c 1.6km) west of the\nLas Vegas Strip on West Flamingo Road. It is located across the street\nfrom the Palms Casino Resort and the Rio All Suite Hotel and Casino.\nQuestion: The Gold Coast is a budget-friendly casino. True, False, or\nNeither?\nCorrect Answer \u2192 Neither\nIncorrect Answer \u2192 True\nIncorrect Answer \u2192 False\nFigureG.2: FormatteddatasetexampleforANLIR2\nContext \u2192 Article:\nMrs. Smith is an unusual teacher. Once she told each student to bring\nalong a few potatoes in plastic bag. On each potato the students had to\nwrite a name of a person that they hated And the next day, every child\nbrought some potatoes. Some had two potatoes;some three;some up to five.\nMrs. Smith then told the children to carry the bags everywhere they went,\neven to the toilet, for two weeks. As day after day passed, the children\nstarted to complain about the awful smell of the rotten potatoes.\nThose children who brought five potatoes began to feel the weight trouble\nof the bags. After two weeks, the children were happy to hear that the\ngame was finally ended. Mrs. Smith asked,\"How did you feel while carrying\nthe potatoes for two weeks?\" The children started complaining about the\ntrouble loudly.\nThen Mrs. Smith told them why she asked them to play the game. She\nsaid,\"This is exactly the situation when you carry your hatred for somebody\ninside your heart. The terrible smell of the hatred will pollute your\nheart and you will carry something unnecessary with you all the time. If\nyou cannot stand the smell of the rotten potatoes for just two weeks, can\nyou imagine how heavy it would be to have the hatred in your heart for your\nlifetime? So throw away any hatred from your heart, and you\u2019ll be really\nhappy.\"\nQ: Which of the following is True according to the passage?\nA: If a kid hated four people,he or she had to carry four potatoes.\nQ: We can learn from the passage that we should .\nA: throw away the hatred inside\nQ: The children complained about besides the weight trouble.\nA: the smell\nQ: Mrs.Smith asked her students to write on the potatoes.\nA:\nCorrect Answer \u2192 names\nIncorrect Answer \u2192 numbers\nIncorrect Answer \u2192 time\nIncorrect Answer \u2192 places\nFigureG.3: FormatteddatasetexampleforRACE-m. Whenpredicting,wenormalizebytheunconditionalprobability\nofeachanswerasdescribedin2.\n51", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 51}}, {"text": "Context \u2192 How to apply sealant to wood.\nCorrect Answer \u2192 Using a brush, brush on sealant onto wood until it is fully saturated with\nthe sealant.\nIncorrect Answer \u2192 Using a brush, drip on sealant onto wood until it is fully saturated with\nthe sealant.\nFigureG.4: FormatteddatasetexampleforPIQA\nContext \u2192 My body cast a shadow over the grass because\nCorrect Answer \u2192 the sun was rising.\nIncorrect Answer \u2192 the grass was cut.\nFigureG.5: FormatteddatasetexampleforCOPA\nContext \u2192 (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while\nserving as Prime Minister of Israel, criticized Donald Trump for appealing\nto \"Second Amendment people\" in a speech and warned that the words that\npoliticians use can incite violence and undermine democracy. \"Trump\u2019s\nwords are an incitement to the type of political violence that touched\nme personally,\" Rabin wrote in USAToday. He said that Trump\u2019s appeal to\n\"Second Amendment people\" to stop Hillary Clinton -- comments that were\ncriticized as a call for violence against Clinton, something Trump denied\n-- \"were a new level of ugliness in an ugly campaign season.\"\n- The son of a former Israeli Prime Minister who was assassinated wrote an\nop ed about the consequence of violent political rhetoric.\n- Warns of \"parallels\" between Israel of the 1990s and the U.S. today.\nCorrect Answer \u2192 - Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Donald Trump\u2019s\naggressive rhetoric.\nCorrect Answer \u2192 - Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Trump\u2019s aggressive\nrhetoric.\nIncorrect Answer \u2192 - Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Hillary Clinton\u2019s\naggressive rhetoric.\nIncorrect Answer \u2192 - Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned U.S.\u2019s aggressive\nrhetoric.\nIncorrect Answer \u2192 - Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Yitzhak Rabin\u2019s\naggressive rhetoric.\nFigureG.6: FormatteddatasetexampleforReCoRD.Weconsiderthecontextabovetobeasingle\u201dproblem\u201dbecause\nthisishowthetaskispresentedintheReCoRDdatasetandscoredintheReCoRDevaluationscript.\nContext \u2192 anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician\nwho is a Scottish National Party (SNP) Member of Scottish Parliament\nfor the constituency of Coatbridge and Chryston. MacGregor is currently\nParliamentary Liaison Officer to Shona Robison, Cabinet Secretary for\nHealth & Sport. He also serves on the Justice and Education & Skills\ncommittees in the Scottish Parliament.\nQuestion: Fulton James MacGregor is a Scottish politican who is a Liaison\nofficer to Shona Robison who he swears is his best friend. True, False, or\nNeither?\nCorrect Answer \u2192 Neither\nIncorrect Answer \u2192 True\nIncorrect Answer \u2192 False\nFigureG.7: FormatteddatasetexampleforANLIR1\n52", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 52}}, {"text": "Context \u2192 Organisms require energy in order to do what?\nCorrect Answer \u2192 mature and develop.\nIncorrect Answer \u2192 rest soundly.\nIncorrect Answer \u2192 absorb light.\nIncorrect Answer \u2192 take in nutrients.\nFigure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional\nprobabilityofeachanswerasdescribedin2.\nContext \u2192 Making a cake: Several cake pops are shown on a display. A woman and girl\nare shown making the cake pops in a kitchen. They\nCorrect Answer \u2192 bake them, then frost and decorate.\nIncorrect Answer \u2192 taste them as they place them on plates.\nIncorrect Answer \u2192 put the frosting on the cake as they pan it.\nIncorrect Answer \u2192 come out and begin decorating the cake as well.\nFigureG.9: FormatteddatasetexampleforHellaSwag\nContext \u2192 anli 3: anli 3: We shut the loophole which has American workers actually\nsubsidizing the loss of their own job. They just passed an expansion of\nthat loophole in the last few days: $43 billion of giveaways, including\nfavors to the oil and gas industry and the people importing ceiling fans\nfrom China.\nQuestion: The loophole is now gone True, False, or Neither?\nCorrect Answer \u2192 False\nIncorrect Answer \u2192 True\nIncorrect Answer \u2192 Neither\nFigureG.10: FormatteddatasetexampleforANLIR3\nContext \u2192 Question: George wants to warm his hands quickly by rubbing them. Which\nskin surface will produce the most heat?\nAnswer:\nCorrect Answer \u2192 dry palms\nIncorrect Answer \u2192 wet palms\nIncorrect Answer \u2192 palms covered with oil\nIncorrect Answer \u2192 palms covered with lotion\nFigureG.11: FormatteddatasetexampleforARC(Challenge). Whenpredicting,wenormalizebytheunconditional\nprobabilityofeachanswerasdescribedin2.\nContext \u2192 lull is to trust as\nCorrect Answer \u2192 cajole is to compliance\nIncorrect Answer \u2192 balk is to fortitude\nIncorrect Answer \u2192 betray is to loyalty\nIncorrect Answer \u2192 hinder is to destination\nIncorrect Answer \u2192 soothe is to passion\nFigureG.12: FormatteddatasetexampleforSATAnalogies\nCorrect Context \u2192 Grace was happy to trade me her sweater for my jacket. She thinks the\nsweater\nIncorrect Context \u2192 Grace was happy to trade me her sweater for my jacket. She thinks the\njacket\nTarget Completion \u2192 looks dowdy on her.\nFigureG.13:FormatteddatasetexampleforWinograd.The\u2018partial\u2019evaluationmethodweusecomparestheprobability\nofthecompletiongivenacorrectandincorrectcontext.\n53", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 53}}, {"text": "Correct Context \u2192 Johnny likes fruits more than vegetables in his new keto diet because the\nfruits\nIncorrect Context \u2192 Johnny likes fruits more than vegetables in his new keto diet because the\nvegetables\nTarget Completion \u2192 are saccharine.\nFigure G.14: Formatted dataset example for Winogrande. The \u2018partial\u2019 evaluation method we use compares the\nprobabilityofthecompletiongivenacorrectandincorrectcontext.\nContext \u2192 READING COMPREHENSION ANSWER KEY\nWhile this process moved along, diplomacy continued its rounds. Direct\npressure on the Taliban had proved unsuccessful. As one NSC staff note\nput it, \"Under the Taliban, Afghanistan is not so much a state sponsor\nof terrorism as it is a state sponsored by terrorists.\" In early 2000,\nthe United States began a high-level effort to persuade Pakistan to use\nits influence over the Taliban. In January 2000, Assistant Secretary\nof State Karl Inderfurth and the State Department\u2019s counterterrorism\ncoordinator, Michael Sheehan, met with General Musharraf in Islamabad,\ndangling before him the possibility of a presidential visit in March as a\nreward for Pakistani cooperation. Such a visit was coveted by Musharraf,\npartly as a sign of his government\u2019s legitimacy. He told the two envoys\nthat he would meet with Mullah Omar and press him on Bin Laden. They\nleft, however, reporting to Washington that Pakistan was unlikely in fact\nto do anything,\" given what it sees as the benefits of Taliban control\nof Afghanistan.\" President Clinton was scheduled to travel to India.\nThe State Department felt that he should not visit India without also\nvisiting Pakistan. The Secret Service and the CIA, however, warned in\nthe strongest terms that visiting Pakistan would risk the President\u2019s\nlife. Counterterrorism officials also argued that Pakistan had not done\nenough to merit a presidential visit. But President Clinton insisted\non including Pakistan in the itinerary for his trip to South Asia. His\none-day stopover on March 25, 2000, was the first time a U.S. president\nhad been there since 1969. At his meeting with Musharraf and others,\nPresident Clinton concentrated on tensions between Pakistan and India\nand the dangers of nuclear proliferation, but also discussed Bin Laden.\nPresident Clinton told us that when he pulled Musharraf aside for a brief,\none-on-one meeting, he pleaded with the general for help regarding Bin\nLaden.\" I offered him the moon when I went to see him, in terms of better\nrelations with the United States, if he\u2019d help us get Bin Laden and deal\nwith another issue or two.\" The U.S. effort continued.\nWho did The State Department feel should visit both India and Pakistan?\nCorrect Answer \u2192 - [False] Bin Laden\nIncorrect Answer \u2192 - [True] Bin Laden\nFigureG.15: FormatteddatasetexampleforMultiRC.TherearethreelevelswithinMultiRC:(1)thepassage,(2)the\nquestions,and(3)theanswers. Duringevaluation,accuracyisdeterminedattheper-questionlevel,withaquestion\nbeingconsideredcorrectifandonlyifalltheanswerswithinthequestionarelabeledcorrectly. Forthisreason,weuse\nK torefertothenumberofquestionsshownwithinthecontext.\nContext \u2192 Question: Which factor will most likely cause a person to develop a fever?\nAnswer:\nCorrect Answer \u2192 a bacterial population in the bloodstream\nIncorrect Answer \u2192 a leg muscle relaxing after exercise\nIncorrect Answer \u2192 several viral particles on the skin\nIncorrect Answer \u2192 carbohydrates being digested in the stomach\nFigure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional\nprobabilityofeachanswerasdescribedin2.\n54", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 54}}, {"text": "Context \u2192 Bob went to the gas station to fill up his car. His tank was completely\nempty and so was his wallet. The cashier offered to pay for his gas if he\ncame back later to pay. Bob felt grateful as he drove home.\nCorrect Answer \u2192 Bob believed that there were good people in the world.\nIncorrect Answer \u2192 Bob contemplated how unfriendly the world was.\nFigureG.17: FormatteddatasetexampleforStoryCloze\nContext \u2192 Helsinki is the capital and largest city of Finland. It is in the region\nof Uusimaa, in southern Finland, on the shore of the Gulf of Finland.\nHelsinki has a population of , an urban population of , and a metropolitan\npopulation of over 1.4 million, making it the most populous municipality\nand urban area in Finland. Helsinki is some north of Tallinn, Estonia,\neast of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki\nhas close historical connections with these three cities.\nThe Helsinki metropolitan area includes the urban core of Helsinki, Espoo,\nVantaa, Kauniainen, and surrounding commuter towns. It is the world\u2019s\nnorthernmost metro area of over one million people, and the city is the\nnorthernmost capital of an EU member state. The Helsinki metropolitan\narea is the third largest metropolitan area in the Nordic countries\nafter Stockholm and Copenhagen, and the City of Helsinki is the third\nlargest after Stockholm and Oslo. Helsinki is Finland\u2019s major political,\neducational, financial, cultural, and research center as well as one of\nnorthern Europe\u2019s major cities. Approximately 75% of foreign companies\nthat operate in Finland have settled in the Helsinki region. The nearby\nmunicipality of Vantaa is the location of Helsinki Airport, with frequent\nservice to various destinations in Europe and Asia.\nQ: what is the most populous municipality in Finland?\nA: Helsinki\nQ: how many people live there?\nA: 1.4 million in the metropolitan area\nQ: what percent of the foreign companies that operate in Finland are in\nHelsinki?\nA: 75%\nQ: what towns are a part of the metropolitan area?\nA:\nTarget Completion \u2192 Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns\nFigureG.18: FormatteddatasetexampleforCoQA\nContext \u2192 Please unscramble the letters into a word, and write that word:\nasinoc =\nTarget Completion \u2192 casino\nFigureG.19: FormatteddatasetexampleforCycledLetters\n55", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 55}}, {"text": "Context \u2192 Passage: Saint Jean de Br\u00b4ebeuf was a French Jesuit missionary who\ntravelled to New France in 1625. There he worked primarily with the Huron\nfor the rest of his life, except for a few years in France from 1629 to\n1633. He learned their language and culture, writing extensively about\neach to aid other missionaries. In 1649, Br\u00b4ebeuf and another missionary\nwere captured when an Iroquois raid took over a Huron village . Together\nwith Huron captives, the missionaries were ritually tortured and killed\non March 16, 1649. Br\u00b4ebeuf was beatified in 1925 and among eight Jesuit\nmissionaries canonized as saints in the Roman Catholic Church in 1930.\nQuestion: How many years did Saint Jean de Br\u00b4ebeuf stay in New France\nbefore he went back to France for a few years?\nAnswer:\nTarget Completion \u2192 4\nFigureG.20: FormatteddatasetexampleforDROP\nContext \u2192 Fill in blank:\nShe held the torch in front of her.\nShe caught her breath.\n\"Chris? There\u2019s a step.\"\n\"What?\"\n\"A step. Cut in the rock. About fifty feet ahead.\" She moved faster.\nThey both moved faster. \"In fact,\" she said, raising the torch higher,\n\"there\u2019s more than a . ->\nTarget Completion \u2192 step\nFigureG.21: FormatteddatasetexampleforLAMBADA\nContext \u2192 Please unscramble the letters into a word, and write that word:\nskicts =\nTarget Completion \u2192 sticks\nFigureG.22: FormatteddatasetexampleforAnagrams1(A1)\nContext \u2192 Please unscramble the letters into a word, and write that word:\nvolwskagen =\nTarget Completion \u2192 volkswagen\nFigureG.23: FormatteddatasetexampleforAnagrams2\nContext \u2192 Q: Who played tess on touched by an angel?\nA:\nTarget Completion \u2192 Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known\nprofessionally as Della Reese\nFigureG.24: FormatteddatasetexampleforNaturalQuestions\n56", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 56}}, {"text": "Context \u2192 TITLE: William Perry (American football) - Professional career\nPARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL\nDraft by the Chicago Bears; he had been hand-picked by coach Mike Ditka.\nHowever, defensive coordinator Buddy Ryan, who had a highly acrimonious\nrelationship with Ditka, called Perry a \"wasted draft-pick\". Perry\nsoon became a pawn in the political power struggle between Ditka and\nRyan. Perry\u2019s \"Refrigerator\" nickname followed him into the NFL and he\nquickly became a favorite of the Chicago Bears fans. Teammates called\nhim \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused\nto play Perry, Ditka decided to use Perry as a fullback when the team was\nnear the opponents\u2019 goal line or in fourth and short situations, either\nas a ball carrier or a lead blocker for star running back Walter Payton.\nDitka stated the inspiration for using Perry as a fullback came to him\nduring five-yard sprint exercises. During his rookie season, Perry\nrushed for two touchdowns and caught a pass for one. Perry even had\nthe opportunity to run the ball during Super Bowl XX, as a nod to his\npopularity and contributions to the team\u2019s success. The first time he\ngot the ball, he was tackled for a one-yard loss while attempting to throw\nhis first NFL pass on a halfback option play. The second time he got the\nball, he scored a touchdown (running over Patriots linebacker Larry McGrew\nin the process). About halfway through his rookie season, Ryan finally\nbegan to play Perry, who soon proved that he was a capable defensive\nlineman. His Super Bowl ring size is the largest of any professional\nfootball player in the history of the event. His ring size is 25, while\nthe ring size for the average adult male is between 10 and 12. Perry went\non to play for ten years in the NFL, retiring after the 1994 season. In\nhis ten years as a pro, he regularly struggled with his weight, which\nhampered his performance at times. He played in 138 games, recording\n29.5 sacks and five fumble recoveries, which he returned for a total of\n71 yards. In his offensive career he ran five yards for two touchdowns,\nand had one reception for another touchdown. Perry later attempted a\ncomeback, playing an unremarkable 1996 season with the London Monarchs of\nthe World League of American Football (later NFL Europa).\nQ: what team did he play for?\nA:\nTarget Completion \u2192 the Chicago Bears\nFigureG.25: FormatteddatasetexampleforQuAC\nContext \u2192 Please unscramble the letters into a word, and write that word:\nr e!c.i p r o.c a/l =\nTarget Completion \u2192 reciprocal\nFigureG.26: FormatteddatasetexampleforSymbolInsertion\nContext \u2192 Please unscramble the letters into a word, and write that word:\ntaefed =\nTarget Completion \u2192 defeat\nFigureG.27: FormatteddatasetexampleforReversedWords\n57", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 57}}, {"text": "Context \u2192 Title: The Blitz\nBackground: From the German point of view, March 1941 saw an improvement.\nThe Luftwaffe flew 4,000 sorties that month, including 12 major and\nthree heavy attacks. The electronic war intensified but the Luftwaffe\nflew major inland missions only on moonlit nights. Ports were easier to\nfind and made better targets. To confuse the British, radio silence was\nobserved until the bombs fell. X- and Y-Ger\u00a8at beams were placed over\nfalse targets and switched only at the last minute. Rapid frequency\nchanges were introduced for X-Ger\u00a8at, whose wider band of frequencies and\ngreater tactical flexibility ensured it remained effective at a time when\nBritish selective jamming was degrading the effectiveness of Y-Ger\u00a8at.\nQ: How many sorties were flown in March 1941?\nA: 4,000\nQ: When did the Luftwaffe fly inland missions?\nA:\nTarget Completion \u2192 only on moonlit nights\nFigureG.28: FormatteddatasetexampleforSQuADv2\nContext \u2192 Normal force -- In a simple case such as an object resting upon a table,\nthe normal force on the object is equal but in opposite direction to the\ngravitational force applied on the object (or the weight of the object),\nthat is, N = m g (\\displaystyle N=mg), where m is mass, and g is the\ngravitational field strength (about 9.81 m/s on Earth). The normal force\nhere represents the force applied by the table against the object that\nprevents it from sinking through the table and requires that the table is\nsturdy enough to deliver this normal force without breaking. However, it\nis easy to assume that the normal force and weight are action-reaction\nforce pairs (a common mistake). In this case, the normal force and\nweight need to be equal in magnitude to explain why there is no upward\nacceleration of the object. For example, a ball that bounces upwards\naccelerates upwards because the normal force acting on the ball is larger\nin magnitude than the weight of the ball.\nquestion: is the normal force equal to the force of gravity?\nanswer:\nTarget Completion \u2192 yes\nFigureG.29: FormatteddatasetexampleforBoolQ\nContext \u2192 The trend toward lower rents may seem surprising given that some\ncommunities in New York are bemoaning the loss of favorite local\nbusinesses to high rents. But, despite the recent softening, for many\nof these retailers there\u2019s still been too big a jump from the rental rates\nof the late 1970s, when their leases were signed. Certainly, the recent\ndrop in prices doesn\u2019t mean Manhattan comes cheap.\nquestion: Manhattan comes cheap. true, false, or neither?\nanswer:\nTarget Completion \u2192 false\nFigureG.30: FormatteddatasetexampleforCB\n58", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 58}}, {"text": "Context \u2192 The bet, which won him dinner for four, was regarding the existence and\nmass of the top quark, an elementary particle discovered in 1995.\nquestion: The Top Quark is the last of six flavors of quarks predicted by\nthe standard model theory of particle physics. True or False?\nanswer:\nTarget Completion \u2192 False\nFigureG.31: FormatteddatasetexampleforRTE\nContext \u2192 An outfitter provided everything needed for the safari.\nBefore his first walking holiday, he went to a specialist outfitter to buy\nsome boots.\nquestion: Is the word \u2018outfitter\u2019 used in the same way in the two\nsentences above?\nanswer:\nTarget Completion \u2192 no\nFigureG.32: FormatteddatasetexampleforWiC\nContext \u2192 Final Exam with Answer Key\nInstructions: Please carefully read the following passages. For each\npassage, you must identify which noun the pronoun marked in *bold* refers\nto.\n=====\nPassage: Mr. Moncrieff visited Chester\u2019s luxurious New York apartment,\nthinking that it belonged to his son Edward. The result was that Mr.\nMoncrieff has decided to cancel Edward\u2019s allowance on the ground that\nhe no longer requires *his* financial support.\nQuestion: In the passage above, what does the pronoun \"*his*\" refer to?\nAnswer:\nTarget Completion \u2192 mr. moncrieff\nFigureG.33: FormatteddatasetexampleforWSC\nContext \u2192 Q: \u2018Nude Descending A Staircase\u2019 is perhaps the most famous painting by\nwhich 20th century artist?\nA:\nTarget Completion \u2192 MARCEL DUCHAMP\nTarget Completion \u2192 r mutt\nTarget Completion \u2192 duchamp\nTarget Completion \u2192 marcel duchamp\nTarget Completion \u2192 R.Mutt\nTarget Completion \u2192 Marcel duChamp\nTarget Completion \u2192 Henri-Robert-Marcel Duchamp\nTarget Completion \u2192 Marcel du Champ\nTarget Completion \u2192 henri robert marcel duchamp\nTarget Completion \u2192 Duchampian\nTarget Completion \u2192 Duchamp\nTarget Completion \u2192 duchampian\nTarget Completion \u2192 marcel du champ\nTarget Completion \u2192 Marcel Duchamp\nTarget Completion \u2192 MARCEL DUCHAMP\nFigureG.34: FormatteddatasetexampleforTriviaQA.TriviaQAallowsformultiplevalidcompletions.\n59", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 59}}, {"text": "Context \u2192 Q: What school did burne hogarth establish?\nA:\nTarget Completion \u2192 School of Visual Arts\nFigureG.35: FormatteddatasetexampleforWebQA\nContext \u2192 Keinesfalls d\u00a8urfen diese f\u00a8ur den kommerziellen Gebrauch verwendet werden.\n=\nTarget Completion \u2192 In no case may they be used for commercial purposes.\nFigureG.36: FormatteddatasetexampleforDe\u2192En. Thisistheformatforone-andfew-shotlearning,forthisand\notherlangaugetasks, theformatforzero-shotlearningis\u201cQ:Whatisthe{language}translationof{sentence}A:\n{translation}.\u201d\nContext \u2192 In no case may they be used for commercial purposes. =\nTarget Completion \u2192 Keinesfalls d\u00a8urfen diese f\u00a8ur den kommerziellen Gebrauch verwendet werden.\nFigureG.37: FormatteddatasetexampleforEn\u2192De\nContext \u2192 Analysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females. =\nTarget Completion \u2192 L\u2019analyse de la distribution de fr\u00b4equence des stades larvaires d\u2019I.\nverticalis dans une s\u00b4erie d\u2019\u00b4etangs a \u00b4egalement d\u00b4emontr\u00b4e que les larves\nm^ales \u00b4etaient `a des stades plus avanc\u00b4es que les larves femelles.\nFigureG.38: FormatteddatasetexampleforEn\u2192Fr\nContext \u2192 L\u2019analyse de la distribution de fr\u00b4equence des stades larvaires d\u2019I.\nverticalis dans une s\u00b4erie d\u2019\u00b4etangs a \u00b4egalement d\u00b4emontr\u00b4e que les larves\nm^ales \u00b4etaient `a des stades plus avanc\u00b4es que les larves femelles. =\nTarget Completion \u2192 Analysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females.\nFigureG.39: FormatteddatasetexampleforFr\u2192En\nContext \u2192 The truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey\u2019s accession\nto the European Union, despite Turkey\u2019s continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill. =\nTarget Completion \u2192 Adev\u02d8arul este c\u02d8a v\u02d8a dorit\u00b8i, cu orice pret\u00b8 \u00b8si ^\u0131mpotriva dorint\u00b8ei\neuropenilor, s\u02d8a continuat\u00b8i negocierile de aderare a Turciei la Uniunea\nEuropean\u02d8a, ^\u0131n ciuda refuzului continuu al Turciei de a recunoa\u00b8ste Ciprul\n\u00b8si ^\u0131n ciuda faptului c\u02d8a reformele democratice au ajuns ^\u0131ntr-un punct mort.\nFigureG.40: FormatteddatasetexampleforEn\u2192Ro\n60", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 60}}, {"text": "Context \u2192 Adev\u02d8arul este c\u02d8a v\u02d8a dorit\u00b8i, cu orice pret\u00b8 \u00b8si ^\u0131mpotriva dorint\u00b8ei\neuropenilor, s\u02d8a continuat\u00b8i negocierile de aderare a Turciei la Uniunea\nEuropean\u02d8a, ^\u0131n ciuda refuzului continuu al Turciei de a recunoa\u00b8ste Ciprul\n\u00b8si ^\u0131n ciuda faptului c\u02d8a reformele democratice au ajuns ^\u0131ntr-un punct mort.\n=\nTarget Completion \u2192 The truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey\u2019s accession\nto the European Union, despite Turkey\u2019s continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill.\nFigureG.41: FormatteddatasetexampleforRo\u2192En\nContext \u2192 Q: What is (2 * 4) * 6?\nA:\nTarget Completion \u2192 48\nFigureG.42: FormatteddatasetexampleforArithmetic1DC\nContext \u2192 Q: What is 17 minus 14?\nA:\nTarget Completion \u2192 3\nFigureG.43: FormatteddatasetexampleforArithmetic2D-\nContext \u2192 Q: What is 98 plus 45?\nA:\nTarget Completion \u2192 143\nFigureG.44: FormatteddatasetexampleforArithmetic2D+\nContext \u2192 Q: What is 95 times 45?\nA:\nTarget Completion \u2192 4275\nFigureG.45: FormatteddatasetexampleforArithmetic2Dx\nContext \u2192 Q: What is 509 minus 488?\nA:\nTarget Completion \u2192 21\nFigureG.46: FormatteddatasetexampleforArithmetic3D-\nContext \u2192 Q: What is 556 plus 497?\nA:\nTarget Completion \u2192 1053\nFigureG.47: FormatteddatasetexampleforArithmetic3D+\nContext \u2192 Q: What is 6209 minus 3365?\nA:\nTarget Completion \u2192 2844\nFigureG.48: FormatteddatasetexampleforArithmetic4D-\n61", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 61}}, {"text": "Context \u2192 Q: What is 9923 plus 617?\nA:\nTarget Completion \u2192 10540\nFigureG.49: FormatteddatasetexampleforArithmetic4D+\nContext \u2192 Q: What is 40649 minus 78746?\nA:\nTarget Completion \u2192 -38097\nFigureG.50: FormatteddatasetexampleforArithmetic5D\u2212\nContext \u2192 Q: What is 65360 plus 16204?\nA:\nTarget Completion \u2192 81564\nFigureG.51: FormatteddatasetexampleforArithmetic5D+\n62", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 62}}, {"text": "H ResultsonAllTasksforAllModelSizes\nZero-Shot One-Shot Few-Shot\nFine-tune 175B\nName Metric Split SOTA K SmallMedLargeXL 2.7B6.7B13B175B SmallMedLargeXL 2.7B6.7B13B175B SmallMedLargeXL 2.7B6.7B13B175B(testserver)\nHellaSwag acc dev 85.6 20 33.7 43.651.0 54.762.867.470.978.9 33.0 42.950.5 53.561.966.570.078.1 33.5 43.151.3 54.962.967.371.379.3\nLAMBADA acc test 68.0 15 42.7 54.360.4 63.667.170.372.576.2 22.0 47.152.6 58.361.165.469.072.5 22.0 40.463.2 57.078.179.181.386.4\nLAMBADA ppl test 8.63 15 18.6 9.096.53 5.444.604.003.563.00 165.011.68.29 6.465.534.614.063.35 165.027.66.63 7.452.892.562.561.92\nStoryCloze acc test 91.8 70 63.3 68.572.4 73.477.277.779.583.2 62.3 68.772.3 74.277.378.779.784.7 62.3 70.273.9 76.180.281.283.087.7\nNQs acc test 44.5 64 0.64 1.752.71 4.406.015.797.8414.6 1.19 3.074.79 5.438.739.7813.723.0 1.72 4.467.89 9.7213.217.021.029.9\nTriviaQA acc dev 68.0 64 4.15 7.6114.0 19.731.338.741.864.3 4.19 12.920.5 26.535.944.451.368.0 6.96 16.326.5 32.142.351.657.571.2 71.2\nWebQs acc test 45.5 64 1.77 3.204.33 4.637.927.738.2214.4 2.56 6.208.51 9.1514.515.119.025.3 5.46 12.615.9 19.624.827.733.541.5\nRo\u2192En16 BLEU-mb test 39.9 64 2.08 2.713.09 3.1516.38.3420.219.9 0.55 15.423.0 26.330.633.235.638.6 1.25 20.725.8 29.233.134.837.039.5\nRo\u2192En16 BLEU-sb test 64 2.39 3.083.49 3.5616.88.7520.820.9 0.65 15.923.6 26.831.334.236.740.0 1.40 21.326.6 30.134.336.238.441.3\nEn\u2192Ro16 BLEU-mb test 38.5 64 2.14 2.652.53 2.503.464.245.3214.1 0.35 3.307.89 8.7213.215.117.320.6 1.25 5.909.33 10.714.316.318.021.0\nEn\u2192Ro16 BLEU-sb test 64 2.61 3.113.07 3.094.265.316.4318.0 0.55 3.909.15 10.315.718.220.824.9 1.64 7.4010.9 12.917.219.621.825.8\nFr\u2192En14 BLEU-mb test 35.0 64 1.81 2.533.47 3.1320.615.121.821.2 1.28 15.923.7 26.329.030.530.233.7 4.98 25.528.5 31.133.734.936.639.2\nFr\u2192En14 BLEU-sb test 64 2.29 2.993.90 3.6021.215.522.421.9 1.50 16.324.4 27.030.031.631.435.6 5.30 26.229.5 32.235.136.438.341.4\nEn\u2192Fr14 BLEU-mb test 45.6 64 1.74 2.162.73 2.1515.18.8212.025.2 0.49 8.0014.8 15.920.323.324.928.3 4.08 14.519.3 21.524.927.329.532.6\nEn\u2192Fr14 BLEU-sb test 45.9 64 2.44 2.753.54 2.8219.311.415.331.3 0.81 10.018.2 19.324.728.330.134.1 5.31 18.023.6 26.130.333.335.539.9\nDe\u2192En16 BLEU-mb test 40.2 64 2.06 2.873.41 3.6321.517.323.027.2 0.83 16.222.5 24.728.230.733.030.4 3.25 22.726.2 29.232.734.837.340.6\nDe\u2192En16 BLEU-sb test 64 2.39 3.273.85 4.0422.518.224.428.6 0.93 17.123.4 25.829.231.934.532.1 3.60 23.827.5 30.534.136.539.143.0\nEn\u2192De16 BLEU-mb test 41.2 64 1.70 2.272.31 2.4312.98.6610.424.6 0.50 7.0012.9 13.118.320.922.526.2 3.42 12.315.4 17.120.923.026.629.7\nEn\u2192De16 BLEU-sb test 41.2 64 2.09 2.652.75 2.9213.79.3611.025.3 0.54 7.4013.4 13.418.821.723.327.3 3.78 12.916.1 17.721.724.127.730.9\nWinograd acc test 93.8 7 66.3 72.974.7 76.982.485.787.988.3 63.4 68.572.9 76.982.484.686.189.7 63.4 67.473.6 76.984.385.482.488.6\nWinogrande acc dev 84.6 50 52.0 52.157.4 58.762.364.567.970.2 51.3 53.058.3 59.161.765.866.973.2 51.3 52.657.5 59.162.667.470.077.7\nPIQA acc dev 77.1 50 64.6 70.272.9 75.175.678.078.581.0 64.3 69.371.8 74.474.376.377.880.5 64.3 69.472.0 74.375.477.879.982.3 82.8\nARC(Challenge)acc test 78.5 50 26.6 29.531.8 35.538.041.443.751.4 25.5 30.231.6 36.438.441.543.153.2 25.5 28.432.3 36.739.543.744.851.5\nARC(Easy) acc test 92.0 50 43.6 46.553.0 53.858.260.263.868.8 42.7 48.254.6 55.960.362.666.871.2 42.7 51.058.1 59.162.165.869.170.1\nOpenBookQA acc test 87.2 100 35.6 43.245.2 46.853.050.455.657.6 37.0 39.846.2 46.453.453.055.858.8 37.0 43.648.0 50.655.655.260.865.4\nQuac f1 dev 74.4 5 21.2 26.831.0 30.134.736.138.441.5 21.1 26.931.9 32.337.439.040.643.4 21.6 27.632.9 34.238.239.940.944.3\nRACE-h acc test 90.0 10 35.2 37.940.1 40.942.444.144.645.5 34.3 37.740.0 42.043.844.344.645.9 34.3 37.040.4 41.442.344.745.146.8\nRACE-m acc test 93.1 10 42.1 47.252.1 52.354.754.456.758.4 42.3 47.351.7 55.256.154.756.957.4 42.3 47.052.7 53.055.655.458.158.1\nSQuADv2 em dev 90.7 16 22.6 32.833.9 43.143.645.449.052.6 25.1 37.537.9 47.947.951.156.060.1 27.5 40.539.2 53.550.056.662.664.9\nSQuADv2 f1 dev 93.0 16 28.3 40.241.4 50.351.052.756.359.5 30.1 43.644.1 54.054.157.161.865.4 32.1 45.544.9 58.755.962.167.769.8\nCoQA f1 dev 90.7 5 34.5 55.061.8 65.371.172.876.381.5 30.6 52.161.6 66.171.875.177.984.0 31.1 52.062.7 66.873.277.379.985.0\nDROP f1 dev 89.1 20 9.40 13.614.4 16.419.717.024.023.6 11.7 18.120.9 23.026.427.329.234.3 12.9 18.724.0 25.629.729.732.336.5\nBoolQ acc dev 91.0 32 49.7 60.358.9 62.467.165.466.260.5 52.6 61.760.4 63.768.468.769.076.7 43.1 60.662.0 64.170.370.070.277.5 76.4\nCB acc dev 96.9 32 0.00 32.18.93 19.619.628.619.646.4 55.4 53.653.6 48.257.133.955.464.3 42.9 58.953.6 69.667.960.766.182.1 75.6\nCB f1 dev 93.9 32 0.00 29.311.4 17.422.425.120.342.8 60.1 39.845.6 37.545.728.544.652.5 26.1 40.432.6 48.345.744.646.057.2 52.0\nCopa acc dev 94.8 32 66.0 68.073.0 77.076.080.084.091.0 62.0 64.066.0 74.076.082.086.087.0 67.0 64.072.0 77.083.083.086.092.0 92.0\nRTE acc dev 92.5 32 47.7 49.848.4 56.046.655.262.863.5 53.1 47.349.5 49.554.954.956.370.4 52.3 48.446.9 50.956.349.560.672.9 69.0\nWiC acc dev 76.1 32 0.00 0.000.00 0.000.000.000.000.00 50.0 50.350.3 49.249.450.350.048.6 49.8 55.053.0 53.051.653.151.155.3 49.4\nWSC acc dev 93.8 32 59.6 56.765.4 61.566.360.664.465.4 58.7 58.760.6 62.566.360.666.369.2 58.7 60.654.8 49.062.567.375.075.0 80.1\nMultiRC acc dev 62.3 32 4.72 9.6512.3 13.614.318.424.227.6 4.72 9.6512.3 13.614.318.424.227.6 6.09 11.816.8 20.824.723.825.032.5 30.5\nMultiRC f1a dev 88.2 32 57.0 59.760.4 59.960.064.571.472.9 57.0 59.760.4 59.960.064.571.472.9 45.0 55.964.2 65.469.566.469.374.8 75.4\nReCoRD acc dev 92.5 32 70.8 78.582.1 84.186.288.689.090.2 69.8 77.080.7 83.085.988.088.890.2 69.8 77.281.3 83.186.687.988.989.0 90.2\nReCoRD f1 dev 93.3 32 71.9 79.282.8 85.287.389.590.491.0 70.7 77.881.6 83.986.888.889.791.2 70.7 77.982.1 84.087.588.889.890.1 91.1\nSuperGLUE average dev 89.0 40.6 47.446.8 49.650.152.354.458.2 54.4 55.156.7 57.861.259.764.368.9 50.2 56.256.8 60.064.363.666.973.2 71.8\nANLIR1 acc test 73.8 50 33.4 34.233.4 33.434.232.333.234.6 32.1 31.631.9 34.630.631.632.732.0 32.1 32.530.9 32.533.533.133.336.8\nANLIR2 acc test 50.7 50 33.2 31.933.3 33.333.833.533.535.4 35.7 33.733.2 32.732.733.933.933.9 35.7 33.832.1 31.432.633.332.634.0\nANLIR3 acc test 48.3 50 33.6 34.033.8 33.435.334.834.434.5 35.0 32.633.0 33.934.133.132.535.1 35.0 34.435.1 36.032.733.934.540.2\n2D+ acc n/a 50 0.70 0.650.70 0.851.102.5415.476.9 2.00 0.553.15 4.0012.119.673.099.6 2.00 4.103.50 4.508.9011.955.5100.0\n2D- acc n/a 50 1.25 1.251.25 1.251.607.6012.658.0 1.15 0.951.45 1.953.8511.544.686.4 1.15 1.452.25 2.707.3513.652.498.9\n3D+ acc n/a 50 0.10 0.100.05 0.100.100.251.4034.2 0.15 0.000.10 0.300.450.9515.465.5 0.15 0.450.30 0.550.750.908.4080.4\n3D- acc n/a 50 0.05 0.050.05 0.050.050.451.3548.3 0.05 0.150.25 0.300.551.606.1578.7 0.05 0.100.15 0.350.651.059.2094.2\n4D+ acc n/a 50 0.05 0.050.00 0.000.050.050.154.00 0.00 0.000.10 0.000.000.100.8014.0 0.00 0.050.05 0.000.150.150.4025.5\n4D- acc n/a 50 0.00 0.000.00 0.000.000.000.107.50 0.00 0.000.00 0.000.050.000.5014.0 0.00 0.050.00 0.000.100.050.4026.8\n5D+ acc n/a 50 0.00 0.000.00 0.000.000.000.000.65 0.00 0.000.00 0.000.000.000.053.45 0.00 0.000.00 0.000.000.000.059.30\n5D- acc n/a 50 0.00 0.000.00 0.000.000.000.000.80 0.00 0.000.00 0.000.000.000.053.75 0.00 0.000.00 0.000.000.000.009.90\n2Dx acc n/a 50 2.20 2.252.65 2.102.555.806.1519.8 1.35 2.353.35 2.354.759.1511.027.4 1.35 2.902.70 2.854.256.107.0529.2\n1DC acc n/a 50 1.25 2.952.75 0.050.302.350.759.75 1.90 2.802.85 3.656.459.158.2014.3 1.70 2.153.90 5.756.207.609.9521.3\nCycledLetters acc n/a 100 0.62 0.712.85 0.000.631.352.583.66 1.67 4.365.68 6.466.259.4115.121.7 4.63 9.2710.7 14.516.721.927.737.9\nAnagrams1 acc n/a 100 0.10 0.140.40 0.000.270.691.162.28 0.21 0.611.12 1.271.602.723.728.62 0.50 1.272.13 3.053.815.498.3815.1\nAnagrams2 acc n/a 100 0.81 1.212.69 0.011.713.754.538.91 1.19 2.624.70 4.776.9710.214.625.9 1.94 4.807.59 9.8712.618.925.639.7\nSymbolInsertionacc n/a 100 0.00 0.000.10 0.000.050.420.898.26 0.03 0.050.57 1.181.673.466.6245.4 0.11 0.282.19 4.186.6111.027.367.2\nReversedWords acc n/a 100 0.00 0.010.01 0.010.020.030.030.09 0.02 0.010.01 0.000.050.070.110.48 0.00 0.050.00 0.170.240.300.420.44\nSATAnalogies acc n/a 20 35.6 39.045.2 44.150.049.252.753.7 30.5 41.243.1 46.555.154.353.559.1 30.5 40.442.8 40.648.451.953.565.2\nTableH.1: Scoresforeverytask,settingandmodelthatweinvestigateinthispaper.\n63", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 63}}, {"text": "References\n[ADG+16] MarcinAndrychowicz,MishaDenil,SergioGomez,MatthewWHoffman,DavidPfau,TomSchaul,\nBrendanShillingford,andNandoDeFreitas. Learningtolearnbygradientdescentbygradientdescent.\nInAdvancesinneuralinformationprocessingsystems,pages3981\u20133989,2016.\n[AI19] WeChatAI. Tr-mt(ensemble),December2019.\n[AJF19] RoeeAharoni,MelvinJohnson,andOrhanFirat. Massivelymultilingualneuralmachinetranslation. In\nProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational\nLinguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),2019.\n[BBDIW20] SuLinBlodgett,SolonBarocas,HalDaume\u00b4 III,andHannaWallach. Language(technology)ispower:\nAcriticalsurveyof\u201cbias\u201dinnlp. arXivpreprintarXiv:2005.14050,2020.\n[BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answerpairs. InProceedingsofthe2013conferenceonempiricalmethodsinnaturallanguage\nprocessing,pages1533\u20131544,2013.\n[BDD+09] LuisaBentivogli,IdoDagan,HoaTrangDang,DaniloGiampiccolo,andBernardoMagnini. Thefifth\nPASCALrecognizingtextualentailmentchallenge. 2009.\n[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical\nresourceforsentimentanalysisandopinionmining. InLrec,volume10,pages2200\u20132204,2010.\n[BHDD+06] RoyBarHaim,IdoDagan,BillDolan,LisaFerro,DaniloGiampiccolo,BernardoMagnini,andIdan\nSzpektor. ThesecondPASCALrecognisingtextualentailmentchallenge. 2006.\n[BHT+20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, JacobAndreas, Yoshua Bengio, Joyce Chai, Mirella\nLapata,AngelikiLazaridou,JonathanMay,AleksandrNisnevich,etal. Experiencegroundslanguage.\narXivpreprintarXiv:2004.10151,2020.\n[BLC13] YoshuaBengio,NicholasLe\u00b4onard,andAaronC.Courville. Estimatingorpropagatinggradientsthrough\nstochasticneuronsforconditionalcomputation. Arxiv,2013.\n[BZB+19] YonatanBisk,RowanZellers,RonanLeBras,JianfengGao,andYejinChoi. Piqa: Reasoningabout\nphysicalcommonsenseinnaturallanguage. arXivpreprintarXiv:1911.11641,2019.\n[Car97] RichCaruana. Multitasklearning. Machinelearning,28(1),1997.\n[CB78] SusanCareyandElsaBartlett.Acquiringasinglenewword.ProceedingsoftheStanfordChildLanguage\nConference,1978.\n[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge. ArXiv,\nabs/1803.05457,2018.\n[CGRS19] RewonChild,ScottGray,AlecRadford,andIlyaSutskever. Generatinglongsequenceswithsparse\ntransformers,2019.\n[CHI+18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\nZettlemoyer. Quac: Questionansweringincontext. Arxiv,2018.\n[CLC+19] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristina\nToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044,2019.\n[CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjingLiu. Uniter: Learninguniversalimage-textrepresentations. arXivpreprintarXiv:1909.11740,\n2019.\n[Cra17] KateCrawford. Thetroublewithbias. NIPS2017Keynote,2017.\n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.\n68", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 68}}, {"text": "[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. InMachinelearningchallenges.evaluatingpredictiveuncertainty,visualobjectclassification,\nandrecognisingtextualentailment,pages177\u2013190.Springer,2006.\n[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. Arxiv,2018.\n[DHKH14] NadirDurrani,BarryHaddow,PhilippKoehn,andKennethHeafield. Edinburgh\u2019sphrase-basedmachine\ntranslationsystemsforwmt-14.InProceedingsoftheNinthWorkshoponStatisticalMachineTranslation,\npages97\u2013104,2014.\n[DL15] AndrewM.DaiandQuocV.Le. Semi-supervisedsequencelearning. InAdvancesinneuralinformation\nprocessingsystems,2015.\n[DMST19] Marie-CatherineDeMarneffe,MandySimons,andJudithTonhauser. TheCommitmentBank: Investigat-\ningprojectioninnaturallyoccurringdiscourse. 2019. ToappearinproceedingsofSinnundBedeutung\n23.Datacanbefoundathttps://github.com/mcdm/CommitmentBank/.\n[DSC+16] YanDuan, JohnSchulman, XiChen, PeterL.Bartlett, IlyaSutskever, andPieterAbbeel. Rl2: Fast\nreinforcementlearningviaslowreinforcementlearning. ArXiv,abs/1611.02779,2016.\n[DWD+19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDrop: Areadingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs. arXivpreprint\narXiv:1903.00161,2019.\n[DYY+19] ZihangDai,ZhilinYang,YimingYang,JaimeG.Carbonell,QuocV.Le,andRuslanSalakhutdinov.\nTransformer-xl: Attentivelanguagemodelsbeyondafixed-lengthcontext. Arxiv,2019.\n[EOAG18] SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier. Understandingback-translationatscale.\narXivpreprintarXiv:1808.09381,2018.\n[FAL17] ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptationof\ndeepnetworks. ArXiv,abs/1703.03400,2017.\n[Fyo00] YaroslavFyodorov. Anaturallogicinferencesystem,2000.\n[GG19] HilaGonenandYoavGoldberg. Lipstickonapig: Debiasingmethodscoverupsystematicgenderbiases\ninwordembeddingsbutdonotremovethem. arXivpreprintarXiv:1903.03862,2019.\n[GLT+20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmentedlanguagemodelpre-training. arXivpreprintarXiv:2002.08909,2020.\n[GMDD07] DaniloGiampiccolo,BernardoMagnini,IdoDagan,andBillDolan. ThethirdPASCALrecognizing\ntextualentailmentchallenge. InProceedingsoftheACL-PASCALworkshopontextualentailmentand\nparaphrasing,pages1\u20139.AssociationforComputationalLinguistics,2007.\n[Gra16] AlexGraves. Adaptivecomputationtimeforrecurrentneuralnetworks. Arxiv,2016.\n[GSL+18] SuchinGururangan,SwabhaSwayamdipta,OmerLevy,RoySchwartz,SamuelRBowman,andNoahA\nSmith. Annotationartifactsinnaturallanguageinferencedata. arXivpreprintarXiv:1803.02324,2018.\n[GSR19] SebastianGehrmann,HendrikStrobelt,andAlexanderM.Rush. Gltr: Statisticaldetectionandvisualiza-\ntionofgeneratedtext. arXivpreprintarXiv: 1906.04043,2019.\n[GWC+18] JiataoGu,YongWang,YunChen,KyunghyunCho,andVictorOKLi. Meta-learningforlow-resource\nneuralmachinetranslation. arXivpreprintarXiv:1808.08437,2018.\n[HB20] DanielHernandezandTomBrown. Aiandefficiency,May2020.\n[HBFC19] AriHoltzman,JanBuys,MaxwellForbes,andYejinChoi. Thecuriouscaseofneuraltextdegeneration.\nCoRR,abs/1904.09751,2019.\n[HLW+20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrainedtransformersimproveoutofdistributionrobustness. arXivpreprintarXiv:2004.06100,2020.\n69", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 69}}, {"text": "[HNA+17] JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKianinejad,Md.\nMostofaAliPatwary,YangYang,andYanqiZhou. Deeplearningscalingispredictable,empirically.\narXivpreprintarXiv:1712.00409,2017.\n[HR18] JeremyHowardandSebastianRuder. Universallanguagemodelfine-tuningfortextclassification. arXiv\npreprintarXiv:1801.06146,2018.\n[HVD15] GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeinaneuralnetwork. arXiv\npreprintarXiv:1503.02531,2015.\n[HYC01] SeppHochreiter,AStevenYounger,andPeterRConwell. LearningtoLearnUsingGradientDescent.\nInInternationalConferenceonArtificialNeuralNetworks,pages87\u201394.Springer,2001.\n[HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini,\nDaniYogatama,andPushmeetKohli. Reducingsentimentbiasinlanguagemodelsviacounterfactual\nevaluation. arXivpreprintarXiv:1911.03064,2019.\n[IBGC+14] MohitIyyer,JordanBoyd-Graber,LeonardoClaudino,RichardSocher,andHalDaume\u00b4 III. Aneural\nnetworkforfactoidquestionansweringoverparagraphs. InEmpiricalMethodsinNaturalLanguage\nProcessing,2014.\n[IDCBE19] DaphneIppolito,DanielDuckworth,ChrisCallison-Burch,andDouglasEck. Automaticdetectionof\ngeneratedtextiseasiestwhenhumansarefooled. arXivpreprintarXiv:1911.00650,2019.\n[JCWZ17] MandarJoshi,EunsolChoi,DanielS.Weld,andLukeZettlemoyer. TriviaQA:Alargescaledistantly\nsupervisedchallengedatasetforreadingcomprehension. arXivpreprintarXiv:1705.03551,2017.\n[JN20] ZhengJunyuanandGammaLabNYC. Numerictransformer-albert,March2020.\n[JVS+16] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploringthelimits\noflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\n[JYS+19] Xiaoqi Jiao, YichunYin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and QunLiu.\nTinyBERT:DistillingBERTfornaturallanguageunderstanding. arXivpreprintarXiv:1909.10351,2019.\n[JZC+19] YingJu,FubangZhao,ShijieChen,BowenZheng,XuefengYang,andYunfengLiu. Technicalreporton\nconversationalquestionanswering. arXivpreprintarXiv:1909.10772,2019.\n[KCR+18] DanielKhashabi,SnigdhaChaturvedi,MichaelRoth,ShyamUpadhyay,andDanRoth. Lookingbeyond\nthesurface: Achallengesetforreadingcomprehensionovermultiplesentences. InProceedingsofNorth\nAmericanChapteroftheAssociationforComputationalLinguistics(NAACL),2018.\n[KKS+20] DanielKhashabi,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUnifiedqa: Crossingformatboundarieswithasingleqasystem. arXivpreprintarXiv:2005.00700,2020.\n[KMB20] SarahE.Kreps,MilesMcCain,andMilesBrundage. Allthenewsthat\u2019sfittofabricate: Ai-generated\ntextasatoolofmediamisinformation,2020.\n[KMH+20] JaredKaplan,SamMcCandlish,TomHenighan,TomB.Brown,BenjaminChess,RewonChild,Scott\nGray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels,2020.\n[KPR+19] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,AnkurParikh,ChrisAlberti,\nDanielleEpstein,IlliaPolosukhin,MatthewKelcey,JacobDevlin,KentonLee,KristinaN.Toutanova,\nLlionJones,Ming-WeiChang,AndrewDai,JakobUszkoreit,QuocLe,andSlavPetrov. Naturalques-\ntions: abenchmarkforquestionansweringresearch. TransactionsoftheAssociationofComputational\nLinguistics,2019.\n[KR16] YoonKimandAlexanderM.Rush. Sequence-levelknowledgedistillation. Arxiv,2016.\n[LB02] EdwardLoperandStevenBird. Nltk: Thenaturallanguagetoolkit,2002.\n[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291,2019.\n70", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 70}}, {"text": "[LCG+19] ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,andRaduSori-\ncut. ALBERT:AliteBERTforself-supervisedlearningoflanguagerepresentations. arXivpreprint\narXiv:1909.11942,2019.\n[LCH+20] XiaodongLiu,HaoCheng,PengchengHe,WeizhuChen,YuWang,HoifungPoon,andJianfengGao.\nAdversarialtrainingforlargeneurallanguagemodels. arXivpreprintarXiv:2004.08994,2020.\n[LDL19] ZhongyangLi,XiaoDing,andTingLiu. Storyendingpredictionbytransferablebert. arXivpreprint\narXiv:1905.07504,2019.\n[LDM12] HectorLevesque,ErnestDavis,andLeoraMorgenstern. TheWinogradschemachallenge. InThirteenth\nInternationalConferenceonthePrinciplesofKnowledgeRepresentationandReasoning,2012.\n[LGG+20] YinhanLiu,JiataoGu,NamanGoyal,XianLi,SergeyEdunov,MarjanGhazvininejad,MikeLewis,and\nLukeZettlemoyer. Multilingualdenoisingpre-trainingforneuralmachinetranslation. arXivpreprint\narXiv:2001.08210,2020.\n[LGH+15] XiaodongLiu, JianfengGao, XiaodongHe, LiDeng, KevinDuh, andYe-YiWang. Representation\nlearningusingmulti-taskdeepneuralnetworksforsemanticclassificationandinformationretrieval. In\nProceedingsofthe2015ConferenceoftheNorthAmericanChapteroftheAssociationforComputational\nLinguistics: HumanLanguageTechnologies,2015.\n[LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101,2017.\n[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\nnetworksviaknowledgedistillationfornaturallanguageunderstanding.arXivpreprintarXiv:1904.09482,\n2019.\n[LHCG19b] XiaodongLiu,PengchengHe,WeizhuChen,andJianfengGao. Multi-taskdeepneuralnetworksfor\nnaturallanguageunderstanding. arXivpreprintarXiv:1901.11504,2019.\n[Lin20] TalLinzen.Howcanweaccelerateprogresstowardshuman-likelinguisticgeneralization? arXivpreprint\narXiv:2005.00955,2020.\n[LLG+19] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,\nVesStoyanov,andLukeZettlemoyer. Bart: Denoisingsequence-to-sequencepre-trainingfornatural\nlanguagegeneration,translation,andcomprehension. arXivpreprintarXiv:1910.13461,2019.\n[LM17] KeLiandJitendraMalik. Learningtooptimizeneuralnets. arXivpreprintarXiv:1703.00441,2017.\n[LOG+19] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,\nLukeZettlemoyer,andVeselinStoyanov. RoBERTa: ArobustlyoptimizedBERTpretrainingapproach.\narXivpreprintarXiv:1907.11692,2019.\n[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Ku\u00a8ttler, Mike Lewis, Wen-tau Yih, Tim Rockta\u00a8schel, Sebastian Riedel, and Kiela Douwe.\nRetrieval-augmentedgenerationforknowledge-intensivenlptasks. arXivpreprintarXiv:2005.11401,\n2020.\n[LSP+18] PeterJ.Liu,MohammadSaleh,EtiennePot,BenGoodrich,RyanSepassi,LukaszKaiser,andNoam\nShazeer. GeneratingWikipediabysummarizinglongsequences. arXivpreprintarXiv:1801.10198,2018.\n[LWS+20] ZhuohanLi,EricWallace,ShengShen,KevinLin,KurtKeutzer,DanKlein,andJosephE.Gonzalez.\nTrainlarge,thencompress: Rethinkingmodelsizeforefficienttrainingandinferenceoftransformers,\n2020.\n[LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading\ncomprehensiondatasetfromexaminations. arXivpreprintarXiv:1704.04683,2017.\n[LYN+20] Sheng-ChiehLin,Jheng-HongYang,RodrigoNogueira,Ming-FengTsai,Chuan-JuWang,andJimmy\nLin. Tttttacklingwinograndeschemas. arXivpreprintarXiv:2003.08380,2020.\n[Mac92] David.MacKay. Information-basedobjectivefunctionsforactivedataselection. NeuralComputation,\n1992.\n71", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 71}}, {"text": "[MBXS17] BryanMcCann,JamesBradbury,CaimingXiong,andRichardSocher. Learnedintranslation: Con-\ntextualizedwordvectors. InAdvancesinNeuralInformationProcessingSystems,pages6294\u20136305,\n2017.\n[MCCD13] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimationofwordrepresentations\ninvectorspace. arXivpreprintarXiv:1301.3781,2013.\n[MCH+16] NasrinMostafazadeh,NathanaelChambers,XiaodongHe,DeviParikh,DhruvBatra,LucyVanderwende,\nPushmeetKohli, andJamesAllen. Acorpusandevaluationframeworkfordeeperunderstandingof\ncommonsensestories. arXivpreprintarXiv:1604.01696,2016.\n[MCKS18] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal.Canasuitofarmorconductelectricity?\nanewdatasetforopenbookquestionanswering. ArXiv,abs/1809.02789,2018.\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of\nlarge-batchtraining,2018.\n[MKM+94] MitchellMarcus,GraceKim,MaryAnnMarcinkiewicz,RobertMacIntyre,AnnBies,MarkFerguson,\nKaren Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure.\nIn Proceedings of the workshop on Human Language Technology, pages 114\u2013119. Association for\nComputationalLinguistics,1994.\n[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitasklearningasquestionanswering. arXivpreprintarXiv:1806.08730,2018.\n[MPL19] RThomasMcCoy,ElliePavlick,andTalLinzen. Rightforthewrongreasons: Diagnosingsyntactic\nheuristicsinnaturallanguageinference. arXivpreprintarXiv:1902.01007,2019.\n[MWZ+18] MargaretMitchell,SimoneWu,AndrewZaldivar,ParkerBarnes,LucyVasserman,BenHutchinson,\nElenaSpitzer,InioluwaDeborahRaji,andTimnitGebru. Modelcardsformodelreporting,2018.\n[NBR20] MoinNadeem,AnnaBethke,andSivaReddy. Stereoset: Measuringstereotypicalbiasinpretrained\nlanguagemodels. arXivpreprintarXiv:2004.09456,2020.\n[NK19] TimothyNivenandHung-YuKao.Probingneuralnetworkcomprehensionofnaturallanguagearguments.\narXivpreprintarXiv:1907.07355,2019.\n[Nor09] PeterNorvig. Naturallanguagecorpusdata,2009.\n[NvNvdG19] MalvinaNissim,RikvanNoord,andRobvanderGoot. Fairisbetterthansensational: Manistodoctor\naswomanistodoctor. arXivpreprintarXiv:1905.09866,2019.\n[NWD+19] YixinNie,AdinaWilliams,EmilyDinan,MohitBansal,JasonWeston,andDouweKiela. Adversarial\nnli: Anewbenchmarkfornaturallanguageunderstanding. arXivpreprintarXiv:1910.14599,2019.\n[oR16] UniversityofRegensburg. Fascha,2016.\n[PCC18] MohammadTaherPilehvarandJoseCamacho-Collados. WIC:10,000examplepairsforevaluating\ncontext-sensitiverepresentations. arXivpreprintarXiv:1808.09121,2018.\n[PFB18] JasonPhang,ThibaultFe\u00b4vry,andSamuelR.Bowman. SentenceencodersonSTILTs: Supplementary\ntrainingonintermediatelabeled-datatasks. arXivpreprintarXiv:1811.01088,2018.\n[PHR+18] AdamPoliak,AparajitaHaldar,RachelRudinger,J.EdwardHu,ElliePavlick,AaronStevenWhite,and\nBenjaminVanDurme.Collectingdiversenaturallanguageinferenceproblemsforsentencerepresentation\nevaluation. InProceedingsofEMNLP,2018.\n[PKL+16] DenisPaperno,Germa\u00b4nKruszewski,AngelikiLazaridou,QuanNgocPham,RaffaellaBernardi,Sandro\nPezzelle,MarcoBaroni,GemmaBoleda,andRaquelFerna\u00b4ndez. Thelambadadataset: Wordprediction\nrequiringabroaddiscoursecontext. arXivpreprintarXiv:1606.06031,2016.\n[PNZtY18] MatthewE.Peters,MarkNeumann,LukeZettlemoyer,andWentauYih. Dissectingcontextualword\nembeddings: Architectureandrepresentation,2018.\n[Pos18] MattPost. AcallforclarityinreportingBLEUscores. arXivpreprintarXiv:1804.08771,2018.\n72", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 72}}, {"text": "[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing(EMNLP),2014.\n[QIA20] QIANXIN. Sa-netonalbert(ensemble),April2020.\n[QMZH19] YusuQian,UrwaMuaz,BenZhang,andJaeWonHyun. Reducinggenderbiasinword-levellanguage\nmodelswithagender-equalizinglossfunction. arXivpreprintarXiv:1905.12801,2019.\n[RBG11] MelissaRoemmele,CosminAdrianBejan,andAndrewSGordon. Choiceofplausiblealternatives: An\nevaluationofcommonsensecausalreasoning. In2011AAAISpringSymposiumSeries,2011.\n[RCM19] SivaReddy, DanqiChen, andChristopherDManning. Coqa: Aconversationalquestionanswering\nchallenge. TransactionsoftheAssociationforComputationalLinguistics,7:249\u2013266,2019.\n[RCP+17] Scott Reed, Yutian Chen, Thomas Paine, Aa\u00a8ron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals,andNandodeFreitas. Few-shotautoregressivedensityestimation: Towardslearningtolearn\ndistributions. arXivpreprintarXiv:1710.10304,2017.\n[RJL18] PranavRajpurkar,RobinJia,andPercyLiang. Knowwhatyoudon\u2019tknow: Unanswerablequestionsfor\nsquad. arXivpreprintarXiv:1806.03822,2018.\n[RL16] SachinRaviandHugoLarochelle. Optimizationasamodelforfew-shotlearning. ICLR2017(oral),\n2016.\n[RLL+19] QiuRan,YankaiLin,PengLi,JieZhou,andZhiyuanLiu. NumNet: Machinereadingcomprehension\nwithnumericalreasoning. InProceedingsofEMNLP,2019.\n[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\ncoreferenceresolution. arXivpreprintarXiv:1804.09301,2018.\n[RNSS18] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improvinglanguageunderstanding\nbygenerativepre-training,2018.\n[Ros12] R.S.Ross. Guideforconductingriskassessments. NISTSpecialPublication,2012.\n[RRBS19] JonathanS.Rosenfeld,AmirRosenfeld,YonatanBelinkov,andNirShavit. Aconstructivepredictionof\nthegeneralizationerroracrossscales,2019.\n[RRS20] AdamRoberts,ColinRaffel,andNoamShazeer. Howmuchknowledgecanyoupackintotheparameters\nofalanguagemodel? arXivpreprintarXiv:2002.08910,2020.\n[RSR+19] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer,2019.\n[RWC+19] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever. Language\nmodelsareunsupervisedmultitasklearners,2019.\n[SBBC19] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande: Anadversarial\nwinogradschemachallengeatscale,2019.\n[SBC+19] IreneSolaiman,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeffWu,AlecRadford,\nGretchenKrueger,JongWookKim,SarahKreps,MilesMcCain,AlexNewhouse,JasonBlazakis,Kris\nMcGuffie,andJasmineWang. Releasestrategiesandthesocialimpactsoflanguagemodels,2019.\n[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a\nbabysitter: Onbiasesinlanguagegeneration. arXivpreprintarXiv:1909.01326,2019.\n[SDCW19] VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf. DistilBERT,adistilledversionof\nBERT:smaller,faster,cheaperandlighter. arXivpreprintarXiv:1910.01108,2019.\n[SDSE19] RoySchwartz,JesseDodge,NoahA.Smith,andOrenEtzioni. GreenAI. CoRR,abs/1907.10597,2019.\n[SHB15] RicoSennrich,BarryHaddow,andAlexandraBirch. Improvingneuralmachinetranslationmodelswith\nmonolingualdata. arXivpreprintarXiv:1511.06709,2015.\n73", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 73}}, {"text": "[SMM+17] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,andJeff\nDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer. arXivpreprint\narXiv:1701.06538,2017.\n[SPP+19] MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatanzaro.\nMegatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism,2019.\n[SS20] TimoSchickandHinrichSchu\u00a8tze. Exploitingclozequestionsforfew-shottextclassificationandnatural\nlanguageinference. arXivpreprintarXiv:2001.07676,2020.\n[STQ+19] KaitaoSong,XuTan,TaoQin,JianfengLu,andTie-YanLiu. MASS:Maskedsequencetosequence\npre-trainingforlanguagegeneration. arXivpreprintarXiv:1905.02450,2019.\n[TFR+17] JoshTobin,RachelFong,AlexRay,JonasSchneider,WojciechZaremba,andPieterAbbeel. Domain\nrandomizationfortransferringdeepneuralnetworksfromsimulationtotherealworld.In2017IEEE/RSJ\ninternationalconferenceonintelligentrobotsandsystems(IROS),pages23\u201330.IEEE,2017.\n[TL05] PeterD.TurneyandMichaelL.Littman. Corpus-basedlearningofanalogiesandsemanticrelations.\nCoRR,abs/cs/0508103,2005.\n[TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847,2018.\n[TLBS03] PeterD.Turney,MichaelL.Littman,JeffreyBigham,andVictorShnayder. Combiningindependent\nmodulestosolvemultiple-choicesynonymandanalogyproblems. CoRR,cs.CL/0309035,2003.\n[Tur20] ProjectTuring. Microsoftresearchblog,Feb2020.\n[VBL+16] OriolVinyals,CharlesBlundell,TimothyLillicrap,DaanWierstra,etal. MatchingNetworksforOne\nShotLearning. InAdvancesinneuralinformationprocessingsystems,pages3630\u20133638,2016.\n[VSP+17] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,\u0141ukasz\nKaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinneuralinformationprocessing\nsystems,2017.\n[WPN+19] AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,FelixHill,Omer\nLevy,andSamuelBowman. Superglue: Astickierbenchmarkforgeneral-purposelanguageunderstand-\ningsystems. InAdvancesinNeuralInformationProcessingSystems,pages3261\u20133275,2019.\n[WXH+18] YirenWang,YingceXia,TianyuHe,FeiTian,TaoQin,ChengXiangZhai,andTie-YanLiu. Multi-agent\nduallearning. ICLR2019,2018.\n[XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data\naugmentationforconsistencytraining,2019.\n[YdC+19] DaniYogatama,CypriendeMassond\u2019Autume,JeromeConnor,TomasKocisky,MikeChrzanowski,\nLingpengKong,AngelikiLazaridou,WangLing,LeiYu,ChrisDyer,etal. Learningandevaluating\ngenerallinguisticintelligence. arXivpreprintarXiv:1901.11373,2019.\n[YDY+19] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RuslanSalakhutdinov,andQuocV.Le. XLNet:\nGeneralizedautoregressivepretrainingforlanguageunderstanding. arXivpreprintarXiv:1906.08237,\n2019.\n[ZHB+19] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Canamachine\nreallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.\n[ZHR+19] RowanZellers,AriHoltzman,HannahRashkin,YonatanBisk,AliFarhadi,FranziskaRoesner,andYejin\nChoi. Defendingagainstneuralfakenews. arXivpreprintarXiv:1905.12616,2019.\n[ZLL+18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nReCoRD:Bridgingthegapbetweenhumanandmachinecommonsensereadingcomprehension. arXiv\npreprintarXiv:1810.12885,2018.\n[ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences,2019.\n74", "metadata": {"paper_title": "2005.14165v4.pdf", "page_number": 74}}]